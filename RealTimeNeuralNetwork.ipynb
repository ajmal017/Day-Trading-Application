{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "from nyse_dates_prds import NYSE_holidays2\n",
    "from RealTimeNN import *\n",
    "from dateutil import rrule \n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg.blas\n",
    "import datetime\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network(neural_network, delay_input=[0], delay_internal=[], delay_output=[]):\n",
    "    \"\"\" Create Neural Network\n",
    "    Example: \n",
    "      network         = [2,3,4,1] network w/ 2 inputs, 2 hidden layers w/ 3 and 4 neurons, \n",
    "                                  and 1 linear output layer                             \n",
    "      delay_input     = [0,1,5] will use inputs of timestep t, t-1 and t-5\n",
    "      delay_internal  = [1,2,5] adds recurrent connection for output of each layer \n",
    "                                in timestep t-1, t-2 and t-5\n",
    "      delay_output    = [1,3,4] adds recurrent connection for output of each layer in \n",
    "                                timestep t-1, t-3 and t-5\n",
    "    \n",
    "    Args:\n",
    "        network:        structure of the neural network [I HL1 HL2 ... HLN OL]\n",
    "                        number of layers is the length of the list-1\n",
    "                        number neurons in each layer is the given number\n",
    "    \n",
    "        delay_input:    Time delays for NN inputs. \n",
    "                        To use only the input of timestep t dIn = [0]\n",
    "            \n",
    "        delay_internal: Time delays for recurrent internal connections of NN.\n",
    "                        dIntern has to be greater than zero (layer output at timestep t-x)!\n",
    "                        if non-empty list given, recurrent connection from every layer \n",
    "                        to itself and every layer before is added\n",
    "            \n",
    "        delay_output:   Time delays for recurrent connections of output to first hidden \n",
    "                        layer. dOut has to be greater than zero (output at timestep t-x)!\n",
    "                        if non-empty list given, recurrent connection from NN output to first \n",
    "                        hidden layer is added \n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "        net: untrained neural network \n",
    "    \"\"\"\n",
    "    \n",
    "    network                = {}          \n",
    "    \n",
    "    network['delay']       = {'input'    :delay_input, \n",
    "                              'internal' :delay_internal, \n",
    "                              'output'   :delay_output}   \n",
    "    \n",
    "    #Structure\n",
    "    network['network']     = neural_network  \n",
    "    \n",
    "    network['num_layers']  = len(neural_network) - 1  \n",
    "    \n",
    "    #structure without inputs\n",
    "    network['layers']      = neural_network[1:] \n",
    "    \n",
    "    #maximum time delay\n",
    "    network['max_delay']   = max( max(delay_input, delay_internal, delay_output) )\n",
    "    \n",
    "    #initialize random weight vector and specify sets\n",
    "    network                = create_weight_vector(network)    \n",
    "    \n",
    "    #weight vector used for calculation\n",
    "    network['weight_vect'] = network['w0'].copy()\n",
    "    \n",
    "    #number of weights\n",
    "    network['num_weights'] = len(network['w0'])           \n",
    "    \n",
    "    return network\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_weight_vector(network):\n",
    "    \"\"\"\n",
    "    Creates random weight vector of NN and defines sets needed for derivative calculation\n",
    "    \n",
    "    Returns: \n",
    "        neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    num_layers_network  = network['num_layers']   \n",
    "    layers              = network['layers'] \n",
    "    delay               = network['delay'] \n",
    "    inputs              = network['network'][0]\n",
    "    \n",
    "    # Input layers or layers w/ internal delay > 0\n",
    "    input_layers     = [] \n",
    "    # Output of layer used for cost func calc or added to input layer w/ delay > 1\n",
    "    output_layers    = []  \n",
    "    \n",
    "    # Connection weight matrix layer m -> layer l w/ delay d\n",
    "    connection_weight_matrices  = {}  \n",
    "    bias                        = {}\n",
    "    input_weight_matrices       = {}\n",
    "    layers_bkwd_connect_layerM  = {} \n",
    "    layers_fwd_connect_layerM   = {} \n",
    "    delay_layerM_toL            = {}   \n",
    "    input_layersU_connect_to    = {}            \n",
    "    output_layersX_connect_to   = {}\n",
    "            \n",
    "    '''Inputs'''\n",
    "    inputs_connect_layer1      = {} \n",
    "    inputs_connect_layer1[1]   = [1]           \n",
    "    delay_input_layer1         = {}\n",
    "    delay_input_layer1[1, 1]   = delay['input']\n",
    "  \n",
    "    for x in delay_input_layer1[1, 1]:\n",
    "        # Input-weight matrix set to random values [-0.5,0.5]\n",
    "        input_weight_matrices[1, 1, x] = np.random.rand(layers[0], inputs) - 0.5  \n",
    "    \n",
    "    # First layer is input layer\n",
    "    input_layers.append(1)  \n",
    "    \n",
    "    '''Internal Connection Weight Matrices'''\n",
    "    for m in range(1, num_layers_network + 1):\n",
    "        layers_bkwd_connect_layerM[m] = []     \n",
    "        layers_fwd_connect_layerM[m]  = []\n",
    "            \n",
    "        # Forward connects\n",
    "        if m > 1:\n",
    "            l = m - 1\n",
    "            # No delay for forward connects\n",
    "            delay_layerM_toL[m, l]              = [0]                                            \n",
    "            connection_weight_matrices[m, l, 0] = np.random.rand(layers[m - 1], \n",
    "                                                                 layers[l - 1]) - 0.5 \n",
    "            \n",
    "            layers_bkwd_connect_layerM[l].append(m)  \n",
    "            layers_fwd_connect_layerM[m].append(l)      \n",
    "    \n",
    "        # Recursive connects\n",
    "        for l in range(m, num_layers_network + 1):\n",
    "            if (m == 1) and (l == num_layers_network):            \n",
    "                # Delays from output to first layer\n",
    "                delay_layerM_toL[m, l] = delay['output']          \n",
    "            else:\n",
    "                # Internal delays\n",
    "                delay_layerM_toL[m, l] = delay['internal']       \n",
    "                \n",
    "            # All delays for connect l->m    \n",
    "            for d in delay_layerM_toL[m, l]:                                                     \n",
    "                connection_weight_matrices[m, l, d] = np.random.rand(layers[m - 1], \n",
    "                                                                     layers[l - 1]) - 0.5\n",
    "                \n",
    "                # Add if haven't yet\n",
    "                if (l not in layers_fwd_connect_layerM[m]): \n",
    "                    layers_fwd_connect_layerM[m].append(l)  \n",
    "                    \n",
    "                # If recurrent connect\n",
    "                if (l >= m) and(d > 0): \n",
    "                    if (m not in input_layers):  \n",
    "                        input_layers.append(m) \n",
    "                        \n",
    "                    if (l not in output_layers): \n",
    "                        output_layers.append(l)\n",
    "        \n",
    "        # Create bias vect for layer m\n",
    "        bias[m] = np.random.rand(layers[m - 1]) - 0.5  \n",
    "    \n",
    "    if num_layers_network not in output_layers:\n",
    "        output_layers.append(num_layers_network)\n",
    "        \n",
    "    for u in output_layers:\n",
    "        input_layersU_connect_to[u] = []\n",
    "        \n",
    "        for x in input_layers: \n",
    "            # If input layer in lfwd[x] \n",
    "            #  and connect x -> u has delay > 0 \n",
    "            #  and x not yet in inputlayersUconnto[u]\n",
    "            if  (u in layers_fwd_connect_layerM[x]) and \\\n",
    "                (np.any(np.array(delay_layerM_toL[x, u]) > 0)) and \\\n",
    "                (x not in input_layersU_connect_to[u]):\n",
    "                \n",
    "                input_layersU_connect_to[u].append(x)\n",
    "                \n",
    "    for x in range(1, num_layers_network + 1):\n",
    "        output_layersX_connect_to[x] = []\n",
    "        \n",
    "        for u in output_layers:\n",
    "            try:\n",
    "                # If connect u -> x has delay > 0\n",
    "                if np.any(np.array(delay_layerM_toL[x, u]) > 0): \n",
    "                    output_layersX_connect_to[x].append(u)\n",
    "            \n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    #Add to network\n",
    "    network['output_layers']               = output_layers\n",
    "    network['input_layers']                = input_layers\n",
    "    network['delay_layerM_toL']            = delay_layerM_toL\n",
    "    network['delay_input_layer1']          = delay_input_layer1\n",
    "    network['layers_bkwd_connect_layerM']  = layers_bkwd_connect_layerM\n",
    "    network['layers_fwd_connect_layerM']   = layers_fwd_connect_layerM\n",
    "    network['inputs_connect_layer1']       = inputs_connect_layer1\n",
    "    network['input_layersU_connect_to']    = input_layersU_connect_to\n",
    "    network['output_layersX_connect_to']   = output_layersX_connect_to\n",
    "    \n",
    "    network['w0'] = convert_matrices_to_vector(network, \n",
    "                                               input_weight_matrices, \n",
    "                                               connection_weight_matrices, \n",
    "                                               bias)\n",
    "    return network\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def convert_matrices_to_vector(network, input_weight_matrices, connection_weight_matrices, bias):\n",
    "    \"\"\"\n",
    "    Converts input weight matrices(IW), connection weight matrices(LW) and bias \n",
    "    vectors(b) to weight vector(w)\n",
    "    \"\"\"\n",
    "    \n",
    "    delay_layerM_toL           = network['delay_layerM_toL']    \n",
    "    delay_input_layer1         = network['delay_input_layer1']    \n",
    "    inputs_connect_layer1      = network['inputs_connect_layer1']     \n",
    "    layers_fwd_connect_layerM  = network['layers_fwd_connect_layerM']   \n",
    "    num_layers_network         = network['num_layers']     \n",
    "    weight_vect                = np.array([])\n",
    "    \n",
    "    # Input weights\n",
    "    for m in range(1, num_layers_network + 1): \n",
    "        if m == 1:\n",
    "            for i in inputs_connect_layer1[m]:\n",
    "                for d in delay_input_layer1[m, i]:\n",
    "                    weight_vect = np.append(weight_vect, \n",
    "                                            input_weight_matrices[m, i, d].flatten('F'))\n",
    "                    \n",
    "        # Internal connect weights\n",
    "        for l in layers_fwd_connect_layerM[m]:\n",
    "            for d in delay_layerM_toL[m, l]:\n",
    "                weight_vect = np.append(weight_vect, \n",
    "                                        connection_weight_matrices[m, l, d].flatten('F'))\n",
    "                \n",
    "        # Bias weights\n",
    "        weight_vect = np.append(weight_vect, bias[m])\n",
    "    \n",
    "    return weight_vect\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def RTRL(network, data):\n",
    "    \"\"\" \n",
    "    Jacobian Matrix    == derivatives of error with respect to weight vector\n",
    "    Mean Squared Error == MSE of network compared to training data\n",
    "    Error Vector       == difference of network output and target data\n",
    "    \"\"\"\n",
    "    \n",
    "    #data == training data\n",
    "    network_inputs               = data['inputs']      \n",
    "    network_outputs              = data['outputs']      \n",
    "    layer_outputs                = data['layer_outputs']\n",
    "    num_prev_data_pts            = data['q0'] \n",
    "    \n",
    "    delay_layerM_toL             = network['delay_layerM_toL']      \n",
    "    delay_input_layer1           = network['delay_input_layer1']\n",
    "    \n",
    "    inputs_connect_layer1        = network['inputs_connect_layer1']       \n",
    "    layers_fwd_connect_layerM    = network['layers_fwd_connect_layerM']     \n",
    "    layers_bkwd_connect_layerM   = network['layers_bkwd_connect_layerM']     \n",
    "    num_layers_network           = network['num_layers']   \n",
    "    \n",
    "    inputs                       = network['network'][0]   \n",
    "    outputs                      = network['network'][-1]\n",
    "    \n",
    "    layers                       = network['layers']  \n",
    "    input_layers                 = network['output_layers']       \n",
    "    output_layers                = network['input_layers']       \n",
    "    output_layersX_connect_to    = network['output_layersX_connect_to']   \n",
    "    \n",
    "    input_weight_matrices, connection_weight_matrices, bias = convert_vector_to_matrices(network)\n",
    "    \n",
    "    # 1. Calc network output\n",
    "    network_out, sum_output_layers, layer_outputs = get_network_output(\n",
    "        network_inputs, \n",
    "        network, \n",
    "        input_weight_matrices,                                     \n",
    "        connection_weight_matrices, \n",
    "        bias, \n",
    "        layer_outputs     = layer_outputs, \n",
    "        num_prev_data_pts = num_prev_data_pts)\n",
    "    \n",
    "    # 2. Calc cost func\n",
    "    error_matrix       = network_outputs - network_out                                             \n",
    "    error_vector       = np.reshape(error_matrix, (1, np.size(error_matrix)), order = 'F')[0]\n",
    "    mean_squared_error = np.dot(error_vector, error_vector.transpose())/float(len(error_vector))\n",
    "    \n",
    "    # 3. Backpropagation RTRL\n",
    "    num_of_input_datapts            = network_inputs.shape[1]                       \n",
    "    num_of_datapts_without_old_data = num_of_input_datapts - num_prev_data_pts \n",
    "    \n",
    "    \n",
    "    deriv_layer_outputsOfU_respect_bias_vect                     = {} \n",
    "    deriv_layer_outputsOfU_respect_input_weight_matrices         = {} \n",
    "    deriv_layer_outputsOfU_respect_connectection_weight_matrices = {}\n",
    "    deriv_layer_outputs_respect_weight_vect                      = {} \n",
    "    sensitivity_matrix                                           = {} \n",
    "    layersM_with_existing_sensitivity_matrix                     = {} \n",
    "    input_layersX_with_existing_sensitivity_matrix               = {} \n",
    "                                                                        \n",
    "    # Init\n",
    "    jacobian_matrix = np.zeros((num_of_datapts_without_old_data * layers[-1], \n",
    "                                network['num_weights']))\n",
    "    \n",
    "    for q in range(1, num_prev_data_pts + 1):\n",
    "        for u in input_layers:\n",
    "            deriv_layer_outputsOfU_respect_connectection_weight_matrices[q, u] = np.zeros(\n",
    "                (layers[u - 1], \n",
    "                network['num_weights']))\n",
    "    \n",
    "    #########\n",
    "    max_delay  = network['max_delay']\n",
    "    max_layers = len(layers)\n",
    "    #########\n",
    "    \n",
    "    # Begin RTRL\n",
    "    for q in range(num_prev_data_pts + 1, num_of_input_datapts + 1):\n",
    "        # Init, set needed for calculating sensitivities\n",
    "        input_layers_ = [] \n",
    "        for u in input_layers:\n",
    "            layersM_with_existing_sensitivity_matrix[u]       = []\n",
    "            input_layersX_with_existing_sensitivity_matrix[u] = []\n",
    "            deriv_layer_outputs_respect_weight_vect[q, u]     = 0\n",
    "        \n",
    "        # Calc sensitivity matrices, decrement m in backprop order\n",
    "        for m in range(num_layers_network, 0, -1): \n",
    "            for u in input_layers_:\n",
    "                # Sensitivity Matrix layer u -> m\n",
    "                sensitivity_matrix[q, u, m] = 0 \n",
    "                \n",
    "                for l in layers_bkwd_connect_layerM[m]:\n",
    "                    #recursive calculation of Sensitivity Matrix layer u -> m\n",
    "                    sensitivity_matrix[q, u, m] += np.dot(\n",
    "                        np.dot(sensitivity_matrix[q, u, l], connection_weight_matrices[l, m, 0]),\n",
    "                        np.diag(1 - (np.tanh(sum_output_layers[q, m])) ** 2)) \n",
    "                      \n",
    "                if m not in layersM_with_existing_sensitivity_matrix[u]:\n",
    "                    layersM_with_existing_sensitivity_matrix[u].append(m) \n",
    "                    \n",
    "                    if m in output_layers:\n",
    "                        input_layersX_with_existing_sensitivity_matrix[u].append(m)\n",
    "                        \n",
    "            if m in input_layers:\n",
    "                # Output layer is linear, no transfer function\n",
    "                if m == num_layers_network: \n",
    "                    sensitivity_matrix[q, m, m] = np.diag(np.ones(outputs)) \n",
    "                else:\n",
    "                    sensitivity_matrix[q, m, m] = np.diag(1 - (np.tanh(sum_output_layers[q, m])) \n",
    "                                                                              ** 2)\n",
    "                \n",
    "                # Add m to U'\n",
    "                input_layers_.append(m) \n",
    "                layersM_with_existing_sensitivity_matrix[m].append(m)\n",
    "                \n",
    "                if m in output_layers:\n",
    "                    input_layersX_with_existing_sensitivity_matrix[m].append(m)\n",
    "          \n",
    "        '''Calc derivs, static deriv calc'''\n",
    "        for u in sorted(input_layers): \n",
    "            # Static deriv vector: explicit deriv layer outputs w/ respect to weight vect\n",
    "            deriv_layer_outputs_respect_weight_vect_ = np.empty((layers[u - 1], 0))\n",
    "            \n",
    "            # Input weights\n",
    "            for m in range(1, num_layers_network + 1): \n",
    "                if m == 1:\n",
    "                    for i in inputs_connect_layer1[m]:\n",
    "                        for d in delay_input_layer1[m, i]:\n",
    "                            # If no sensivity matrix exists or d >= q: deriv is zero\n",
    "                            if ((q, u, m) not in sensitivity_matrix.keys()) or (d >= q): \n",
    "                                deriv_layer_outputsOfU_respect_input_weight_matrices[m, i, d] = \\\n",
    "                                    np.kron(network_inputs[:, q - d - 1].transpose(), \n",
    "                                    np.zeros((layers[u - 1], layers[m - 1])))\n",
    "                            else: \n",
    "                                deriv_layer_outputsOfU_respect_input_weight_matrices[m,i,d] = \\\n",
    "                                    np.kron(network_inputs[:, q - d - 1].transpose(), \n",
    "                                    sensitivity_matrix[q, u, m])\n",
    "\n",
    "                            # Append to static deriv vect\n",
    "                            deriv_layer_outputs_respect_weight_vect_ = np.append(\n",
    "                                deriv_layer_outputs_respect_weight_vect_, \n",
    "                                deriv_layer_outputsOfU_respect_input_weight_matrices[m, i, d], \n",
    "                                1) \n",
    "        \n",
    "                # Connect weights\n",
    "                for l in layers_fwd_connect_layerM[m]:\n",
    "                    for d in delay_layerM_toL[m, l]:\n",
    "                        # If no sensivity matrix exists or d >= q: deriv is zero\n",
    "                        if ((q, u, m) not in sensitivity_matrix.keys()) or (d >= q): \n",
    "                            deriv_layer_outputsOfU_respect_connectection_weight_matrices[m,l,d]=\\\n",
    "                                np.kron(layer_outputs[q, l].transpose(), np.zeros((\n",
    "                                            layers[u - 1], layers[m - 1])))\n",
    "                        else:\n",
    "                            deriv_layer_outputsOfU_respect_connectection_weight_matrices[m,l,d]=\\\n",
    "                                np.kron(layer_outputs[q - d, l].transpose(), \n",
    "                                        sensitivity_matrix[q, u, m]) \n",
    "\n",
    "                        # Append to static deriv vect\n",
    "                        deriv_layer_outputs_respect_weight_vect_ = np.append(\n",
    "                            deriv_layer_outputs_respect_weight_vect_, \n",
    "                            deriv_layer_outputsOfU_respect_connectection_weight_matrices[m,l,d], \n",
    "                            1) \n",
    "                        \n",
    "                # Bias weights\n",
    "                if ((q, u, m) not in sensitivity_matrix.keys()):\n",
    "                    # Deriv is zero\n",
    "                    deriv_layer_outputsOfU_respect_bias_vect[m] = np.zeros((layers[u - 1], \n",
    "                                                                            layers[m - 1])) \n",
    "                else:\n",
    "                    deriv_layer_outputsOfU_respect_bias_vect[m] = sensitivity_matrix[q, u, m] \n",
    "\n",
    "                # Append to static deriv vect\n",
    "                deriv_layer_outputs_respect_weight_vect_ = np.append(\n",
    "                                        deriv_layer_outputs_respect_weight_vect_, \n",
    "                                        deriv_layer_outputsOfU_respect_bias_vect[m], \n",
    "                                        1) \n",
    "            \n",
    "            '''Dynamic deriv calc'''\n",
    "            dyn_deriv_sum_allX = 0\n",
    "            for x in input_layersX_with_existing_sensitivity_matrix[u]:\n",
    "                # Sum of all u_\n",
    "                sum_u_ = 0 \n",
    "                \n",
    "                for u_ in output_layersX_connect_to[x]:\n",
    "                    # Sum of all d\n",
    "                    sum_d = 0 \n",
    "                    \n",
    "                    for d in delay_layerM_toL[x, u_]:\n",
    "                        # Delays > 0 and < q\n",
    "                        if (q - d > 0) and (d > 0): \n",
    "                            sum_d += np.dot(connection_weight_matrices[x, u_, d], \n",
    "                                            deriv_layer_outputs_respect_weight_vect[q - d, u_])\n",
    "\n",
    "                    sum_u_ += sum_d\n",
    "\n",
    "                if sum_u_ is not 0:\n",
    "                    # Sum up dynamic deriv\n",
    "                    dyn_deriv_sum_allX += np.dot(sensitivity_matrix[q, u, x], sum_u_) \n",
    "            \n",
    "            # Static + dynamic deriv, total deriv output layer u with respect to w\n",
    "            deriv_layer_outputs_respect_weight_vect[q, u] = \\\n",
    "                    deriv_layer_outputs_respect_weight_vect_ + dyn_deriv_sum_allX \n",
    "        # Jacobian matrix\n",
    "        jacobian_matrix[range(((q - num_prev_data_pts) - 1) * outputs, \n",
    "                              (q - num_prev_data_pts) * outputs), :] = \\\n",
    "            -deriv_layer_outputs_respect_weight_vect[q, num_layers_network]\n",
    "\n",
    "        ############!!!!#########!!!#################!@!@!@!@!#####\n",
    "        if q > max_delay:\n",
    "            new_dA_dw = {}\n",
    "            for dd in range(max_delay):\n",
    "                for ll in xrange(1,max_layers+1):\n",
    "                    new_dA_dw[(q-dd,ll)] = deriv_layer_outputs_respect_weight_vect[(q-dd,ll)]\n",
    "            deriv_layer_outputs_respect_weight_vect = new_dA_dw\n",
    "            sensitivity_matrix = {}\n",
    "        ############!!!!#########!!!#################!@!@!@!@!#####\n",
    "\n",
    "    return jacobian_matrix, mean_squared_error, error_vector\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def train_LM(nn_inputs, \n",
    "             nn_outputs, \n",
    "             net, \n",
    "             iteration_max = 10, \n",
    "             MSE_stop      = 1e-6, \n",
    "             damp_factor   = 3.0, \n",
    "             damp_const    = 10.0, \n",
    "             verbose       = False):\n",
    "    \"\"\"\n",
    "    Levenberg-Marquardt(LM) algorithm\n",
    "      - Least-squares estimation of nonlinear parameters\n",
    "      \n",
    "    Args:\n",
    "        iteration_max:  max # of iterations\n",
    "        MSE_stop:       termination error, training stops when MSE <= MSE_stop\n",
    "        damp_const:     constant to adapt damping factor of LM\n",
    "        damp_factor:    damping factor of LM\n",
    "    \n",
    "    Returns:\n",
    "        net:            trained Neural Network \n",
    "    \"\"\"\n",
    "    data, net = prepare_data(nn_inputs, nn_outputs, net)\n",
    "    \n",
    "    # Calc for first iteration\n",
    "    Jacobian, Mean_squared_error, error_vect = RTRL(net, data)\n",
    "    \n",
    "    # Vect for error history\n",
    "    iteration               = 0\n",
    "    ErrorHistory            = np.zeros(iteration_max + 1) \n",
    "    ErrorHistory[iteration] = Mean_squared_error\n",
    "    \n",
    "    if verbose:\n",
    "        print('Iteration: ',    iteration, \n",
    "              'Error: ',        Mean_squared_error, \n",
    "              'scale factor: ', damp_factor)\n",
    "\n",
    "    # Run loop until either max iterations or MSE_stop reached\n",
    "    while True:\n",
    "        #####\n",
    "        seconds, minutes, hours = 60, 60, 2\n",
    "        t_end = time.time() + (seconds * minutes * hours)\n",
    "        \n",
    "        JTJ = scipy.linalg.blas.dgemm(alpha=1.0, a=Jacobian.T, b=Jacobian.T, trans_b=True)\n",
    "        #####\n",
    "        weight_vect = net['weight_vect']\n",
    "        \n",
    "        # Repeat until optimizing step successful\n",
    "        while True:\n",
    "            gradient = np.dot(Jacobian.transpose(), error_vect)\n",
    "\n",
    "            # Calc scaled inverse Hessian\n",
    "            try:\n",
    "                #####\n",
    "                scaled_inv_hessian = scipy.linalg.inv(JTJ + damp_factor * \n",
    "                                                      np.eye(net['num_weights'])) \n",
    "                #####\n",
    "            except scipy.linalg.LinAlgError:\n",
    "                # Not invertible, do small step in gradient direction\n",
    "                weight_delta = 1.0 / 1e10 * gradient\n",
    "            else:\n",
    "                # Calc weight modification\n",
    "                weight_delta = np.dot(-scaled_inv_hessian, gradient)\n",
    "            \n",
    "            # New weight vect\n",
    "            net['weight_vect'] = weight_vect + weight_delta  \n",
    "            new_mean_squared_error = calc_error(net, data)\n",
    "            \n",
    "            # If optimization step successful, adapt scale factor, then go next iteration\n",
    "            if new_mean_squared_error < Mean_squared_error:\n",
    "                damp_factor = damp_factor / damp_const \n",
    "                break\n",
    "            elif time.time() > t_end:\n",
    "                return net\n",
    "            else:\n",
    "                damp_factor = damp_factor * damp_const\n",
    "        \n",
    "        # Calc for next iteration\n",
    "        Jacobian, Mean_squared_error, error_vect = RTRL(net, data)\n",
    "        iteration += 1\n",
    "        ErrorHistory[iteration] = Mean_squared_error\n",
    "        \n",
    "        if verbose:\n",
    "            print('Iteration: ',    iteration,\n",
    "                  'Error: ',        Mean_squared_error,\n",
    "                  'scale factor: ', damp_factor)\n",
    "\n",
    "        # Check if termination condition hit\n",
    "        if iteration >= iteration_max:\n",
    "            print('Max # of iterations reached')\n",
    "            break\n",
    "        elif Mean_squared_error <= MSE_stop:\n",
    "            print('Termination error reached')\n",
    "            break\n",
    "\n",
    "    net['ErrorHistory'] = ErrorHistory[:iteration]\n",
    "    return net\n",
    "\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def calc_error(network, data):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        MAE of network compared to training data\n",
    "    \"\"\"\n",
    "    \n",
    "    network_inputs    = data['inputs']\n",
    "    network_outputs   = data['outputs'] \n",
    "    layer_outputs     = data['layer_outputs']\n",
    "    num_prev_data_pts = data['q0'] \n",
    "    \n",
    "    input_weight_matrices, connection_weight_matrices, bias = convert_vector_to_matrices(network)\n",
    "    \n",
    "    network_out, sum_output_layers, layer_outputs = get_network_output(\n",
    "        network_inputs, \n",
    "        network, \n",
    "        input_weight_matrices,                                     \n",
    "        connection_weight_matrices, \n",
    "        bias, \n",
    "        layer_outputs     = layer_outputs, \n",
    "        num_prev_data_pts = num_prev_data_pts)\n",
    "\n",
    "    # Outputs_delta = error matrix\n",
    "    outputs_delta       = network_outputs - network_out \n",
    "    error_vect          = np.reshape(outputs_delta, (1, np.size(outputs_delta)), order='F')[0]\n",
    "    Mean_squared_error  = np.dot(error_vect, error_vect.transpose())/float(len(error_vect))\n",
    "\n",
    "    #return Mean_squared_error\n",
    "    return Mean_squared_error\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_learntest(num, key, out_num, net=False):\n",
    "    \"\"\"\n",
    "    Retrieve our dataframes with all of our indicators for our 6 helper indexes/funds that \n",
    "    help in the prediction process for a certain company we choose to predict on. The 6 \n",
    "    indexes are the Dow Jones Index, the S&P 500 Index, the Nasdaq Composite, \n",
    "    United States Oil Fund, the SPDR S&P 500 ETF, and SPDR Gold Shares. \n",
    "    \n",
    "    We take these dataframes, and concatenate to our stock we will predict for,\n",
    "    calling it with a stock key that we feed in through a parameter. \n",
    "    \n",
    "    We then adjust for missing dates that many of the 6 helper stocks lack but that many\n",
    "    stocks do have, removing those certain dates from our data. We then take the list of \n",
    "    indicators we are going to use by loading a list with all of the 474 names of \n",
    "    indicators. There are over 2000 possible indicators, we chose only 474 for memory and\n",
    "    computation costs of using more than that. \n",
    "    \n",
    "    You can change the indicators as you like, just as long as it's a list of indicator \n",
    "    names as strings. We get our 474 indicator dataframe then set our output as our stock\n",
    "    choices return values at 1 minute intervals. \n",
    "    \n",
    "    We then set up our learn/test lists which have to be lists due to better efficiency\n",
    "    using numpy rather than pandas dataframes.\n",
    "    \n",
    "    Return the inputs, outputs, test inputs, test outputs, and the neural net if we already\n",
    "    have one set up.\n",
    "    \"\"\"\n",
    "    # Grab our pickled files from our NewBase/STOCKSYMBOL/ directories\n",
    "    # the num is referring to one of fifteen parts to each company indicator dataframe\n",
    "    opp       = open('NewBase/^GSPC/^GSPC_df'+str(num)+'.pickle', 'rb')\n",
    "    opp2      = open('NewBase/^IXIC/^IXIC_df'+str(num)+'.pickle', 'rb')\n",
    "    opp3      = open('NewBase/^DJI/^DJI_df'+str(num)+'.pickle', 'rb')\n",
    "    opp4      = open('NewBase/GLD/GLD_df'+str(num)+'.pickle', 'rb')\n",
    "    opp5      = open('NewBase/USO/USO_df'+str(num)+'.pickle', 'rb')\n",
    "    opp6      = open('NewBase/SPY/SPY_df'+str(num)+'.pickle', 'rb')\n",
    "    opp7      = open('NewBase/'+key+'/'+key+'_df'+str(num)+'.pickle', 'rb')\n",
    "    opp8      = open('HLC/'+key+'.pickle', 'rb')\n",
    "    gspc      = pickle.load(opp)\n",
    "    ixic      = pickle.load(opp2)\n",
    "    dji       = pickle.load(opp3)\n",
    "    gld       = pickle.load(opp4)\n",
    "    uso       = pickle.load(opp5)\n",
    "    spy       = pickle.load(opp6)\n",
    "    df        = pickle.load(opp7)\n",
    "    hlc       = pickle.load(opp8)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    opp3.close()\n",
    "    opp4.close()\n",
    "    opp5.close()\n",
    "    opp6.close()\n",
    "    opp7.close()\n",
    "    opp8.close()\n",
    "    \n",
    "    start_date = str(df.index[0])[:10]\n",
    "    end_date   = str(df.index[-1])[:10]\n",
    "    closes_df  = hlc['Closes'].loc[start_date:end_date]\n",
    "\n",
    "    # Also grab the indicator list that contains all the indicators we'll be using from\n",
    "    # our indicator dataframes since we can't process that many indicators\n",
    "    try:\n",
    "        opp9      = open('Pickles/final_lst474'+key+'.pickle','rb')\n",
    "        final_lst = pickle.load(opp9)\n",
    "        opp9.close()\n",
    "    except:\n",
    "        final_lst = create_indicator_list(key)\n",
    "        pass\n",
    "\n",
    "    # Combine the dataframes into a single dataframe, making sure there aren't any\n",
    "    # duplicate rows which happened curiously a few times during testing.\n",
    "    comp_lst = [gspc,ixic,dji,gld,uso,spy,df,closes_df]\n",
    "    for x in range(8):\n",
    "        comp_lst[x] = comp_lst[x].T.groupby(level=0).first().T\n",
    "    new_df = pd.concat(comp_lst, axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Remove these dates because some dataframes contain data from them but others\n",
    "    # don't and could affect calculations with up to 390 consecutive incorrect values\n",
    "    dates = ['2013-08-28', '2013-10-28', '2014-02-12', '2014-02-18', '2014-10-02', \n",
    "             '2014-10-06', '2014-10-08', '2014-10-09', '2014-10-13', '2014-10-14', \n",
    "             '2014-10-15', '2014-10-20', '2015-01-14', '2015-04-21', '2015-05-18', \n",
    "             '2015-06-08', '2015-07-08', '2015-08-20', '2015-08-31', '2015-09-08', \n",
    "             '2016-02-08', '2016-03-15', '2016-03-21', '2016-03-22', '2016-04-13', \n",
    "             '2016-06-15', '2015-03-30', '2015-05-05', '2014-02-25']\n",
    "    for each in dates:\n",
    "        try:\n",
    "            nums   = new_df.index.get_loc(each)\n",
    "            new_df = new_df.drop(new_df.index[nums.start:nums.stop])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    new_df        = new_df.T.groupby(level=0).first().T\n",
    "    # Create the output indicator name so we can call it from our indicator df\n",
    "    rets_name     = key+'_rets'+str(out_num)\n",
    "    # Get a dataframe with just our indicators for training/testing, and then\n",
    "    # the output column, learning_rets\n",
    "    result        = new_df[final_lst]\n",
    "    learning_rets = new_df[rets_name]\n",
    "    closes_df     = new_df['Closes']\n",
    "\n",
    "    # Adjust the input/output so inputs/output are in sync. If not, we'd be predicting\n",
    "    # for for the incorrect returns.\n",
    "    length        = len(result)\n",
    "    ret_lst       = [1,2,3,4,5,10,14,16,18,20,25,30,40,50,75,100,125,150,200,250,300]\n",
    "    result        = result.iloc[:length-ret_lst[out_num]]\n",
    "    learning_rets = learning_rets.iloc[ret_lst[out_num]:]\n",
    "    \n",
    "    # If already have a neural net trained on previous data, net will be true. Then we\n",
    "    # set our net_var to the already trained net in prep to train further with it.\n",
    "    net_var = []\n",
    "    if net == True:\n",
    "        opp = open('Pickles/net474attrs'+key+'_'+str(out_num)+'.pickle','rb')\n",
    "        net_var = pickle.load(opp)\n",
    "        opp.close()\n",
    "    \n",
    "    # Set up our test input/output dataframes\n",
    "    result_test   = result.copy(deep=True)\n",
    "    testing_rets  = learning_rets.copy(deep=True)\n",
    "    \n",
    "    # Leave the last 405 records for testing net w/ test data\n",
    "    sml = 0\n",
    "    mid = len(result) - 405\n",
    "    \n",
    "    # Convert dataframes to numpy arrays for speed/efficiency purposes\n",
    "    learning_outputs = learning_rets.iloc[sml:mid]\n",
    "    testing_outputs  = testing_rets.iloc[mid:mid+400]\n",
    "    learning_inputs  = result.iloc[sml:mid]\n",
    "    testing_inputs   = result_test.iloc[mid:mid+400]\n",
    "    testing_start_dt = testing_inputs.index[0]\n",
    "\n",
    "    inputs           = learning_inputs.values.T\n",
    "    outputs          = learning_outputs.values.T\n",
    "    test_inputs      = testing_inputs.values.T\n",
    "    test_outputs     = testing_outputs.values.T\n",
    "\n",
    "    print mid\n",
    "    return inputs,outputs,test_inputs,test_outputs,key,net_var,testing_start_dt,closes_df\n",
    "\n",
    "def create_indicator_list(nm_key):\n",
    "    lst = [nm_key,'^GSPC','^IXIC','^DJI','GLD','USO','SPY']\n",
    "    lst2 = ['rsi', 'vol', 'sma', 'cci', 'per', 'mom', 'bol', \n",
    "            'aro', 'mac', 'mactwo', 'adx', 'kdo', 'rets']\n",
    "    lst3 = ['rsi', 'vol', 'sma', 'cci', 'per', 'mom', 'bol', \n",
    "            'aro', 'mac', 'mactwo', 'adx', 'kdo']\n",
    "    test = []\n",
    "    for key in lst:\n",
    "        for nm in lst2:\n",
    "            name = key+'_'+nm+'0'\n",
    "            name2 = key+'_'+nm+'1'\n",
    "            name3 = key+'_'+nm+'2'\n",
    "            if (name!=nm_key+'_rets0')or(name2!=nm_key+'_rets1')or(name3!=nm_key+'_rets2'):\n",
    "                test.append(name)\n",
    "                test.append(name2)\n",
    "                test.append(name3)\n",
    "\n",
    "    for nm in lst3:\n",
    "        for x in xrange(3,20):\n",
    "            test.append(nm_key+'_'+nm+str(x))\n",
    "\n",
    "    opp = open('Pickles/final_lst474'+nm_key+'.pickle','wb')\n",
    "    pickle.dump(test, opp)\n",
    "    opp.close()\n",
    "    return test\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def start_training(inputs, outputs, key, output_num, net=[]):\n",
    "    \"\"\"\n",
    "    If we are using an already created neural network and training futher on it,\n",
    "    we'll use that to train further, else we create a neural network with 474 \n",
    "    inputs, 9 hidden neurons, and 1 output.\n",
    "    \"\"\"\n",
    "    # If we don't already have a trained network for further training, create new one\n",
    "    if net == []:\n",
    "        net = create_neural_network([474,9,1], \n",
    "                    delay_input    = [0,1,2],        \n",
    "                    delay_internal = [1,2],         \n",
    "                    delay_output   = [1,2])\n",
    "    \n",
    "    # Return our new trained network\n",
    "    net = train_LM(inputs, outputs, net, verbose=True, iteration_max=10, MSE_stop=1e-10)\n",
    "\n",
    "    # Dump that network into a pickled file\n",
    "    opp = open('Pickles/net474attrs'+key+'_'+str(output_num)+'.pickle','wb')\n",
    "    pickle.dump(net, opp)\n",
    "    opp.close()\n",
    "    \n",
    "    # Return net for testing/graphing\n",
    "    return net\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def graph_predictions(test_inputs, test_outputs, net, beg, end, inputs=None, outputs=None,):\n",
    "    \"\"\"\n",
    "    Graph the predicted stock return value compared to the actual return value.\n",
    "    beg is the row your testing, end is the ending row. For example, if \n",
    "    beg == 0, and end == 50, we'll look at the predictions for the first 50\n",
    "    rows of the test inputs.\n",
    "    \"\"\"\n",
    "    if inputs != None:\n",
    "        previous_inputs  = np.array([]).reshape(0,5)\n",
    "        previous_outputs = np.array([]).reshape(0,5)\n",
    "        length = len(inputs[0])\n",
    "        for prev in (i[length-5:] for i in inputs):\n",
    "            previous_inputs = np.vstack([previous_inputs, prev])\n",
    "        previous_outputs = np.vstack([previous_outputs, outputs[length-5:]])\n",
    "    \n",
    "        # Create our predictions\n",
    "        ytest  = NNOut(test_inputs, net, P0=previous_inputs, Y0=previous_outputs[0])\n",
    "    else:\n",
    "        ytest  = NNOut(test_inputs, net)\n",
    "    \n",
    "    # On how much data will we graph\n",
    "    total  = end - beg\n",
    "    \n",
    "    # Graph our predicted vs actual output\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(total), ytest[beg:end], 'b-', label='test')\n",
    "    plt.plot(range(total), test_outputs[beg:end], 'r', label='real')\n",
    "    fig.suptitle('Predicted VS Actual Output', fontsize=20)\n",
    "    plt.xlabel('Timestamp', fontsize=18)\n",
    "    plt.ylabel('Return Value', fontsize=16)\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Predicted Output')\n",
    "    red_patch  = mpatches.Patch(color='red', label='Actual Output')\n",
    "    plt.legend(handles=[red_patch, blue_patch])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_prediction_results(test_inputs, test_outputs, net, test_dt, \n",
    "                           closes_df, key, inputs=None, outputs=None):\n",
    "    \"\"\"\n",
    "    Here we'll take our trained network and test our predictions and for each\n",
    "    interval make a choice to either buy, short, or stay. We'll then take those\n",
    "    decisions and test them against the actual results in return values and either\n",
    "    add or subtract to our profitability variable. Print the results after this\n",
    "    is done.\n",
    "    \"\"\"\n",
    "    guess_list = []\n",
    "    dff  = pd.DataFrame(columns=['cls','out','chg','gue','res','rgue','res2'],\n",
    "                        index=[range(len(test_outputs))])\n",
    "    rand = {True:'buy',False:'short'}\n",
    "    result,result2,count = 0,0,0\n",
    "    correct,correct2,incorrect,incorrect2 = 0.,0.,0.,0.\n",
    "    #closes = closes_df.loc[test_dt:].iloc[:20098].values.tolist()\n",
    "    closes = closes_df.loc[test_dt:].iloc[:len(closes_df)-3406].values.tolist()\n",
    "    \n",
    "    # Make our predictions using our trained network and test inputs, with optional\n",
    "    # addition of using the previous 5 inputs/outputs.\n",
    "    if inputs != None:\n",
    "        previous_inputs  = np.array([]).reshape(0,5)\n",
    "        previous_outputs = np.array([]).reshape(0,5)\n",
    "        length = len(inputs[0])\n",
    "        for prev in (i[length-5:] for i in inputs):\n",
    "            previous_inputs = np.vstack([previous_inputs, prev])\n",
    "        previous_outputs = np.vstack([previous_outputs, outputs[length-5:]])\n",
    "        ytest  = NNOut(test_inputs, net, P0=previous_inputs, Y0=previous_outputs[0])\n",
    "    else:\n",
    "        ytest  = NNOut(test_inputs, net)\n",
    "    \n",
    "    # For each return guess value, if its expected to be greater, we choose buy\n",
    "    # if less, we choose short, if 0, we choose stay\n",
    "    for return_guess in ytest:\n",
    "        if return_guess > 0:\n",
    "            guess_list.append('buy')\n",
    "        elif return_guess < 0:\n",
    "            guess_list.append('short')\n",
    "        else:\n",
    "            guess_list.append('stay')\n",
    "    \n",
    "    # If we choose the right choice, we gain that amount of price difference\n",
    "    # if not, we lose that price difference. If the actual output is 0, regardless\n",
    "    # of what we choose, we stay the same.\n",
    "    for o,g,c in zip(test_outputs, guess_list, closes):\n",
    "        price_change = 1000 * o * c\n",
    "        if g == 'buy':\n",
    "            if o > 0:\n",
    "                result += price_change\n",
    "                correct += 1.\n",
    "            else:\n",
    "                result -= abs(price_change)\n",
    "                if o < 0:\n",
    "                    incorrect += 1\n",
    "        elif g == 'short':\n",
    "            if o < 0:\n",
    "                result += abs(price_change)\n",
    "                correct += 1\n",
    "            else:\n",
    "                result -= price_change\n",
    "                if o > 0:\n",
    "                    incorrect += 1\n",
    "\n",
    "        rg = rand[bool(random.getrandbits(1))]\n",
    "        if rg == 'buy':\n",
    "            if o > 0:\n",
    "                result2 += price_change\n",
    "                correct2 += 1.\n",
    "            else:\n",
    "                result2 -= abs(price_change)\n",
    "                if o < 0:\n",
    "                    incorrect2 += 1.\n",
    "        elif rg == 'short':\n",
    "            if o < 0:\n",
    "                result2 += abs(price_change)\n",
    "                correct2 += 1\n",
    "            else:\n",
    "                result2 -= price_change\n",
    "                if o > 0:\n",
    "                    incorrect2 += 1\n",
    "\n",
    "        dff.iloc[count] = [c,o,price_change,g,result,rg,result2]\n",
    "        count += 1\n",
    "                \n",
    "    if result >= 0:\n",
    "        print 'Success, profitable:', result, '% correct:', correct/(correct+incorrect)\n",
    "    else:\n",
    "        print 'Fail, unprofitable:', result, '% correct:', correct/(correct+incorrect)\n",
    "    if result2 >= 0:\n",
    "        print 'Random success, profitable:', result2, '% correct:', correct2/(correct2+incorrect2)\n",
    "    else:\n",
    "        print 'Random fail, unprofitable:', result2, '% correct:', correct2/(correct2+incorrect2)\n",
    "    return dff\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The first parameter is the piece of indicators to work on, for example 0, is the oldest data,\n",
    "# while 14 is the newest data. For testing you can train a network with a single piece or use\n",
    "# all of them, simply adding 1 after each testing round. It will automatically utilize \n",
    "# the trained network by setting net=True after you've trained the network for at least a \n",
    "# single piece. The second parameter is the stock indicator symbol that you are training for. The\n",
    "# third parameter is the return length you're predicting for, 0-31, with 0 being, 1 minute ahead,\n",
    "# up to 31 being 125 days into the future. 4th optional parameter is net=True or False, false, if\n",
    "# you haven't trained on at least one piece of the indicator dataframe for that company, true if\n",
    "# you have.\n",
    "inputs,outputs,test_inputs,test_outputs,key,net,testing_dt,closes_df=create_learntest(0,'SIRI',0)\n",
    "#inputs,outputs,test_inputs,test_outputs,key,net,testing_dt,closes_df=create_learntest(1,'SIRI',0,net=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start training using the inputs, outputs, stock symbol, the return period your predicting for,\n",
    "# and the net. If it's your first time training and you don't have a net yet, net is passed in as\n",
    "# [] from the create_learntest() function and it automatically sets up a net for you in this \n",
    "# function when you pass that.\n",
    "net = start_training(inputs, outputs, key, 0, net=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will graph 100 stock minutes of test output for you\n",
    "graph_predictions(test_inputs, test_outputs, net, 10, 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will return a dataframe containing your guess, the outcomes, and profitability\n",
    "dff = get_prediction_results(test_inputs, test_outputs, net, testing_dt, closes_df, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
