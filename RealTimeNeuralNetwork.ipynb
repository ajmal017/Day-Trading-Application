{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg.blas\n",
    "import datetime\n",
    "import scipy\n",
    "import time\n",
    "from dateutil import rrule \n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def NYSE_holidays2(a, b): \n",
    "    # Generate ruleset for holiday observances on the NYSE \n",
    "    rs = rrule.rruleset()\n",
    "    \n",
    "    # Include all potential holiday observances \n",
    "    ###############################################\n",
    "    \n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=31, \n",
    "                         byweekday=rrule.FR))               \n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=1))\n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=2, \n",
    "                         byweekday=rrule.MO))                    \n",
    "    # MLK Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,\n",
    "                         byweekday=rrule.MO(3)))                            \n",
    "    # Washington's Bday\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=2,\n",
    "                         byweekday=rrule.MO(3)))                          \n",
    "    # Good Friday\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,byeaster=-2)) \n",
    "    # Memorial Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=5, \n",
    "                         byweekday=rrule.MO(-1)))                         \n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=3, \n",
    "                         byweekday=rrule.FR))              \n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=4))\n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=5, \n",
    "                         byweekday=rrule.MO))               \n",
    "    # Labor Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=9, \n",
    "                         byweekday=rrule.MO(1)))                          \n",
    "    # Thanksgiving Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=11, \n",
    "                         byweekday=rrule.TH(4)))                          \n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=24, \n",
    "                         byweekday=rrule.FR))                \n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=25))\n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=26, \n",
    "                         byweekday=rrule.MO))                \n",
    "    ######################################################\n",
    "    \n",
    "    # Exclude potential holidays that fall on weekends \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,until=b,\n",
    "                          byweekday=(rrule.SA,rrule.SU))) \n",
    "    return rs \n",
    "\n",
    "def NYSE_tradingdays2(a, b):\n",
    "    # Generate ruleset for NYSE trading days\n",
    "    rs = rrule.rruleset() \n",
    "    rs.rrule(rrule.rrule(rrule.DAILY,dtstart=a,until=b)) \n",
    "    \n",
    "    # Exclude weekends and holidays \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,byweekday=(rrule.SA,rrule.SU)))\n",
    "    rs.exrule(NYSE_holidays2(a, b)) \n",
    "    \n",
    "    return rs\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network(neural_network, delay_input=[0], delay_internal=[], delay_output=[]):\n",
    "    \"\"\" Create Neural Network\n",
    "    Example: \n",
    "      network         = [2,3,4,1] network w/ 2 inputs, 2 hidden layers w/ 3 and 4 neurons, \n",
    "                                  and 1 linear output layer                             \n",
    "      delay_input     = [0,1,5] will use inputs of timestep t, t-1 and t-5\n",
    "      delay_internal  = [1,2,5] adds recurrent connection for output of each layer \n",
    "                                in timestep t-1, t-2 and t-5\n",
    "      delay_output    = [1,3,4] adds recurrent connection for output of each layer in \n",
    "                                timestep t-1, t-3 and t-5\n",
    "    \n",
    "    Args:\n",
    "        network:        structure of the neural network [I HL1 HL2 ... HLN OL]\n",
    "                        number of layers is the length of the list-1\n",
    "                        number neurons in each layer is the given number\n",
    "    \n",
    "        delay_input:    Time delays for NN inputs. \n",
    "                        To use only the input of timestep t dIn = [0]\n",
    "            \n",
    "        delay_internal: Time delays for recurrent internal connections of NN.\n",
    "                        dIntern has to be greater than zero (layer output at timestep t-x)!\n",
    "                        if non-empty list given, recurrent connection from every layer \n",
    "                        to itself and every layer before is added\n",
    "            \n",
    "        delay_output:   Time delays for recurrent connections of output to first hidden \n",
    "                        layer. dOut has to be greater than zero (output at timestep t-x)!\n",
    "                        if non-empty list given, recurrent connection from NN output to first \n",
    "                        hidden layer is added \n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "        net: untrained neural network \n",
    "    \"\"\"\n",
    "    \n",
    "    network                = {}          \n",
    "    \n",
    "    network['delay']       = {'input'    :delay_input, \n",
    "                              'internal' :delay_internal, \n",
    "                              'output'   :delay_output}   \n",
    "    \n",
    "    #Structure\n",
    "    network['network']     = neural_network  \n",
    "    \n",
    "    network['num_layers']  = len(neural_network) - 1  \n",
    "    \n",
    "    #structure without inputs\n",
    "    network['layers']      = neural_network[1:] \n",
    "    \n",
    "    #maximum time delay\n",
    "    network['max_delay']   = max( max(delay_input, delay_internal, delay_output) )\n",
    "    \n",
    "    #initialize random weight vector and specify sets\n",
    "    network                = create_weight_vector(network)    \n",
    "    \n",
    "    #weight vector used for calculation\n",
    "    network['weight_vect'] = network['w0'].copy()\n",
    "    \n",
    "    #number of weights\n",
    "    network['num_weights'] = len(network['w0'])           \n",
    "    \n",
    "    return network\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_weight_vector(network):\n",
    "    \"\"\"\n",
    "    Creates random weight vector of NN and defines sets needed for derivative calculation\n",
    "    \n",
    "    Returns: \n",
    "        neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    num_layers_network  = network['num_layers']   \n",
    "    layers              = network['layers'] \n",
    "    delay               = network['delay'] \n",
    "    inputs              = network['network'][0]\n",
    "    \n",
    "    # Input layers or layers w/ internal delay > 0\n",
    "    input_layers     = [] \n",
    "    # Output of layer used for cost func calc or added to input layer w/ delay > 1\n",
    "    output_layers    = []  \n",
    "    \n",
    "    # Connection weight matrix layer m -> layer l w/ delay d\n",
    "    connection_weight_matrices  = {}  \n",
    "    bias                        = {}\n",
    "    input_weight_matrices       = {}\n",
    "    layers_bkwd_connect_layerM  = {} \n",
    "    layers_fwd_connect_layerM   = {} \n",
    "    delay_layerM_toL            = {}   \n",
    "    input_layersU_connect_to    = {}            \n",
    "    output_layersX_connect_to   = {}\n",
    "            \n",
    "    '''Inputs'''\n",
    "    inputs_connect_layer1      = {} \n",
    "    inputs_connect_layer1[1]   = [1]           \n",
    "    delay_input_layer1         = {}\n",
    "    delay_input_layer1[1, 1]   = delay['input']\n",
    "  \n",
    "    for x in delay_input_layer1[1, 1]:\n",
    "        # Input-weight matrix set to random values [-0.5,0.5]\n",
    "        input_weight_matrices[1, 1, x] = np.random.rand(layers[0], inputs) - 0.5  \n",
    "    \n",
    "    # First layer is input layer\n",
    "    input_layers.append(1)  \n",
    "    \n",
    "    '''Internal Connection Weight Matrices'''\n",
    "    for m in range(1, num_layers_network + 1):\n",
    "        layers_bkwd_connect_layerM[m] = []     \n",
    "        layers_fwd_connect_layerM[m]  = []\n",
    "            \n",
    "        # Forward connects\n",
    "        if m > 1:\n",
    "            l = m - 1\n",
    "            # No delay for forward connects\n",
    "            delay_layerM_toL[m, l]              = [0]                                            \n",
    "            connection_weight_matrices[m, l, 0] = np.random.rand(layers[m - 1], \n",
    "                                                                 layers[l - 1]) - 0.5 \n",
    "            \n",
    "            layers_bkwd_connect_layerM[l].append(m)  \n",
    "            layers_fwd_connect_layerM[m].append(l)      \n",
    "    \n",
    "        # Recursive connects\n",
    "        for l in range(m, num_layers_network + 1):\n",
    "            if (m == 1) and (l == num_layers_network):            \n",
    "                # Delays from output to first layer\n",
    "                delay_layerM_toL[m, l] = delay['output']          \n",
    "            else:\n",
    "                # Internal delays\n",
    "                delay_layerM_toL[m, l] = delay['internal']       \n",
    "                \n",
    "            # All delays for connect l->m    \n",
    "            for d in delay_layerM_toL[m, l]:                                                     \n",
    "                connection_weight_matrices[m, l, d] = np.random.rand(layers[m - 1], \n",
    "                                                                     layers[l - 1]) - 0.5\n",
    "                \n",
    "                # Add if haven't yet\n",
    "                if (l not in layers_fwd_connect_layerM[m]): \n",
    "                    layers_fwd_connect_layerM[m].append(l)  \n",
    "                    \n",
    "                # If recurrent connect\n",
    "                if (l >= m) and(d > 0): \n",
    "                    if (m not in input_layers):  \n",
    "                        input_layers.append(m) \n",
    "                        \n",
    "                    if (l not in output_layers): \n",
    "                        output_layers.append(l)\n",
    "        \n",
    "        # Create bias vect for layer m\n",
    "        bias[m] = np.random.rand(layers[m - 1]) - 0.5  \n",
    "    \n",
    "    if num_layers_network not in output_layers:\n",
    "        output_layers.append(num_layers_network)\n",
    "        \n",
    "    for u in output_layers:\n",
    "        input_layersU_connect_to[u] = []\n",
    "        \n",
    "        for x in input_layers: \n",
    "            # If input layer in lfwd[x] \n",
    "            #  and connect x -> u has delay > 0 \n",
    "            #  and x not yet in inputlayersUconnto[u]\n",
    "            if  (u in layers_fwd_connect_layerM[x]) and \\\n",
    "                (np.any(np.array(delay_layerM_toL[x, u]) > 0)) and \\\n",
    "                (x not in input_layersU_connect_to[u]):\n",
    "                \n",
    "                input_layersU_connect_to[u].append(x)\n",
    "                \n",
    "    for x in range(1, num_layers_network + 1):\n",
    "        output_layersX_connect_to[x] = []\n",
    "        \n",
    "        for u in output_layers:\n",
    "            try:\n",
    "                # If connect u -> x has delay > 0\n",
    "                if np.any(np.array(delay_layerM_toL[x, u]) > 0): \n",
    "                    output_layersX_connect_to[x].append(u)\n",
    "            \n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    #Add to network\n",
    "    network['output_layers']               = output_layers\n",
    "    network['input_layers']                = input_layers\n",
    "    network['delay_layerM_toL']            = delay_layerM_toL\n",
    "    network['delay_input_layer1']          = delay_input_layer1\n",
    "    network['layers_bkwd_connect_layerM']  = layers_bkwd_connect_layerM\n",
    "    network['layers_fwd_connect_layerM']   = layers_fwd_connect_layerM\n",
    "    network['inputs_connect_layer1']       = inputs_connect_layer1\n",
    "    network['input_layersU_connect_to']    = input_layersU_connect_to\n",
    "    network['output_layersX_connect_to']   = output_layersX_connect_to\n",
    "    \n",
    "    network['w0'] = convert_matrices_to_vector(network, \n",
    "                                               input_weight_matrices, \n",
    "                                               connection_weight_matrices, \n",
    "                                               bias)\n",
    "    return network\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def convert_matrices_to_vector(network, input_weight_matrices, connection_weight_matrices, bias):\n",
    "    \"\"\"\n",
    "    Converts input weight matrices(IW), connection weight matrices(LW) and bias \n",
    "    vectors(b) to weight vector(w)\n",
    "    \"\"\"\n",
    "    \n",
    "    delay_layerM_toL           = network['delay_layerM_toL']    \n",
    "    delay_input_layer1         = network['delay_input_layer1']    \n",
    "    inputs_connect_layer1      = network['inputs_connect_layer1']     \n",
    "    layers_fwd_connect_layerM  = network['layers_fwd_connect_layerM']   \n",
    "    num_layers_network         = network['num_layers']     \n",
    "    weight_vect                = np.array([])\n",
    "    \n",
    "    # Input weights\n",
    "    for m in range(1, num_layers_network + 1): \n",
    "        if m == 1:\n",
    "            for i in inputs_connect_layer1[m]:\n",
    "                for d in delay_input_layer1[m, i]:\n",
    "                    weight_vect = np.append(weight_vect, \n",
    "                                            input_weight_matrices[m, i, d].flatten('F'))\n",
    "                    \n",
    "        # Internal connect weights\n",
    "        for l in layers_fwd_connect_layerM[m]:\n",
    "            for d in delay_layerM_toL[m, l]:\n",
    "                weight_vect = np.append(weight_vect, \n",
    "                                        connection_weight_matrices[m, l, d].flatten('F'))\n",
    "                \n",
    "        # Bias weights\n",
    "        weight_vect = np.append(weight_vect, bias[m])\n",
    "    \n",
    "    return weight_vect\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def convert_vector_to_matrices(network):\n",
    "    \"\"\"\n",
    "    Converts weight vector w to Input Weight matrices IW, connection weight \n",
    "    matrices LW and bias vectors b\n",
    "    \"\"\"\n",
    "    \n",
    "    delay_layerM_toL            = network['delay_layerM_toL']       \n",
    "    delay_input_layer1          = network['delay_input_layer1']       \n",
    "    inputs_connect_layer1       = network['inputs_connect_layer1']        \n",
    "    layers_fwd_connect_layerM   = network['layers_fwd_connect_layerM']      \n",
    "    num_layers_network          = network['num_layers']        \n",
    "    layers                      = network['layers']   \n",
    "    inputs                      = network['network'][0]    \n",
    "    weight_vect_temp            = network['weight_vect'].copy() \n",
    "    input_weight_matrices       = {}              \n",
    "    connection_weight_matrices  = {}              \n",
    "    bias                        = {}              \n",
    "    \n",
    "    for m in range(1, num_layers_network + 1):\n",
    "        # Input weights\n",
    "        if m == 1:\n",
    "            for i in inputs_connect_layer1[m]:\n",
    "                for d in delay_input_layer1[m, i]:\n",
    "                    weight_i                       = inputs * layers[m - 1]\n",
    "                    vec                            = weight_vect_temp[0 : weight_i]\n",
    "                    weight_vect_temp               = weight_vect_temp[weight_i :]\n",
    "                    \n",
    "                    input_weight_matrices[m, i, d] = np.reshape(vec, (layers[m - 1], \n",
    "                                                                      len(vec) / layers[m - 1]), \n",
    "                                                                      order = 'F')\n",
    "        \n",
    "        # Internal connect weights\n",
    "        for l in layers_fwd_connect_layerM[m]:\n",
    "            for d in delay_layerM_toL[m, l]:\n",
    "                weight_i                            = layers[l - 1] * layers[m - 1]\n",
    "                vec                                 = weight_vect_temp[0 : weight_i]\n",
    "                weight_vect_temp                    = weight_vect_temp[weight_i :]\n",
    "                \n",
    "                connection_weight_matrices[m, l, d] = np.reshape(vec, (layers[m - 1], \n",
    "                                                                       len(vec) / layers[m - 1]), \n",
    "                                                                       order = 'F')\n",
    "        \n",
    "        # Bias vector of layer m\n",
    "        weight_i         = layers[m - 1]\n",
    "        bias[m]          = weight_vect_temp[0 : weight_i]\n",
    "        weight_vect_temp = weight_vect_temp[weight_i :]\n",
    "\n",
    "    return input_weight_matrices, connection_weight_matrices, bias\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def get_network_output(network_inputs, \n",
    "                       network, \n",
    "                       input_weight_matrices, \n",
    "                       connection_weight_matrices, \n",
    "                       bias, \n",
    "                       layer_outputs = {}, \n",
    "                       num_prev_data_pts = 0):\n",
    "    \"\"\"\n",
    "    Calculates network output for given inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    delay_layerM_toL              = network['delay_layerM_toL']                                 \n",
    "    delay_input_layer1            = network['delay_input_layer1']                                 \n",
    "    inputs_connect_layer1         = network['inputs_connect_layer1']                                  \n",
    "    layers_fwd_connect_layerM     = network['layers_fwd_connect_layerM']                                \n",
    "    num_layers_network            = network['num_layers']                                  \n",
    "    outputs                       = network['network'][-1]                             \n",
    "    sum_output_layers             = {}                                        \n",
    "    num_of_input_datapts          = network_inputs.shape[1]                        \n",
    "    network_output                = np.zeros((outputs, num_of_input_datapts)) \n",
    "    \n",
    "    # For all datapoints\n",
    "    for q in range(num_prev_data_pts + 1, num_of_input_datapts + 1): \n",
    "        layer_outputs[q, 1] = 0\n",
    "        # For all layers\n",
    "        for m in range(1, num_layers_network + 1): \n",
    "            # Sum output datapoint q, layer m\n",
    "            sum_output_layers[q, m] = 0         \n",
    "            \n",
    "            # Input weights\n",
    "            if m == 1:\n",
    "                for i in inputs_connect_layer1[m]:\n",
    "                    for d in delay_input_layer1[m, i]:\n",
    "                        if (q - d) > 0:\n",
    "                            sum_output_layers[q, m] += np.dot(input_weight_matrices[m, i, d], \n",
    "                                                              network_inputs[:, q - d - 1])\n",
    "                            \n",
    "            # Connect weights\n",
    "            for l in layers_fwd_connect_layerM[m]:\n",
    "                for d in delay_layerM_toL[m, l]:\n",
    "                    if (q - d) > 0:\n",
    "                        sum_output_layers[q, m] += np.dot(connection_weight_matrices[m, l, d], \n",
    "                                                          layer_outputs[q - d, l])\n",
    "            # Bias\n",
    "            sum_output_layers[q, m] += bias[m]\n",
    "            \n",
    "            # Calc layer output\n",
    "            if m == num_layers_network:\n",
    "                # Linear layer for output\n",
    "                layer_outputs[q, num_layers_network] = sum_output_layers[q, num_layers_network] \n",
    "            else:\n",
    "                layer_outputs[q, m] = np.tanh(sum_output_layers[q, m])\n",
    "        \n",
    "        network_output[:, q - 1] = layer_outputs[q, num_layers_network]\n",
    "    \n",
    "    network_output = network_output[:, num_prev_data_pts :]\n",
    "    \n",
    "    return network_output, sum_output_layers, layer_outputs\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def RTRL(network, data):\n",
    "    \"\"\" \n",
    "    Jacobian Matrix    == derivatives of error with respect to weight vector\n",
    "    Mean Squared Error == MSE of network compared to training data\n",
    "    Error Vector       == difference of network output and target data\n",
    "    \"\"\"\n",
    "    \n",
    "    #data == training data\n",
    "    network_inputs               = data['inputs']      \n",
    "    network_outputs              = data['outputs']      \n",
    "    layer_outputs                = data['layer_outputs']\n",
    "    num_prev_data_pts            = data['q0'] \n",
    "    \n",
    "    delay_layerM_toL             = network['delay_layerM_toL']      \n",
    "    delay_input_layer1           = network['delay_input_layer1']\n",
    "    \n",
    "    inputs_connect_layer1        = network['inputs_connect_layer1']       \n",
    "    layers_fwd_connect_layerM    = network['layers_fwd_connect_layerM']     \n",
    "    layers_bkwd_connect_layerM   = network['layers_bkwd_connect_layerM']     \n",
    "    num_layers_network           = network['num_layers']   \n",
    "    \n",
    "    inputs                       = network['network'][0]   \n",
    "    outputs                      = network['network'][-1]\n",
    "    \n",
    "    layers                       = network['layers']  \n",
    "    input_layers                 = network['output_layers']       \n",
    "    output_layers                = network['input_layers']       \n",
    "    output_layersX_connect_to    = network['output_layersX_connect_to']   \n",
    "    \n",
    "    input_weight_matrices, connection_weight_matrices, bias = convert_vector_to_matrices(network)\n",
    "    \n",
    "    # 1. Calc network output\n",
    "    network_out, sum_output_layers, layer_outputs = get_network_output(\n",
    "        network_inputs, \n",
    "        network, \n",
    "        input_weight_matrices,                                     \n",
    "        connection_weight_matrices, \n",
    "        bias, \n",
    "        layer_outputs     = layer_outputs, \n",
    "        num_prev_data_pts = num_prev_data_pts)\n",
    "    \n",
    "    # 2. Calc cost func\n",
    "    error_matrix       = network_outputs - network_out                                             \n",
    "    error_vector       = np.reshape(error_matrix, (1, np.size(error_matrix)), order = 'F')[0]\n",
    "    #mean_squared_error = np.dot(error_vector, error_vector.transpose())\n",
    "    #mean_absolute_error = np.sum(np.absolute(error_vector))\n",
    "    mean_absolute_error = np.average(np.absolute(error_vector))\n",
    "    \n",
    "    # 3. Backpropagation RTRL\n",
    "    num_of_input_datapts            = network_inputs.shape[1]                       \n",
    "    num_of_datapts_without_old_data = num_of_input_datapts - num_prev_data_pts \n",
    "    \n",
    "    \n",
    "    deriv_layer_outputsOfU_respect_bias_vect                     = {} \n",
    "    deriv_layer_outputsOfU_respect_input_weight_matrices         = {} \n",
    "    deriv_layer_outputsOfU_respect_connectection_weight_matrices = {}\n",
    "    deriv_layer_outputs_respect_weight_vect                      = {} \n",
    "    sensitivity_matrix                                           = {} \n",
    "    layersM_with_existing_sensitivity_matrix                     = {} \n",
    "    input_layersX_with_existing_sensitivity_matrix               = {} \n",
    "                                                                        \n",
    "    # Init\n",
    "    jacobian_matrix = np.zeros((num_of_datapts_without_old_data * layers[-1], \n",
    "                                network['num_weights']))\n",
    "    \n",
    "    for q in range(1, num_prev_data_pts + 1):\n",
    "        for u in input_layers:\n",
    "            deriv_layer_outputsOfU_respect_connectection_weight_matrices[q, u] = np.zeros(\n",
    "                (layers[u - 1], \n",
    "                network['num_weights']))\n",
    "\n",
    "    # Begin RTRL\n",
    "    for q in range(num_prev_data_pts + 1, num_of_input_datapts + 1):\n",
    "        # Init, set needed for calculating sensitivities\n",
    "        input_layers_ = [] \n",
    "        for u in input_layers:\n",
    "            layersM_with_existing_sensitivity_matrix[u]       = []\n",
    "            input_layersX_with_existing_sensitivity_matrix[u] = []\n",
    "            deriv_layer_outputs_respect_weight_vect[q, u]     = 0\n",
    "        \n",
    "        # Calc sensitivity matrices, decrement m in backprop order\n",
    "        for m in range(num_layers_network, 0, -1): \n",
    "            for u in input_layers_:\n",
    "                # Sensitivity Matrix layer u -> m\n",
    "                sensitivity_matrix[q, u, m] = 0 \n",
    "                \n",
    "                for l in layers_bkwd_connect_layerM[m]:\n",
    "                    #recursive calculation of Sensitivity Matrix layer u -> m\n",
    "                    sensitivity_matrix[q, u, m] += np.dot(\n",
    "                        np.dot(sensitivity_matrix[q, u, l], connection_weight_matrices[l, m, 0]),\n",
    "                        np.diag(1 - (np.tanh(sum_output_layers[q, m])) ** 2)) \n",
    "                      \n",
    "                if m not in layersM_with_existing_sensitivity_matrix[u]:\n",
    "                    layersM_with_existing_sensitivity_matrix[u].append(m) \n",
    "                    \n",
    "                    if m in output_layers:\n",
    "                        input_layersX_with_existing_sensitivity_matrix[u].append(m)\n",
    "                        \n",
    "            if m in input_layers:\n",
    "                # Output layer is linear, no transfer function\n",
    "                if m == num_layers_network: \n",
    "                    sensitivity_matrix[q, m, m] = np.diag(np.ones(outputs)) \n",
    "                else:\n",
    "                    sensitivity_matrix[q, m, m] = np.diag(1 - (np.tanh(sum_output_layers[q, m])) \n",
    "                                                                              ** 2)\n",
    "                \n",
    "                # Add m to U'\n",
    "                input_layers_.append(m) \n",
    "                layersM_with_existing_sensitivity_matrix[m].append(m)\n",
    "                \n",
    "                if m in output_layers:\n",
    "                    input_layersX_with_existing_sensitivity_matrix[m].append(m)\n",
    "          \n",
    "        '''Calc derivs, static deriv calc'''\n",
    "        for u in sorted(input_layers): \n",
    "            # Static deriv vector: explicit deriv layer outputs w/ respect to weight vect\n",
    "            deriv_layer_outputs_respect_weight_vect_ = np.empty((layers[u - 1], 0))\n",
    "            \n",
    "            # Input weights\n",
    "            for m in range(1, num_layers_network + 1): \n",
    "                if m == 1:\n",
    "                    for i in inputs_connect_layer1[m]:\n",
    "                        for d in delay_input_layer1[m, i]:\n",
    "                            # If no sensivity matrix exists or d >= q: deriv is zero\n",
    "                            if ((q, u, m) not in sensitivity_matrix.keys()) or (d >= q): \n",
    "                                deriv_layer_outputsOfU_respect_input_weight_matrices[m, i, d] = \\\n",
    "                                    np.kron(network_inputs[:, q - d - 1].transpose(), \n",
    "                                    np.zeros((layers[u - 1], layers[m - 1])))\n",
    "                            else: \n",
    "                                deriv_layer_outputsOfU_respect_input_weight_matrices[m,i,d] = \\\n",
    "                                    np.kron(network_inputs[:, q - d - 1].transpose(), \n",
    "                                    sensitivity_matrix[q, u, m])\n",
    "\n",
    "                            # Append to static deriv vect\n",
    "                            deriv_layer_outputs_respect_weight_vect_ = np.append(\n",
    "                                deriv_layer_outputs_respect_weight_vect_, \n",
    "                                deriv_layer_outputsOfU_respect_input_weight_matrices[m, i, d], \n",
    "                                1) \n",
    "        \n",
    "                # Connect weights\n",
    "                for l in layers_fwd_connect_layerM[m]:\n",
    "                    for d in delay_layerM_toL[m, l]:\n",
    "                        # If no sensivity matrix exists or d >= q: deriv is zero\n",
    "                        if ((q, u, m) not in sensitivity_matrix.keys()) or (d >= q): \n",
    "                            deriv_layer_outputsOfU_respect_connectection_weight_matrices[m,l,d]=\\\n",
    "                                np.kron(layer_outputs[q, l].transpose(), np.zeros((\n",
    "                                            layers[u - 1], layers[m - 1])))\n",
    "                        else:\n",
    "                            deriv_layer_outputsOfU_respect_connectection_weight_matrices[m,l,d]=\\\n",
    "                                np.kron(layer_outputs[q - d, l].transpose(), \n",
    "                                        sensitivity_matrix[q, u, m]) \n",
    "\n",
    "                        # Append to static deriv vect\n",
    "                        deriv_layer_outputs_respect_weight_vect_ = np.append(\n",
    "                            deriv_layer_outputs_respect_weight_vect_, \n",
    "                            deriv_layer_outputsOfU_respect_connectection_weight_matrices[m,l,d], \n",
    "                            1) \n",
    "                        \n",
    "                # Bias weights\n",
    "                if ((q, u, m) not in sensitivity_matrix.keys()):\n",
    "                    # Deriv is zero\n",
    "                    deriv_layer_outputsOfU_respect_bias_vect[m] = np.zeros((layers[u - 1], \n",
    "                                                                            layers[m - 1])) \n",
    "                else:\n",
    "                    deriv_layer_outputsOfU_respect_bias_vect[m] = sensitivity_matrix[q, u, m] \n",
    "\n",
    "                # Append to static deriv vect\n",
    "                deriv_layer_outputs_respect_weight_vect_ = np.append(\n",
    "                                        deriv_layer_outputs_respect_weight_vect_, \n",
    "                                        deriv_layer_outputsOfU_respect_bias_vect[m], \n",
    "                                        1) \n",
    "            \n",
    "            '''Dynamic deriv calc'''\n",
    "            dyn_deriv_sum_allX = 0\n",
    "            for x in input_layersX_with_existing_sensitivity_matrix[u]:\n",
    "                # Sum of all u_\n",
    "                sum_u_ = 0 \n",
    "                \n",
    "                for u_ in output_layersX_connect_to[x]:\n",
    "                    # Sum of all d\n",
    "                    sum_d = 0 \n",
    "                    \n",
    "                    for d in delay_layerM_toL[x, u_]:\n",
    "                        # Delays > 0 and < q\n",
    "                        if (q - d > 0) and (d > 0): \n",
    "                            sum_d += np.dot(connection_weight_matrices[x, u_, d], \n",
    "                                            deriv_layer_outputs_respect_weight_vect[q - d, u_])\n",
    "\n",
    "                    sum_u_ += sum_d\n",
    "\n",
    "                if sum_u_ is not 0:\n",
    "                    # Sum up dynamic deriv\n",
    "                    dyn_deriv_sum_allX += np.dot(sensitivity_matrix[q, u, x], sum_u_) \n",
    "            \n",
    "            # Static + dynamic deriv, total deriv output layer u with respect to w\n",
    "            deriv_layer_outputs_respect_weight_vect[q, u] = \\\n",
    "                    deriv_layer_outputs_respect_weight_vect_ + dyn_deriv_sum_allX \n",
    "        # Jacobian matrix\n",
    "        jacobian_matrix[range(((q - num_prev_data_pts) - 1) * outputs, \n",
    "                              (q - num_prev_data_pts) * outputs), :] = \\\n",
    "            -deriv_layer_outputs_respect_weight_vect[q, num_layers_network]\n",
    "            \n",
    "        ############!!!!#########!!!#################!@!@!@!@!#####\n",
    "        if q >= 7:\n",
    "            your_keys = [(q,2),(q,1),(q-1,2),(q-1,1),(q-2,2),(q-2,1),(q-3,2),\n",
    "                         (q-3,1),(q-4,2),(q-4,1),(q-5,2),(q-5,1),(q-6,2),(q-6,1)]\n",
    "            deriv_layer_outputs_respect_weight_vect = {\n",
    "                key: deriv_layer_outputs_respect_weight_vect[key] \n",
    "                for key in deriv_layer_outputs_respect_weight_vect.keys() \n",
    "                if key in your_keys}\n",
    "            sens_keys = [(q,  2,2),(q,  2,1),(q,  1,1),(q-1,2,2),(q-1,2,1),(q-1,1,1),\n",
    "                         (q-2,2,2),(q-2,2,1),(q-2,1,1),(q-3,2,2),(q-3,2,1),(q-3,1,1),\n",
    "                         (q-4,2,2),(q-4,2,1),(q-4,1,1),(q-5,2,2),(q-5,2,1),(q-5,1,1),\n",
    "                         (q-6,2,2),(q-6,2,1),(q-6,1,1)]\n",
    "            sensitivity_matrix ={key2: sensitivity_matrix[key2] \n",
    "                             for key2 in sensitivity_matrix.keys() \n",
    "                             if key2 in sens_keys}\n",
    "        ############!!!!#########!!!#################!@!@!@!@!#####\n",
    "        \n",
    "    #return jacobian_matrix, mean_squared_error, error_vector\n",
    "    return jacobian_matrix, mean_absolute_error, error_vector\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(network_inputs, \n",
    "                 network_targets, \n",
    "                 network, \n",
    "                 prev_input_data0  = None, \n",
    "                 prev_output_data0 = None):\n",
    "    \"\"\"\n",
    "    Prepare input data for network training and check for errors\n",
    "    \n",
    "    Returns: \n",
    "        dict containing data for training or calculating output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert inputs and outputs to 2D array, if 1D array is given\n",
    "    if network_inputs.ndim == 1:\n",
    "        network_inputs = np.array([network_inputs])\n",
    "    \n",
    "    if network_targets.ndim == 1:\n",
    "        network_targets = np.array([network_targets]) \n",
    "        \n",
    "    # Check if input and output data match structure of network\n",
    "    if np.shape(network_inputs)[0] != network['network'][0]:\n",
    "        raise ValueError(\"Dimension of input data doesn't match # of inputs of network\")\n",
    "    \n",
    "    if np.shape(network_targets)[0] != network['network'][-1]:\n",
    "        raise ValueError(\"Dimension of output data doesn't match # of outputs of network\")\n",
    "    \n",
    "    if np.shape(network_inputs)[1] != np.shape(network_targets)[1]:\n",
    "        raise ValueError(\"Input and output data must have same # of datapoints\")\n",
    "        \n",
    "    # Check if prev data given, convert input and output to 2D array, if 1D array given\n",
    "    if (prev_input_data0 is not None) and (prev_output_data0 is not None): \n",
    "        if prev_input_data0.ndim == 1:\n",
    "            prev_input_data0 = np.array([prev_input_data0])\n",
    "        \n",
    "        if prev_output_data0.ndim == 1:\n",
    "            prev_output_data0 = np.array([prev_output_data0])\n",
    "            \n",
    "        # Check if input and output data match structure of network\n",
    "        if np.shape(prev_input_data0)[0] != network['network'][0]:\n",
    "            raise ValueError(\"Dimension of prev input data(p0) doesn't match # inputs of network\")\n",
    "        \n",
    "        if np.shape(prev_output_data0)[0] != network['network'][-1]:\n",
    "            raise ValueError(\"Dimension of prev output data(y0) doesn't match # outputs network\")\n",
    "        \n",
    "        if np.shape(prev_input_data0)[1] != np.shape(prev_output_data0)[1]:\n",
    "            raise ValueError(\"Prev input and output data must have same # of datapoints(q0)\")\n",
    "            \n",
    "        num_prev_data_pts = np.shape(prev_input_data0)[1] \n",
    "        \n",
    "        # Init layer outputs\n",
    "        layer_outputs = {} \n",
    "        \n",
    "        for i in range(1, num_prev_data_pts + 1):\n",
    "            for j in range(1, network['num_layers']):\n",
    "                # Layer ouputs of hidden layers are unknown -> set to zero\n",
    "                layer_outputs[i, j] = np.zeros(network['network'][-1]) \n",
    "            \n",
    "            # Set layer ouputs of output layer\n",
    "            layer_outputs[i, network['num_layers']] = prev_output_data0[:, i - 1] / \\\n",
    "                network['normY'] \n",
    "            \n",
    "        # Add prev inputs and outputs to input/output matrices\n",
    "        updated_inputs  = np.concatenate([prev_input_data0, network_inputs], axis=1)\n",
    "        updated_outputs = np.concatenate([prev_output_data0, network_targets], axis=1)\n",
    "    \n",
    "    # Keep inputs and outputs as is and set q0 and a to default vals\n",
    "    else: \n",
    "        updated_inputs    = network_inputs.copy()\n",
    "        updated_outputs   = network_targets.copy()\n",
    "        num_prev_data_pts = 0\n",
    "        layer_outputs     = {}\n",
    "        \n",
    "    # Normalize\n",
    "    inputs_normed  = updated_inputs.copy()\n",
    "    outputs_normed = updated_outputs.copy()\n",
    "    \n",
    "    if 'normP' not in network.keys():\n",
    "        normInp = np.ones(np.shape(updated_inputs)[0])\n",
    "\n",
    "        for p in range(np.shape(updated_inputs)[0]):\n",
    "            normInp[p]       = np.max([np.max(np.abs(updated_inputs[p])), 1.0])\n",
    "            inputs_normed[p] = updated_inputs[p] / normInp[p]\n",
    "\n",
    "        normOut = np.ones(np.shape(updated_outputs)[0])\n",
    "\n",
    "        for y in range(np.shape(updated_outputs)[0]):\n",
    "            normOut[y]        = np.max([np.max(np.abs(updated_outputs[y])), 1.0])\n",
    "            outputs_normed[y] = updated_outputs[y] / normOut[y] \n",
    "            \n",
    "        network['normP'] = normInp\n",
    "        network['normY'] = normOut\n",
    "   \n",
    "    else:\n",
    "        for p in range(np.shape(updated_inputs)[0]):\n",
    "            inputs_normed[p] = updated_inputs[p] / network['normP'][p]\n",
    "            \n",
    "        normOut = np.ones(np.shape(network_targets)[0])\n",
    "        \n",
    "        for y in range(np.shape(updated_outputs)[0]):\n",
    "            outputs_normed[y] = updated_outputs[y] / network['normY'][y]\n",
    "            \n",
    "    # Create data dict\n",
    "    data                   = {}\n",
    "    data['inputs']         = inputs_normed\n",
    "    data['outputs']        = outputs_normed\n",
    "    data['layer_outputs']  = layer_outputs\n",
    "    data['q0']             = num_prev_data_pts    \n",
    "    \n",
    "    return data, network\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def train_LM(nn_inputs, \n",
    "             nn_outputs, \n",
    "             net, \n",
    "             iteration_max = 10, \n",
    "             MSE_stop      = 1e-6, \n",
    "             damp_factor   = 3.0, \n",
    "             damp_const    = 10.0, \n",
    "             verbose       = False):\n",
    "    \"\"\"\n",
    "    Levenberg-Marquardt(LM) algorithm\n",
    "      - Least-squares estimation of nonlinear parameters\n",
    "      \n",
    "    Args:\n",
    "        iteration_max:  max # of iterations\n",
    "        MSE_stop:       termination error, training stops when MSE <= MSE_stop\n",
    "        damp_const:     constant to adapt damping factor of LM\n",
    "        damp_factor:    damping factor of LM\n",
    "    \n",
    "    Returns:\n",
    "        net:            trained Neural Network \n",
    "    \"\"\"\n",
    "    \n",
    "    data, net = prepare_data(nn_inputs, nn_outputs, net)\n",
    "    \n",
    "    # Calc for first iteration\n",
    "    Jacobian, Mean_absolute_error, error_vect = RTRL(net, data)\n",
    "    \n",
    "    # Vect for error history\n",
    "    iteration               = 0\n",
    "    ErrorHistory            = np.zeros(iteration_max + 1) \n",
    "    ErrorHistory[iteration] = Mean_absolute_error\n",
    "    \n",
    "    if verbose:\n",
    "        print('Iteration: ',    iteration, \n",
    "              'Error: ',        Mean_absolute_error, \n",
    "              'scale factor: ', damp_factor)\n",
    "    \n",
    "    seconds, minutes, hours = 60, 60, 6\n",
    "    t_end = time.time() + (seconds * minutes * hours)\n",
    "\n",
    "    # Run loop until either max iterations or MSE_stop reached\n",
    "    while True:\n",
    "        #####\n",
    "        JTJ = scipy.linalg.blas.dgemm(alpha=1.0, a=Jacobian.T, b=Jacobian.T, trans_b=True)\n",
    "        #####\n",
    "        weight_vect = net['weight_vect']\n",
    "        \n",
    "        # Repeat until optimizing step successful\n",
    "        while True:\n",
    "            seconds2, minutes2, hours2 = 60, 60, 1.5\n",
    "            t_end2 = time.time() + (seconds2 * minutes2 * hours2)\n",
    "            \n",
    "            gradient = np.dot(Jacobian.transpose(), error_vect)\n",
    "\n",
    "            # Calc scaled inverse Hessian\n",
    "            try:\n",
    "                #####\n",
    "                scaled_inv_hessian = scipy.linalg.inv(JTJ + damp_factor * \n",
    "                                                      np.eye(net['num_weights'])) \n",
    "                #####\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Not invertible, do small step in gradient direction\n",
    "                weight_delta = 1.0 / 1e10 * gradient\n",
    "            else:\n",
    "                # Calc weight modification\n",
    "                weight_delta = np.dot(-scaled_inv_hessian, gradient)\n",
    "            \n",
    "            # New weight vect\n",
    "            net['weight_vect'] = weight_vect + weight_delta  \n",
    "            new_mean_absolute_error = calc_error(net, data)\n",
    "            \n",
    "            # If optimization step successful, adapt scale factor, then go next iteration\n",
    "            if new_mean_absolute_error < Mean_absolute_error:\n",
    "                damp_factor = damp_factor / damp_const \n",
    "                break \n",
    "            else:\n",
    "                damp_factor = damp_factor * damp_const\n",
    "                \n",
    "            if (time.time() > t_end) or (time.time() > t_end2):\n",
    "                break\n",
    "        if (time.time() > t_end) or (time.time() > t_end2):\n",
    "            break\n",
    "        \n",
    "        # Calc for next iteration\n",
    "        Jacobian, Mean_absolute_error, error_vect = RTRL(net, data)\n",
    "        iteration += 1\n",
    "        ErrorHistory[iteration] = Mean_absolute_error\n",
    "        \n",
    "        if verbose:\n",
    "            print('Iteration: ',    iteration,\n",
    "                  'Error: ',        Mean_absolute_error,\n",
    "                  'scale factor: ', damp_factor)\n",
    "\n",
    "        # Check if termination condition hit\n",
    "        if iteration >= iteration_max:\n",
    "            print('Max # of iterations reached')\n",
    "            break\n",
    "        elif Mean_absolute_error <= MSE_stop:\n",
    "            print('Termination error reached')\n",
    "            break\n",
    "\n",
    "    net['ErrorHistory'] = ErrorHistory[:iteration]\n",
    "    return net\n",
    "\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def calc_error(network, data):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        MAE of network compared to training data\n",
    "    \"\"\"\n",
    "    \n",
    "    network_inputs    = data['inputs']\n",
    "    network_outputs   = data['outputs'] \n",
    "    layer_outputs     = data['layer_outputs']\n",
    "    num_prev_data_pts = data['q0'] \n",
    "    \n",
    "    input_weight_matrices, connection_weight_matrices, bias = convert_vector_to_matrices(network)\n",
    "    \n",
    "    network_out, sum_output_layers, layer_outputs = get_network_output(\n",
    "        network_inputs, \n",
    "        network, \n",
    "        input_weight_matrices,                                     \n",
    "        connection_weight_matrices, \n",
    "        bias, \n",
    "        layer_outputs     = layer_outputs, \n",
    "        num_prev_data_pts = num_prev_data_pts)\n",
    "\n",
    "    # Outputs_delta = error matrix\n",
    "    outputs_delta       = network_outputs - network_out \n",
    "    error_vect          = np.reshape(outputs_delta, (1, np.size(outputs_delta)), order='F')[0]\n",
    "    Mean_absolute_error = np.average(np.absolute(error_vect))\n",
    "\n",
    "    #return Mean_squared_error\n",
    "    return Mean_absolute_error\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def NNOut(inputs, net, P0 = None, Y0 = None):\n",
    "    \"\"\"\n",
    "    Calculates network output for given inputs\n",
    "    \n",
    "    Args:\n",
    "        P0: prev input data\n",
    "        Y0: prev output data\n",
    "        \n",
    "    Returns:\n",
    "    Y_NN: Neural Network output for input P\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs   = np.zeros((net['layers'][-1], np.size(inputs) / net['network'][0]))\n",
    "    data, net = prepare_data(inputs, outputs, net)\n",
    "    input_weight_matrices, connection_weight_matrices, bias = convert_vector_to_matrices(net)\n",
    "    \n",
    "    network_out = get_network_output(\n",
    "        data['inputs'], \n",
    "        net, \n",
    "        input_weight_matrices,                                     \n",
    "        connection_weight_matrices, \n",
    "        bias, \n",
    "        layer_outputs     = data['layer_outputs'], \n",
    "        num_prev_data_pts = data['q0'])[0]\n",
    "\n",
    "    # Scale normalized output\n",
    "    network_out_scaled = network_out.copy()\n",
    "    for y in range(np.shape(network_out)[0]):\n",
    "        network_out_scaled[y] = network_out[y] * net['normY'][y]\n",
    "    \n",
    "    if np.shape(network_out_scaled)[0] == 1:\n",
    "        network_out_scaled = network_out_scaled[0]\n",
    "    \n",
    "    return network_out_scaled\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_learntest(num, key, out_num, net=False, keyonly=False):\n",
    "    \"\"\"\n",
    "    Retrieve our dataframes with all of our indicators for our 6 helper indexes/funds that \n",
    "    help in the prediction process for a certain company we choose to predict on. The 6 \n",
    "    indexes are the Dow Jones Index, the S&P 500 Index, the Nasdaq Composite, \n",
    "    United States Oil Fund, the SPDR S&P 500 ETF, and SPDR Gold Shares. \n",
    "    \n",
    "    We take these dataframes, and concatenate to our stock we will predict for,\n",
    "    calling it with a stock key that we feed in through a parameter. \n",
    "    \n",
    "    We then adjust for missing dates that many of the 6 helper stocks lack but that many\n",
    "    stocks do have, removing those certain dates from our data. We then take the list of \n",
    "    indicators we are going to use by loading a list with all of the 222 names of each \n",
    "    indicator. There are over 2000 possible indicators, we chose only 222 for memory and\n",
    "    computation costs of using more than that. \n",
    "    \n",
    "    You can change the indicators as you like, just as long as it's a list of indicator \n",
    "    names as strings. We get our 222 indicator dataframe then set our output as our stock\n",
    "    choices return values at 1 minute intervals. \n",
    "    \n",
    "    We then set up our learn/test lists which have to be lists due to better efficiency\n",
    "    using numpy rather than pandas dataframes.\n",
    "    \n",
    "    Return the inputs, outputs, test inputs, test outputs, and the neural net if we already\n",
    "    have one set up.\n",
    "    \"\"\"\n",
    "    # If false,we're utilizing helper indexes/etfs/natural-resources for prediction process\n",
    "    if keyonly == False:\n",
    "        # Grab our pickled files from our NewBase/STOCKSYMBOL/ directories\n",
    "        # the num is referring to one of fifteen parts to each company indicator dataframe\n",
    "        opp       = open('NewBase/^GSPC/^GSPC_df'+str(num)+'.pickle', 'rb')\n",
    "        opp2      = open('NewBase/^IXIC/^IXIC_df'+str(num)+'.pickle', 'rb')\n",
    "        opp3      = open('NewBase/^DJI/^DJI_df'+str(num)+'.pickle', 'rb')\n",
    "        opp4      = open('NewBase/GLD/GLD_df'+str(num)+'.pickle', 'rb')\n",
    "        opp5      = open('NewBase/USO/USO_df'+str(num)+'.pickle', 'rb')\n",
    "        opp6      = open('NewBase/SPY/SPY_df'+str(num)+'.pickle', 'rb')\n",
    "        opp7      = open('NewBase/'+key+'/'+key+'_df'+str(num)+'.pickle', 'rb')\n",
    "        # Also grab the indicator list that contains all the indicators we'll be using from\n",
    "        # our indicator dataframes since we can't process that many indicators\n",
    "        #opp8      = open('Pickles/final_lst222'+key+'.pickle','rb')\n",
    "        opp8      = open('Pickles/final_lst90'+key+'.pickle','rb')\n",
    "        gspc      = pickle.load(opp)\n",
    "        ixic      = pickle.load(opp2)\n",
    "        dji       = pickle.load(opp3)\n",
    "        gld       = pickle.load(opp4)\n",
    "        uso       = pickle.load(opp5)\n",
    "        spy       = pickle.load(opp6)\n",
    "        df        = pickle.load(opp7)\n",
    "        final_lst = pickle.load(opp8)\n",
    "        opp.close()\n",
    "        opp2.close()\n",
    "        opp3.close()\n",
    "        opp4.close()\n",
    "        opp5.close()\n",
    "        opp6.close()\n",
    "        opp7.close()\n",
    "        opp8.close()\n",
    "\n",
    "        # Combine the dataframes into a single dataframe, making sure there aren't any\n",
    "        # duplicate rows which happened curiously a few times during testing.\n",
    "        comp_lst = [gspc,ixic,dji,gld,uso,spy,df]\n",
    "        for x in range(7):\n",
    "            comp_lst[x] = comp_lst[x].T.groupby(level=0).first().T\n",
    "        new_df = pd.concat(comp_lst, axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "        # Remove these dates because some dataframes contain data from them but others\n",
    "        # don't and could affect calculations with up to 390 consecutive incorrect values\n",
    "        dates = ['2013-08-28', '2013-10-28', '2014-02-12', '2014-02-18', '2014-10-02', \n",
    "                 '2014-10-06', '2014-10-08', '2014-10-09', '2014-10-13', '2014-10-14', \n",
    "                 '2014-10-15', '2014-10-20', '2015-01-14', '2015-04-21', '2015-05-18', \n",
    "                 '2015-06-08', '2015-07-08', '2015-08-20', '2015-08-31', '2015-09-08', \n",
    "                 '2016-02-08', '2016-03-15', '2016-03-21', '2016-03-22', '2016-04-13', \n",
    "                 '2016-06-15', '2015-03-30', '2015-05-05', '2014-02-25']\n",
    "        for each in dates:\n",
    "            try:\n",
    "                nums   = new_df.index.get_loc(each)\n",
    "                new_df = new_df.drop(new_df.index[nums.start:nums.stop])\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        # If true, we only use indicators from the stock we're predicting on\n",
    "        opp       = open('NewBase/'+key+'/'+key+'_df'+str(num)+'.pickle', 'rb')\n",
    "        opp2      = open('Pickles/final_lst216'+key+'only.pickle','rb')\n",
    "        new_df    = pickle.load(opp)\n",
    "        final_lst = pickle.load(opp2)\n",
    "        opp.close()\n",
    "        opp2.close()\n",
    "    \n",
    "    new_df        = new_df.T.groupby(level=0).first().T\n",
    "    # Create the output indicator name so we can call it from our indicator df\n",
    "    rets_name     = key+'_rets'+str(out_num)\n",
    "    # Get a dataframe with just our indicators for training/testing, and then\n",
    "    # the output column, learning_rets\n",
    "\n",
    "    result        = new_df[final_lst]\n",
    "    learning_rets = new_df[rets_name]\n",
    "    \n",
    "    length        = len(result)\n",
    "    # Adjust the input/output so inputs/output are in sync. If not, we'd be predicting\n",
    "    # for for the incorrect returns.\n",
    "    result        = result.iloc[:length-(out_num+1)]\n",
    "    learning_rets = learning_rets.iloc[(out_num+1):]\n",
    "    \n",
    "    # If already have a neural net trained on previous data, net will be true. Then we\n",
    "    # set our net_var to the already trained net in prep to train further with it.\n",
    "    net_var = []\n",
    "    if net == True:\n",
    "        if keyonly == False:\n",
    "            opp = open('Pickles/net222attrs'+key+'.pickle','rb')\n",
    "        else:\n",
    "            opp = open('Pickles/net216attrs'+key+'only.pickle','rb')\n",
    "        net_var = pickle.load(opp)\n",
    "        opp.close()\n",
    "    \n",
    "    # Set up our test input/output dataframes\n",
    "    result_test   = result.copy(deep=True)\n",
    "    testing_rets  = learning_rets.copy(deep=True)\n",
    "    \n",
    "    # Leave the last 205 records for testing net w/ test data\n",
    "    sml = 0\n",
    "    mid = len(result) - 205\n",
    "    \n",
    "    # Convert dataframes to numpy arrays for speed/efficiency purposes\n",
    "    learning_outputs = learning_rets.iloc[sml:mid]\n",
    "    testing_outputs  = testing_rets.iloc[mid:mid+200]\n",
    "    learning_inputs  = result.iloc[sml:mid]\n",
    "    testing_inputs   = result_test[mid:mid+200]\n",
    "\n",
    "    inputs           = learning_inputs.values.T\n",
    "    outputs          = learning_outputs.values.T\n",
    "    test_inputs      = testing_inputs.values.T\n",
    "    test_outputs     = testing_outputs.values.T\n",
    "\n",
    "    print mid\n",
    "    return inputs, outputs, test_inputs, test_outputs, key, net_var\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def start_training(inputs, outputs, key, net=[], keyonly=False):\n",
    "    \"\"\"\n",
    "    If we are using an already created neural network and training futher on it,\n",
    "    we'll set net2 to net, else we create a neural network with 222 inputs, 12\n",
    "    hidden neurons, and 1 output.\n",
    "    \n",
    "    If we are using our second example indicator list that only contains \n",
    "    indicators from our prediction stock rather than both the stock and our\n",
    "    base etfs/indexes indicators, we'll use our 216 inputs list, and dump\n",
    "    that file to our net216attrsKEYonly.pickle file.\n",
    "    \"\"\"\n",
    "    # If we don't already have a trained network for further training, create new one\n",
    "    if net == []:\n",
    "        if keyonly == False:\n",
    "            net = create_neural_network([222, 12, 1], \n",
    "                       delay_input    = [0,1,2,3,4],        \n",
    "                       delay_internal = [1,2,3,4],         \n",
    "                       delay_output   = [1,2,3,4])\n",
    "        else:\n",
    "            net = create_neural_network([216, 12, 1], \n",
    "                       delay_input    = [0,1,2,3,4],        \n",
    "                       delay_internal = [1,2,3,4],         \n",
    "                       delay_output   = [1,2,3,4])\n",
    "    elif net == 0:\n",
    "        net = create_neural_network([90, 24, 1], \n",
    "                       delay_input    = [0,1,2,3],        \n",
    "                       delay_internal = [1,2,3],         \n",
    "                       delay_output   = [1,2,3])\n",
    "    else:\n",
    "        net = net\n",
    "    \n",
    "    # Return our new trained network\n",
    "    net = train_LM(inputs, outputs, net, verbose=True, iteration_max=6, MSE_stop=1e-6)\n",
    "\n",
    "    # Dump that network into a pickled file\n",
    "    #if keyonly == False:\n",
    "    #    opp = open('Pickles/net222attrs'+key+'.pickle','wb')\n",
    "    #else:\n",
    "    #    opp = open('Pickles/net216attrs'+key+'only.pickle','wb')\n",
    "    #pickle.dump(net, opp)\n",
    "    #opp.close()\n",
    "    \n",
    "    # Return net for testing/graphing\n",
    "    return net\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def graph_predictions(test_inputs, test_outputs, net, beg, end):\n",
    "    \"\"\"\n",
    "    Graph the predicted stock return value compared to the actual return value.\n",
    "    beg is the row your testing, end is the ending row. For example, if \n",
    "    beg == 0, and end == 50, we'll look at the predictions for the first 50\n",
    "    rows of the test inputs.\n",
    "    \"\"\"\n",
    "    # Create our predictions\n",
    "    ytest  = NNOut(test_inputs, net, P0=None, Y0=None)\n",
    "    \n",
    "    # On how much data will we graph\n",
    "    total  = end - beg\n",
    "    \n",
    "    # Graph our predicted vs actual output\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(total), ytest[beg:end], 'b-', label='Ytest')\n",
    "    plt.plot(range(total), test_outputs[beg:end], 'r', label='Ytrue')\n",
    "    fig.suptitle('Predicted VS Actual Output', fontsize=20)\n",
    "    plt.xlabel('Timestamp', fontsize=18)\n",
    "    plt.ylabel('Return Value', fontsize=16)\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Predicted Output')\n",
    "    red_patch  = mpatches.Patch(color='red', label='Actual Output')\n",
    "    plt.legend(handles=[red_patch, blue_patch])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24818\n"
     ]
    }
   ],
   "source": [
    "inputs,outputs,test_inputs,test_outputs,key,net=create_learntest(2,'GOOG',0, net=True)\n",
    "#inputs,outputs,test_inputs,test_outputs,key,net=create_learntest(5,'AAPL',0, net=True, keyonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = start_training(inputs, outputs, key, net=0)\n",
    "#net = start_training(inputs, outputs, key, net=net, keyonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_predictions(test_inputs, test_outputs, net, 55, 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst = ['GOOG','^GSPC','^IXIC','^DJI','GLD','USO','SPY']\n",
    "lst2 = ['rsi', 'vol', 'sma', 'cci', 'per', 'mom', 'bol', \n",
    "           'aro', 'mac', 'mactwo', 'adx', 'kdo', 'rets']\n",
    "test = []\n",
    "for key in lst:\n",
    "    for nm in lst2:\n",
    "        if (key+'_'+nm+'0') != 'GOOG_rets0':\n",
    "            test.append(key+'_'+nm+'0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
