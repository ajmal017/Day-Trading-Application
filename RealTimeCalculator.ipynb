{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "from RealTimeNN import *\n",
    "from calc_ind import *\n",
    "from googlefinance import getQuotes, getNews\n",
    "from dateutil import rrule  \n",
    "from sys import stdout\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import urllib\n",
    "import time\n",
    "import math\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def NYSE_holidays(a = datetime.date.today() - datetime.timedelta(days=390), \n",
    "                  b = datetime.date.today() + datetime.timedelta(days=365)): \n",
    "    \n",
    "    # Generate ruleset for holiday observances on the NYSE \n",
    "    rs = rrule.rruleset()\n",
    "    \n",
    "    # Include all potential holiday observances \n",
    "    ###############################################\n",
    "    \n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=31, \n",
    "                         byweekday=rrule.FR))               \n",
    "    # New Years Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=1))\n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=2, \n",
    "                         byweekday=rrule.MO))                   \n",
    "    # MLK Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,\n",
    "                         byweekday=rrule.MO(3)))                            \n",
    "    # Washington's Bday\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=2,\n",
    "                         byweekday=rrule.MO(3)))                          \n",
    "    # Good Friday \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,byeaster=-2)) \n",
    "    # Memorial Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=5, \n",
    "                         byweekday=rrule.MO(-1)))                        \n",
    "    # Independence Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=3, \n",
    "                         byweekday=rrule.FR))              \n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=4))\n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=5, \n",
    "                         byweekday=rrule.MO))               \n",
    "    # Labor Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=9, \n",
    "                         byweekday=rrule.MO(1)))                          \n",
    "    # Thanksgiving Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=11, \n",
    "                         byweekday=rrule.TH(4)))                          \n",
    "    # Christmas \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=24, \n",
    "                         byweekday=rrule.FR))                \n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=25))\n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=26, \n",
    "                         byweekday=rrule.MO))                \n",
    "    ######################################################\n",
    "\n",
    "    # Exclude potential holidays that fall on weekends \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,until=b,\n",
    "                          byweekday=(rrule.SA,rrule.SU))) \n",
    "    return rs \n",
    "\n",
    "def NYSE_tradingdays(a = datetime.date.today() - datetime.timedelta(days=390), \n",
    "                     b = datetime.date.today() + datetime.timedelta(days=365)): \n",
    "    # Generate ruleset for NYSE trading days \n",
    "    rs = rrule.rruleset() \n",
    "    rs.rrule(rrule.rrule(rrule.DAILY,dtstart=a,until=b)) \n",
    "    \n",
    "    # Exclude weekends and holidays \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,byweekday=(rrule.SA,rrule.SU)))\n",
    "    rs.exrule(NYSE_holidays(a, b)) \n",
    "    \n",
    "    return rs \n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_prd_lst2(highlowclose, prd_lst, prd_lst_nums, \n",
    "                    prd_lst_nums2, prd_lst_nums3, prd_lst_nums4):\n",
    "    \"\"\"\n",
    "    Each company has it's own period list which is how many minutes encompass each\n",
    "    different period length. There are differences between companies because of about\n",
    "    20 missing days worth of data in the past 3 years that several companies have and \n",
    "    this is meant to adjust for those differences by getting the length of the period\n",
    "    between each date range that have been already adjusted for the missing days for that \n",
    "    company. \n",
    "    \n",
    "    We then are left with the length of, for example, a 30 day period, which adjusted\n",
    "    for missing days may mean we take the length between 31 days worth of period, that\n",
    "    because of the missing data is then the correct 30 day period length. We do this for\n",
    "    each length and then combine these lengths onto the three day length lists which are\n",
    "    already set up. \n",
    "    \n",
    "    There are four different lists for each company because the different indicators have\n",
    "    different starting values and more periods. For example, the first lst is the base and \n",
    "    is used for almost all the indicators, but the third one is used for MAC indicators. For \n",
    "    those, there are 3 extra, 2 at the beginning, 6 and 8, and one at the end, 350. The last\n",
    "    lst is used for return values.\n",
    "    \"\"\"\n",
    "    prd_dict  = {}\n",
    "    \n",
    "    for name in highlowclose.keys():\n",
    "        close = highlowclose[name]['Closes']\n",
    "        \n",
    "        len_p  = []\n",
    "        all_l  = []\n",
    "        for each in prd_lst[name]:\n",
    "            len_p.append(len(close[each:]))\n",
    "\n",
    "        lst1 = prd_lst_nums  + len_p\n",
    "        lst2 = prd_lst_nums2 + len_p\n",
    "        lst3 = prd_lst_nums3 + len_p\n",
    "        lst4 = prd_lst_nums4 + len_p\n",
    "        \n",
    "        all_l.append(lst1)\n",
    "        all_l.append(lst2)\n",
    "        all_l.append(lst3)\n",
    "        all_l.append(lst4)\n",
    "        \n",
    "        prd_dict[name] = all_l\n",
    "    return prd_dict\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def company_news(newstickers):\n",
    "    \"\"\"\n",
    "    Use Google's server and use https://github.com/hongtaocai/googlefinance module to retrieve\n",
    "    the real-time stock news data for each company stock symbol fed to it.\n",
    "    \"\"\"\n",
    "    tickernewslist = {}\n",
    "    \n",
    "    for each in newstickers:\n",
    "        tickernewslist[each] = getNews(each)  \n",
    "    return tickernewslist\n",
    "\n",
    "def new_trading_news(news_dict, tickers2):\n",
    "    \"\"\"\n",
    "    Find if we have any new news for each company. We'll look at the dates\n",
    "    for each news story and if it's newer than our newest story we already\n",
    "    have then we add it to our dictionary. Eventually we will use these stories\n",
    "    to help in the prediction process.\n",
    "    \"\"\"\n",
    "    todays_news = company_news(tickers2)\n",
    "    \n",
    "    hist_news   = news_dict[0]\n",
    "    last_date   = news_dict[1]\n",
    "\n",
    "    for tick in tickers2:\n",
    "        count     = 0\n",
    "        comp_dict = {}\n",
    "        lastest   = last_date[tick]\n",
    "        old_news  = hist_news[tick]\n",
    "\n",
    "        for value in todays_news[tick]:\n",
    "            d     = value['d']\n",
    "            title = value['t']\n",
    "            url   = value['u']\n",
    "                \n",
    "            try:\n",
    "                date  = datetime.datetime.strptime(d, '%b %d, %Y').date()\n",
    "            except:\n",
    "                date  = datetime.datetime.strptime('Sep 12, 2016', '%b %d, %Y').date()\n",
    "                pass\n",
    "                \n",
    "            if date > latest:\n",
    "                length           = len(old_news)\n",
    "                old_news[length] = {'Date':date, 'Title':title, 'URL':url}\n",
    "                latest           = date\n",
    "            \n",
    "        last_date[tick] = latest\n",
    "        hist_news[tick] = old_news\n",
    "        \n",
    "    new_news_dict = [hist_news, last_date]\n",
    "    tn = open('Pickles/newsdict.pickle', 'wb')\n",
    "    pickle.dump(new_news_dict, tn)\n",
    "    tn.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "def splits(stock_split_dates):\n",
    "    \"\"\"\n",
    "    Read in the stock split site data to discover if any of our watched companies have \n",
    "    stock splits in the near future so the data can be adjusted for it. Return which \n",
    "    companies if any are discovered on it.\n",
    "    \"\"\"\n",
    "    splitup        = urllib.urlopen(stock_split_dates).read()\n",
    "    companies      = splitup.find('Announced')\n",
    "    stop           = splitup.find('th_No_BG', companies)\n",
    "    company_splits = []\n",
    "    \n",
    "    # Find which companies are listed along with the stock split ratio,\n",
    "    #  the ex-date, and the payment date.\n",
    "    while True:\n",
    "        done      = splitup.find(')</a>',stop)+1\n",
    "        next_stop = splitup.rfind('(', stop, done)\n",
    "        ratio_s   = splitup.find('<td>',done)+4\n",
    "        ratio_e   = splitup.find('<',ratio_s)\n",
    "        payment_e = splitup.find('2016',done)+4\n",
    "        payment_s = splitup.rfind('<td>',stop,payment_e)+4\n",
    "        exdate_s  = splitup.find('<td>',payment_e)+4\n",
    "        exdate_e  = splitup.find('</td>',exdate_s)\n",
    "        \n",
    "        company_splits.append([splitup[next_stop+1:done-1],\n",
    "                               splitup[exdate_s:exdate_e],\n",
    "                               splitup[ratio_s:ratio_e],\n",
    "                               splitup[payment_s:payment_e]])\n",
    "        \n",
    "        tbody = splitup.find('</table>',done)\n",
    "        test  = splitup.find('(',done)\n",
    "        \n",
    "        if tbody < test:\n",
    "            break\n",
    "        stop = done\n",
    "    return company_splits\n",
    "\n",
    "def dvds(dividend_dates): \n",
    "    \"\"\"\n",
    "    Like the stock_splits() function, searches a site to find if any of our companies have\n",
    "    an upcoming dividend so as to adjust for it. Return a list of companies, if any.\n",
    "    \"\"\"\n",
    "    div_dic           = {}\n",
    "    for x in xrange(0,1):\n",
    "        company_dividends = []\n",
    "        if x != 0:\n",
    "            dividends      = urllib.urlopen(dividend_dates).read()  \n",
    "            end            = dividends.find('Next')-2\n",
    "            end2           = dividends.rfind('href=',0,end)\n",
    "            end3           = dividends.rfind('href=',0,end2)\n",
    "            end4           = dividends.rfind('href=',0,end3)\n",
    "            end            = dividends.rfind('href=',0,end4)\n",
    "            start          = dividends.rfind('href=',0,end5)\n",
    "            end            = dividends.find('>', start)\n",
    "            next_day       = dividends[start+6:end-1]\n",
    "            dividend_dates = 'http://www.nasdaq.com/dividend-stocks/'+next_day\n",
    "            \n",
    "        dividends = urllib.urlopen(dividend_dates).read()  \n",
    "        start     = dividends.find('Payment Date')\n",
    "        \n",
    "        while True:\n",
    "            start      = dividends.find('&#40;',start)+5\n",
    "            end        = dividends.find('&#41;',start)\n",
    "            ex_date_e  = dividends.find('2016',end)+4\n",
    "            ex_date_s  = dividends.rfind('>',end,ex_date_e)+1\n",
    "            div_e      = dividends.find('</',ex_date_e+1)\n",
    "            div_s      = dividends.rfind('>',ex_date_e,div_e)+1\n",
    "            rec_date_e = dividends.find('2016',div_e)+4\n",
    "            rec_date_s = dividends.rfind('>',div_e,rec_date_e)+1\n",
    "            proceed    = dividends.find('2016',rec_date_e)+4\n",
    "            pay_date_e = dividends.find('2016',proceed)+4\n",
    "            pay_date_s = dividends.rfind('>',proceed,pay_date_e)+1\n",
    "            \n",
    "            company_dividends.append([dividends[start:end],\n",
    "                                    dividends[ex_date_s:ex_date_e],\n",
    "                                    dividends[div_s:div_e],\n",
    "                                    dividends[rec_date_s:rec_date_e],\n",
    "                                    dividends[pay_date_s:pay_date_e]])\n",
    "            \n",
    "            test = dividends.find('&#40;',start)\n",
    "            if test == -1:\n",
    "                break\n",
    "            start = end \n",
    "        div_dic[x] = company_dividends\n",
    "    return div_dic\n",
    "\n",
    "def yahoo_backup(ticks, day_hl, temp, time_value):\n",
    "    \"\"\"\n",
    "    Due to Google occasionally blocking our google server api, a yahoo backup is \n",
    "    set up to retrieve stock info from google finance's page. It is far slower so\n",
    "    it's to be avoided if at all possible. Increasing latency between each call can\n",
    "    help Google from your blocking but if blocked, you usually can get unblocked by\n",
    "    calling this function once.\n",
    "    \"\"\"\n",
    "    prices_array = []\n",
    "    yahoo      = 'http://finance.yahoo.com/q?s='\n",
    "    ticks2 = {'NYSE:LMT':'LMT', 'NYSEARCA:USO':'USO', 'NYSEARCA:GLD':'GLD',\n",
    "              'NYSEARCA:SPY':'SPY', 'INDEXDJX:.DJI':'%5EDJI',\n",
    "              'INDEXSP:.INX':'^GSPC', 'INDEXNASDAQ:.IXIC':'%5EIXIC'}\n",
    "    \n",
    "    # Retrieve the stock price for each company\n",
    "    for name in ticks:\n",
    "        if name not in ticks2.keys():\n",
    "            name2 = name\n",
    "        else:\n",
    "            name2 = ticks2[name]\n",
    "            \n",
    "        stock       = yahoo + name2\n",
    "        page        = urllib.urlopen(stock).read()\n",
    "        price       = page.find('$main-0-Quote.0.1.0.$price.0')\n",
    "        price_stop  = page.find('</span>',price)\n",
    "        price_start = page.rfind('>',price_stop-10,price_stop)+1\n",
    "        \n",
    "        try:\n",
    "            close_value     = float(page[price_start:price_stop].replace(',',''))\n",
    "            day_hl[name][0] = min(close_value, day_hl[name][0])\n",
    "            day_hl[name][1] = max(close_value, day_hl[name][1])\n",
    "            typ_price       = (day_hl[name][1] + day_hl[name][0] + close_value)/3.\n",
    "            temp[name].ix[time_value] = [day_hl[name][1], day_hl[name][0], \n",
    "                                         close_value, typ_price]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return temp\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def real_time_quotes(ticks, attr, prds, indics, day_hl, temp):\n",
    "    \"\"\"\n",
    "    As the function name implies, it retrieves stock data in real time. It will be started at\n",
    "    the beginning of the trading day(9:30AM Eastern) and ends at the end of the trading day\n",
    "    (4:00PM Eastern). It retrieves the data from Google's server by calling getQuotes which uses\n",
    "    the same program used to retrieve stock news. Credit goes to \n",
    "    https://github.com/hongtaocai/googlefinance for creating the program to retrieve the data.\n",
    "    \n",
    "    Once the data is retrieved, it's returned in a list of dictionaries, one for each company. \n",
    "    We'll take this info, convert the price to a float from a string. Then that data is taken \n",
    "    and used to create our indicators in real time. The function calculate indicators is what\n",
    "    calculates all the indicators at each interval.\n",
    "    \n",
    "    If the Google server blocks our retrieval process, we call our yahoo_backup() function \n",
    "    which retrieves the data straight from the yahoo finance's website. This is considerably \n",
    "    slower so it's always better to increase the sleep time between each retrieval to slow down \n",
    "    how often we're retrieving data as this has proved to be effective in combatting being \n",
    "    blocked in the first place. However, once blocked, typically you're unblocked after your \n",
    "    call to yahoo_backup(), unsure why this is but typically the next call to googles servers \n",
    "    will be successful.\n",
    "    \n",
    "    Returns 3 dictionaries:          \n",
    "        -combined   = Dictionary filled with each companies indicators calculated for that day. \n",
    "                      There are 13 indicators ranging from bollinger bands, to PE/Ratios, to \n",
    "                      Volatilities, etc.. as well as daily returns from the 6 major \n",
    "                      indexs/funds/etfs. Each indicator has 32 different values for each \n",
    "                      interval due to calculating for 32 different period lengths at a time.\n",
    "                      \n",
    "        -temp       = Dictionary containing the highs, lows, closing, and typical prices for \n",
    "                      each company at each interval that trading day to be appended to our old \n",
    "                      data after trading day is done.\n",
    "    \"\"\"    \n",
    "    last_close   = {} \n",
    "    ytestlst     = []\n",
    "\n",
    "    count, fail_count = 0, 0\n",
    "    dt_num       = 60. * 1000000000.\n",
    "    rng_tic_lst  = range(len(ticks))\n",
    "    \n",
    "    while True:\n",
    "        now     = datetime.datetime.now().time()\n",
    "        if now >= datetime.time(9,30,1):\n",
    "            break\n",
    "            \n",
    "    # Use python time module to retrieve stock data for the 6.5 hour trading day\n",
    "    seconds, minutes, hours = 60, 60, 6.5\n",
    "    t_end = time.time() + (seconds * minutes * hours) + 60\n",
    "        \n",
    "    while time.time() < t_end:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            # Try to retrieve the quotes from the Google server\n",
    "            next_quotes = getQuotes(ticks) \n",
    "            \n",
    "            for x in rng_tic_lst:\n",
    "                # Set close value with time\n",
    "                name             = ticks[x]\n",
    "                close_value      = float(next_quotes[x]['LastTradePrice'].replace(',',''))\n",
    "                time_value       = pd.to_datetime(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                # Round time to the minute value\n",
    "                time_value       = pd.Timestamp(math.ceil(time_value.value/dt_num)*dt_num)\n",
    "                last_close[name] = [close_value, time_value]\n",
    "                    \n",
    "                # If the latest stock price is a day low or day high, set new high/low \n",
    "                day_hl[name][0]  = min(close_value, day_hl[name][0])\n",
    "                day_hl[name][1]  = max(close_value, day_hl[name][1])\n",
    "                typ_price        = (day_hl[name][1] + day_hl[name][0] + close_value) / 3.\n",
    "                \n",
    "                # Our day's high, low, close and typical prices\n",
    "                temp[name].ix[time_value] = [day_hl[name][1], day_hl[name][0], \n",
    "                                             close_value, typ_price]\n",
    "            \n",
    "            # Call the indicator function which calculates our list of indicators in \n",
    "            #  real time which will be used to feed to the real-time recurrent neural netwk\n",
    "            indics, attr = calculate_indicators(ticks, attr, prds, count, \n",
    "                                                last_close, day_hl, indics)\n",
    "            if len(attr) > 3:\n",
    "                attr, ytestlst  = calculate_neural_output(indics, attr, ytestlst)\n",
    "            \n",
    "            stdout.write(\"\\r%d\" % count)\n",
    "            stdout.flush()\n",
    "            count += 1\n",
    "        except:\n",
    "            # If Google's server denies our retrieval, try calling yahoo_backup()\n",
    "            try:\n",
    "                time_value  = pd.to_datetime(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                time_value  = pd.Timestamp(math.ceil(time_value.value/dt_num)*dt_num)\n",
    "                temp        = yahoo_backup(ticks, day_hl, temp, time_value)\n",
    "                \n",
    "                print \"PRIMARY FAIL\", count\n",
    "                count += 1\n",
    "                fail_count += 1\n",
    "            except:\n",
    "                count += 1\n",
    "                fail_count += 1\n",
    "                \n",
    "                print \"SECONDARY FAIL\"\n",
    "                pass\n",
    "            pass\n",
    "        \n",
    "        # Stop the timing, and subtract this from 60 seconds,ie. every loop = 60secs\n",
    "        cycle_time = time.time() - start\n",
    "        if cycle_time < 58:\n",
    "            time.sleep(60 - cycle_time)\n",
    "    \n",
    "    if len(attr) > 3:\n",
    "        temp = [temp, ytestlst]\n",
    "\n",
    "    return indics, temp, attr\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def calculate_indicators(ticks, attr, prd_dict, count, last_close, day_hl, indicators):\n",
    "    \"\"\"\n",
    "    Calculate indicators in real-time using a combination of data sources, firstly some \n",
    "    pre-calculated dictionaries that are updated after each time this function is called.\n",
    "    These indicators are also calculated using the high/low/close/typical values that are\n",
    "    retrieved and updated every minute from the Google servers. \n",
    "    \n",
    "    We first update our highlowclose dictionary. For each company(tick stands for ticker, \n",
    "    i.e. company/stock) we calculate 13 different indicators, and for each indicator, we \n",
    "    calculate using 32 different period value lengths, essentially calculating 416 indicator\n",
    "    values for each company we're targeting. After each period length calc, we append to a \n",
    "    list and after all 32 period lengths are calculated, we append to our indicator dictionary\n",
    "    for the day.\n",
    "    \"\"\"\n",
    "    hlc_dict  = attr[0]\n",
    "    trpm_dict = attr[1]\n",
    "    dx_dict   = attr[2]\n",
    "    \n",
    "    for name in ticks:\n",
    "        indicator_list, dx_lst = [], []\n",
    "        comp_dxs  = dx_dict[name]\n",
    "\n",
    "        # Todays current low, high, close, time, typical value\n",
    "        day_lo    = day_hl[name][0]\n",
    "        day_hi    = day_hl[name][1]\n",
    "        day_cls   = last_close[name][0]\n",
    "        day_time  = last_close[name][1]\n",
    "        day_typ   = (day_lo + day_hi + day_cls) / 3.\n",
    "        \n",
    "        # Add new values to our dictionary\n",
    "        hlc_dict[name].loc[day_time] = [day_hi, day_lo, day_cls, day_typ]\n",
    "        \n",
    "        # Comps days highs/lows/typicals at each time interval since 2012 and prev years closes\n",
    "        highs       = hlc_dict[name]['Highs']\n",
    "        lows        = hlc_dict[name]['Lows']\n",
    "        typs        = hlc_dict[name]['Typical']\n",
    "        clss        = hlc_dict[name]['Closes']\n",
    "        \n",
    "        # len_prds are pre-calcd prd lengths, length_clss used to create closing prd dfs\n",
    "        len_prds    = prd_dict[name]\n",
    "        length_clss = len(clss)\n",
    "        \n",
    "        # Each company has 32 different period length values to find the most useful period values\n",
    "        for x in range(32):\n",
    "            len_prd    = len_prds[0][x]\n",
    "            len_prd1x0 = len_prds[1][x]\n",
    "            len_prd1x1 = len_prds[1][x+1]\n",
    "            len_prd2x0 = len_prds[2][x]\n",
    "            len_prd2x1 = len_prds[2][x+1]\n",
    "            len_prd2x3 = len_prds[2][x+3]\n",
    "            len_prd3x0 = len_prds[3][x]\n",
    "            \n",
    "            # prd_start is starting value for period(ex. 5days ago)\n",
    "            prd_start       = length_clss - len_prd\n",
    "            prd_start2x3    = length_clss - len_prd2x3\n",
    "            prd_start1x0    = length_clss - len_prd1x0\n",
    "            \n",
    "            # New closing period dataframes from prd start to present\n",
    "            #  Some have larger period lengths due to rolling calculations\n",
    "            #  that need the extra length to calculate properly\n",
    "            clss_prd        = clss.iloc[prd_start:]\n",
    "            clss_prd2       = clss.iloc[prd_start1x0  - len_prd1x1:]\n",
    "            clss_prd3       = clss.iloc[prd_start2x3  - len_prd2x0:]\n",
    "            clss_prd4       = clss.iloc[prd_start2x3  - len_prd2x3:]\n",
    "            typ_prd         = typs.iloc[prd_start:]\n",
    "            lo_prd          = lows.iloc[prd_start1x0  - len_prd1x1:]\n",
    "            hi_prd          = highs.iloc[prd_start1x0 - len_prd1x1:]\n",
    "            \n",
    "            # Price at the beginning of that period (Ex. the price 5 days ago..)\n",
    "            clss_prd_0price = clss_prd.iloc[0]\n",
    "            prev_hi         = highs.iloc[prd_start]\n",
    "            prev_lo         = lows.iloc[prd_start]\n",
    "            \n",
    "            # Retrieve historical tr/posdm/mindm and dx vals\n",
    "            trpm_name       = name+str(x)\n",
    "            tr_df           = trpm_dict[trpm_name]\n",
    "            dx_df           = comp_dxs[x]\n",
    "            \n",
    "\n",
    "            #____________________INDICATOR CALCULATIONS:_________________________\n",
    "            #\n",
    "            # Average Directional Index:\n",
    "            new_tr     = max((day_hi-day_lo),(day_hi-clss_prd_0price),(day_lo-clss_prd_0price))\n",
    "            new_highdm = day_hi  - prev_hi\n",
    "            new_lowdm  = prev_lo - day_lo\n",
    "            if new_highdm >= new_lowdm:\n",
    "                new_plusdm  = max(new_highdm, 0.)\n",
    "                new_minusdm = 0.\n",
    "            else:\n",
    "                new_plusdm  = 0.\n",
    "                new_minusdm = max(new_lowdm, 0.)\n",
    "            tr_df.loc[day_time] = [new_tr, new_plusdm, new_minusdm]\n",
    "            atr_pn    = (tr_df.ewm(ignore_na=False,span=len_prd,min_periods=0,\n",
    "                                   adjust=True).mean()).iloc[-1]\n",
    "            atr, pos_dm, neg_dm = atr_pn[0], atr_pn[1], atr_pn[2]\n",
    "            pos_di    = (pos_dm / atr) * 100.\n",
    "            neg_di    = (neg_dm / atr) * 100.\n",
    "            if (pos_di + neg_di) != 0:\n",
    "                dx    = abs(pos_di - neg_di) / (pos_di + neg_di)\n",
    "            else:\n",
    "                dx    = 0.\n",
    "            dx_df.loc[day_time] = dx\n",
    "            dx_lst.append(dx)\n",
    "            adx       = (dx_df.ewm(ignore_na=False,span=len_prd,min_periods=0,\n",
    "                                   adjust=True).mean()).iloc[-1] * 100.\n",
    "            # Moving Average Convergence Divergence:\n",
    "            small     = clss_prd4.ewm(ignore_na=False,span=len_prd2x1,min_periods=0,\n",
    "                                      adjust=True).mean()\n",
    "            large     = clss_prd4.ewm(ignore_na=False,span=len_prd2x3,min_periods=0,\n",
    "                                      adjust=True).mean()\n",
    "            macd_df   = large[len_prd2x3:] - small[len_prd2x3:]\n",
    "            signal    = macd_df.ewm(ignore_na=False,span=len_prd2x0,min_periods=0,\n",
    "                                    adjust=True).mean()\n",
    "            macd      = macd_df.iloc[-1]\n",
    "            macd2     = (macd / signal.iloc[-1]) - 1.\n",
    "            # Relative Strength Index:\n",
    "            deltas    = clss_prd3 - clss_prd3.shift(len_prd2x0)\n",
    "            up, down  = deltas.copy(), deltas.copy()\n",
    "            up[up < 0], down[down > 0] = 0., 0.\n",
    "            avg_g     = up[len_prd2x0:].mean()\n",
    "            avg_l     = down[len_prd2x0:].mean()\n",
    "            if avg_l != 0:\n",
    "                if (avg_g / avg_l) != -1:\n",
    "                    rsi = 100. - (100. / (1. + (avg_g / avg_l)))\n",
    "                else:\n",
    "                    rsi = 100.\n",
    "            else:\n",
    "                rsi   = 0.\n",
    "            # Aroon index:\n",
    "            lenfl     = float(len_prd)\n",
    "            aro_pos   = ((len_prd - clss_prd.index.get_loc(clss_prd.idxmax())) \n",
    "                                                             / lenfl) * 100.\n",
    "            aro_neg   = ((len_prd - clss_prd.index.get_loc(clss_prd.idxmin()))\n",
    "                                                             / lenfl) * 100.\n",
    "            aro       = aro_pos - aro_neg\n",
    "            # Stochastic Oscillators:\n",
    "            hi_max    = hi_prd.rolling(window=len_prd1x1, center=False).max()\n",
    "            lo_min    = lo_prd.rolling(window=len_prd1x1, center=False).min()\n",
    "            kdo_k     = ((clss_prd2 - lo_min) / \n",
    "                         (hi_max - lo_min).replace(0, np.NaN) * 100.).fillna(0.)\n",
    "            d         = kdo_k[len_prd1x1:].mean()\n",
    "            # Bollinger Bands:\n",
    "            bol_mean  = clss_prd.mean()\n",
    "            bol_std   = clss_prd.std()\n",
    "            bol_uppr  = bol_mean + bol_std * 2.\n",
    "            bol_upmn  = bol_uppr - bol_mean\n",
    "            if bol_upmn != 0:\n",
    "                bol   = (day_cls - bol_mean) / (bol_uppr - bol_mean)\n",
    "            else:\n",
    "                bol   = 0.\n",
    "            # Commodity Channel Index:\n",
    "            typ_mean  = typ_prd.mean()\n",
    "            typ_std   = typ_prd.std()\n",
    "            if typ_std != 0:\n",
    "                cci   = (day_typ - typ_mean) / (0.015 * typ_std)\n",
    "            else:\n",
    "                cci   = 0.\n",
    "            #Simple Moving Average:\n",
    "            sma       = clss_prd.mean()\n",
    "            # Price/Earnings Ratio:\n",
    "            if (day_cls - clss_prd_0price) != 0:\n",
    "                pe    = day_cls / (day_cls - clss_prd_0price)\n",
    "            else:\n",
    "                pe    = 0.\n",
    "            # Momentum:\n",
    "            mom       = ((day_cls - clss_prd_0price) / clss_prd_0price) * 100.\n",
    "            # Volatility:\n",
    "            vol_ret   = (clss_prd3 / clss_prd3.shift(len_prd2x0) - 1.).fillna(0.)\n",
    "            vol       = vol_ret[len_prd2x0:].std() * np.sqrt(float(len_prd2x3))\n",
    "            # Returns:\n",
    "            rets      = ((clss_prd / clss_prd.shift(len_prd3x0)) - 1.).iloc[-1]\n",
    "             \n",
    "            ######################\n",
    "            # Combine indicators #\n",
    "            ######################\n",
    "            indicator_list.append([rsi,vol,sma,cci,pe,mom,bol,aro,macd,macd2,adx,d,rets])\n",
    "            #______________________________________________\n",
    "        \n",
    "        comp_dxs.loc[day_time] = dx_lst\n",
    "        # indicator list is added to prev indicator values for day\n",
    "        df_attrs = []\n",
    "        for y in range(13):\n",
    "            indicator_vals = []\n",
    "            for z in range(32):\n",
    "                indicator_vals.append(indicator_list[z][y])\n",
    "            df_attrs = df_attrs + indicator_vals\n",
    "        indicators[name].loc[day_time] = df_attrs\n",
    "    attr = [hlc_dict, trpm_dict, dx_dict]\n",
    "    return indicators, attr\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def calculate_neural_output(indicators, nn_attr, ytestlst):\n",
    "    \"\"\"\n",
    "    Predict the output for the company we chose at the beginning and run our neural network\n",
    "    test using our precalculated neural network that is re-updated after each trading day. \n",
    "    The col_lst list is a list with our chosen inputs we will use to make our predictions. The\n",
    "    previous input and output are used to help the prediction. \n",
    "    \n",
    "    We combine the indicators from our base stocks, as well as our chosen company, then we \n",
    "    convert to a numpy array that's needed for efficient calculation. The previous inputs \n",
    "    and outputs only use the previous 5 datapoints so we update this after each round. \n",
    "    \n",
    "    We then append our prediction to our prediction list that will be used to compare the test \n",
    "    outputs to real outputs. We'll use these predictions to decide whether or not to \n",
    "    buy/sell/short/stay.\n",
    "    \n",
    "    Return our prediction, the attributes used to predict them, as well as the prediction list\n",
    "    for comparison purposes EOD.\n",
    "    \"\"\"\n",
    "    combined   = pd.DataFrame()\n",
    "    net        = nn_attr[3]\n",
    "    col_lst    = nn_attr[4]\n",
    "    prev_in    = nn_attr[5]\n",
    "    prev_out   = nn_attr[6]\n",
    "    company    = nn_attr[7]\n",
    "    x          = nn_attr[8]\n",
    "    \n",
    "    if ytestlist != []:\n",
    "        predicts     = ytestlst[0]\n",
    "        pred_choices = ytestlst[1]\n",
    "    else:\n",
    "        predicts     = []\n",
    "        pred_choices = []\n",
    "\n",
    "    comps = ['INDEXNASDAQ:.IXIC', 'INDEXSP:.INX', 'INDEXDJX:.DJI',\n",
    "             'NYSEARCA:USO', 'NYSEARCA:GLD', 'NYSEARCA:SPY', company]\n",
    "    \n",
    "    for key in comps:\n",
    "        combined = pd.concat([combined, indicators[key][col_lst[key]]], axis=1)\n",
    "    combined = combined.iloc[-1]\n",
    "        \n",
    "    output_name = company+'_rets'+str(x)\n",
    "    test_inputs = combined.values.T\n",
    "    \n",
    "    prev_out = prev_out[1:].append([indicators[company][output_name].iloc[-1]])\n",
    "    ytest    = NNOut(test_inputs, net, P0=prev_in, Y0=prev_out)\n",
    "    prev_in  = prev_in[1:].append(test_inputs)\n",
    "    \n",
    "    predicts = predicts.append(ytest)\n",
    "    \n",
    "    if ytest > 0:\n",
    "        pred_choices = pred_choices.append('Buy')\n",
    "    elif ytest < 0:\n",
    "        pred_choices = pred_choices.append('Sell')\n",
    "    else:\n",
    "        pred_choices = pred_choices.append('Stay')\n",
    "        \n",
    "    ytestlst   = [predicts, pred_choices]\n",
    "    nn_attr    = nn_attr[:3] + [net, col_lst, prev_in, prev_out, company, x]\n",
    "    \n",
    "    return nn_attr, ytestlst\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def get_data_ready(choices=[]):\n",
    "    \"\"\"\n",
    "    First function called at the beginning of the trading day.\n",
    "    \n",
    "    Setup the stock ticker symbols for both the Google server and data dictionary. Then get\n",
    "    the pickled files for the historical intraday high/low/close/typical price dataframes \n",
    "    for each company thats in a dictionary. Also retrieve the news dictionary file for testing\n",
    "    whether a news story is new or already processed.\n",
    "    \n",
    "    Then find the companies with upcoming stock splits and dividends and compare them to \n",
    "    our list to see if we have any companies with upcoming dates so we can adjust for them. \n",
    "    As of right now (2016-09-03), the function to adjust has not been created but will be soon.\n",
    "    \n",
    "    Finally call the company_news() function to retrieve info for each company and \n",
    "    index/fund/etf for new articles on them. As of right now the function to automatically \n",
    "    read the data, and convert the data to extra information to help in prediction, has not \n",
    "    been created but will be created soon. It will utilize a neural network to comprehend \n",
    "    the data, possible using Google's new Parsy McParseFace module just released that is \n",
    "    built for NLP(Natural Language Processing).\n",
    "    \"\"\"\n",
    "    # Dictionaries that will be used throughout program\n",
    "    indicatorlist  = {}\n",
    "    day_hl         = {}\n",
    "    temp           = {}\n",
    "    dates          = {}\n",
    "    \n",
    "    # Ticker symbols for google servers\n",
    "    tickers        = {'AAPL':'AAPL','NYSEARCA:USO':'USO','NYSEARCA:GLD':'GLD',\n",
    "                      'NYSEARCA:SPY':'SPY','INDEXDJX:.DJI':'^DJI','INDEXSP:.INX':'^GSPC',\n",
    "                      'INDEXNASDAQ:.IXIC':'^IXIC', 'NYSE:LMT':'LMT'}\n",
    "    tickers2       = ['BPOP','FITB','HBAN','CMCSA','EBAY','AAPL','AMAT','BRCD','CSCO',\n",
    "                      'GOOG','INTC','LVLT','MSFT','MU','NVDA','ORCL','QCOM','SIRI','WIN',\n",
    "                      'YHOO','BHP','BP','RIO','XOM','GE','F','MO','XRX','GS','JPM','LYG',\n",
    "                      'MS','RF','USB','WFC','MRK','PFE','NYSE:LMT','MGM','AMD','GLW',\n",
    "                      'HPQ','S','T','NYSEARCA:USO','NYSEARCA:GLD','NYSEARCA:SPY',\n",
    "                      'INDEXDJX:.DJI','INDEXSP:.INX','INDEXNASDAQ:.IXIC']\n",
    "    hlc_cols       = ['Highs','Lows','Closes','Typical']\n",
    "    \n",
    "    plnums   = [10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300]\n",
    "    plnums2  = [8, 10, 14, 16, 18, 20, 25, 30, 40, 50, 75,  100, 125, 150, 200, 250, 300, 350]\n",
    "    plnums3  = [6, 8, 10, 14, 16, 18, 20, 25, 30, 40, 50,  75,  100, 125, 150, 200, 250, 300, 350]\n",
    "    plnums4  = [1,2,3,4,5, 10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300]\n",
    "    rs       = NYSE_tradingdays()\n",
    "    \n",
    "    #Historical intraday trading data for every company/index/sector at 1 min intervals\n",
    "    opp         = open('Pickles/shortpickleintra.pickle', 'rb')\n",
    "    opp2        = open('Pickles/newsdict.pickle', 'rb')\n",
    "    opp3        = open('NewBase/ADXD/adx_d3.pickle', 'rb')\n",
    "    opp4        = open('NewBase/ADXD/adx_d2.pickle', 'rb')\n",
    "    opp5        = open('Pickles/columnnames.pickle', 'rb')\n",
    "    hlc_short   = pickle.load(opp)\n",
    "    news_dict   = pickle.load(opp2)\n",
    "    trpm_dict   = pickle.load(opp3)\n",
    "    dx_dict     = pickle.load(opp4)\n",
    "    nm_dict     = pickle.load(opp5)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    opp3.close()\n",
    "    opp4.close()\n",
    "    opp5.close()\n",
    "    \n",
    "    if choices != []:\n",
    "        comb_df, output_df, cols, net = get_neural_inout(choices)\n",
    "    \n",
    "    ticks = []\n",
    "    for key in tickers2:\n",
    "        if key in hlc_short.keys():\n",
    "            ticks.append(key)\n",
    "    \n",
    "    # Get initial quotes for beginning highs/lows\n",
    "    initial_quotes = getQuotes(ticks) \n",
    "    for key, y in zip(ticks, range(len(ticks))):\n",
    "        # temp used to store our days highs, lows, closes, and typical prices\n",
    "        # indicatorlist will contain our indicator values calculated for the day\n",
    "        indicatorlist[key] = pd.DataFrame(columns=nm_dict[key]) \n",
    "        temp[key]          = pd.DataFrame(columns=[hlc_cols])\n",
    "        \n",
    "        # day_hl is day highs/lows used to keep track throughout day, start at initial day quote\n",
    "        day_hl[key] = [float(initial_quotes[y]['LastTradePrice'].replace(',','')), \n",
    "                       float(initial_quotes[y]['LastTradePrice'].replace(',',''))]\n",
    "        \n",
    "        # Create period dictionaries which gives the period lengths for each companies indicators\n",
    "        tl = []\n",
    "        for x in xrange(268, 135, -1):\n",
    "            try:\n",
    "                dt   = str(rs[x])[:10]\n",
    "                test = hlc_short[key][dt]\n",
    "                tl.append(dt)\n",
    "                if len(tl) > 125:\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "        prd_lst    = [tl[1], tl[2], tl[3], tl[5], tl[8], tl[10], tl[12], tl[14], tl[16], \n",
    "                      tl[20], tl[25], tl[30], tl[40], tl[50], tl[80], tl[125]]\n",
    "        dates[key] = prd_lst \n",
    "    prd_dict = create_prd_lst2(hlc_short, dates, plnums, plnums2, plnums3, plnums4)\n",
    "        \n",
    "    try:\n",
    "        # Find companies with upcoming dividends and stock splits\n",
    "        div_dates = 'http://www.nasdaq.com/dividend-stocks/dividend-calendar.aspx'\n",
    "        spl_dates = 'http://www.nasdaq.com/markets/upcoming-splits.aspx'\n",
    "        dividend  = pd.DataFrame(dvds(div_dates)[0],columns=['Sym','ExDiv','Div','RecDt','PayDt'])\n",
    "        split     = pd.DataFrame(splits(spl_dates),columns=['Sym','ExDt','Ratio','Payab'])\n",
    "        divs      = dividend['Sym'].values\n",
    "        spls      = split['Sym'].values\n",
    "        divs_dts  = dividend['ExDiv'].values\n",
    "        spls_dts  = split['ExDt'].values\n",
    "\n",
    "        # Print out which companies have stock splits and dividends upcoming, if any\n",
    "        divspl_lst = [divs, spls]\n",
    "        divspl_dts = [divs_dts, spls_dts]\n",
    "        for i in  range(2):\n",
    "            lst = divspl_lst[i]\n",
    "            dt  = divspl_dts[i]\n",
    "            for ii in range(len(lst)):\n",
    "                comp = lst[ii]\n",
    "                date = dt[ii]\n",
    "                if comp in ticks:\n",
    "                    if i == 0:\n",
    "                        print comp, \"DIV\", date\n",
    "                    else:\n",
    "                        print comp, \"SPLITS\", date\n",
    "                if comp in tickers.keys():\n",
    "                    if i == 0:\n",
    "                        print tickers[comp], \"DIV\", date\n",
    "                    else:\n",
    "                        print tickers[comp], \"SPLITS\", date\n",
    "        \n",
    "        # Call our trading news function to get any current news stories for each comp\n",
    "        #new_trading_news(news_dict, tickers2)\n",
    "    except:\n",
    "        print \"DIVSPLNEWS FAIL\"\n",
    "        pass\n",
    "    \n",
    "    if choices != []:\n",
    "        attr = [hlc_short, trpm_dict, dx_dict, net, cols, \n",
    "                comb_df, output_df, company_choice, output_choice]\n",
    "    else:\n",
    "        attr = [hlc_short, trpm_dict, dx_dict]\n",
    "    \n",
    "    return tickers2, attr, prd_dict, indicatorlist, day_hl, temp, nm_dict, tickers\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neural_inout(choices, final):\n",
    "    company_choice = choices[0]\n",
    "    output_choice  = choices[1]\n",
    "    \n",
    "    fl_name  = 'Pickles/net222'+company_choice+'.pickle'\n",
    "    fl_name2 = 'Pickles/final_lst222'+company_choice+'.pickle'\n",
    "    opp      = open(fl_name,'rb')\n",
    "    opp2     = open(fl_name2, 'rb')\n",
    "    net      = pickle.load(opp)\n",
    "    final    = pickle.load(opp2)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    \n",
    "    comb_df, cols = pd.DataFrame(), {}\n",
    "    lst  = ['^GSPC', '^IXIC', '^DJI', 'GLD', 'USO', 'SPY', company_choice]\n",
    "    lst2 = {'^IXIC':'INDEXNASDAQ:.IXIC', '^GSPC':'INDEXSP:.INX', \n",
    "            '^DJI':'INDEXDJX:.DJI', 'USO':'NYSEARCA:USO', 'GLD':'NYSEARCA:GLD', \n",
    "            'SPY':'NYSEARCA:SPY'}\n",
    "    \n",
    "    for nm in lst:\n",
    "        if nm in lst2.keys():\n",
    "            nm = lst2[nm]\n",
    "        cols[nm] = []\n",
    "\n",
    "    for name in final:\n",
    "        key = name[:name.find('_')]\n",
    "        key_base = name[name.find('_'):]\n",
    "\n",
    "        ind = name[name.find('_')+1:name.find('_')+5]\n",
    "        ind_base = name[:name.find('_')+5]\n",
    "\n",
    "        if ind != 'rets':\n",
    "            if key not in cols.keys():\n",
    "                key  = company_choice\n",
    "                name = key+key_base\n",
    "            else:\n",
    "                name = name\n",
    "        else:\n",
    "            name = ind_base+str(output_choice)\n",
    "\n",
    "        if key == '^GSPC':\n",
    "            key = lst2[key]\n",
    "            cols[key] = cols[key] + [name]\n",
    "        elif key == '^IXIC':\n",
    "            key = lst2[key]\n",
    "            cols[key] = cols[key] + [name]\n",
    "        elif key == '^DJI':\n",
    "            key = lst2[key]\n",
    "            cols[key] = cols[key] + [name]\n",
    "        elif key == 'GLD':\n",
    "            key = lst2[key]\n",
    "            cols[key] = cols[key] + [name]\n",
    "        elif key == 'USO':\n",
    "            key = lst2[key]\n",
    "            cols[key] = cols[key] + [name]\n",
    "        elif key == 'SPY':\n",
    "            key = lst2[key]\n",
    "            cols[key] = cols[key] + [name]\n",
    "        else:\n",
    "            cols[key] = cols[key] + [name]\n",
    "            \n",
    "    for nm in lst:\n",
    "        if nm != company_choice:\n",
    "            nm2 = lst2[nm]\n",
    "        else:\n",
    "            nm2 = nm\n",
    "        opp = open('NewBase/'+nm+'/'+nm+'_df14.pickle','rb')\n",
    "        dff = pickle.load(opp)\n",
    "        opp.close()\n",
    "        len_dff = len(dff)\n",
    "        comb_df = pd.concat([comb_df, dff[cols[nm2]].iloc[len_dff-5:len_dff]], \n",
    "                            axis=1).fillna(method='ffill').fillna(method='bfill')\n",
    "        if nm == company_choice:\n",
    "            output_df = dff[company_choice+'_rets'+str(output_choice)].iloc[len_dff-4:]\n",
    "            \n",
    "    return comb_df, output_df, cols, net\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_files_EOD2(inds, hlc, ticks, ticks2, cols, attr, prds):\n",
    "    \"\"\"\n",
    "    Update our indicators and highlowclose dictionary at the end of the trading day, appending\n",
    "    our day results to our historical values.\n",
    "    \"\"\"\n",
    "    old_short   = attr[0]\n",
    "    adx_d2      = attr[1]\n",
    "    adx_d3      = attr[2]\n",
    "    \n",
    "    short_lst   = get_short_index(old_short)\n",
    "    short_d3    = get_short_index(adx_d3)\n",
    "    opp         = open('Pickles/pickleadjustedintracomplete.pickle','rb')\n",
    "    old_hlc     = pickle.load(opp)\n",
    "    opp.close()\n",
    "\n",
    "    for name in ticks:\n",
    "        if name in ticks2.keys():\n",
    "            name2 = ticks2[name]\n",
    "        else:\n",
    "            name2 = name  \n",
    "        file_name  = 'NewBase/'+name2+'/'+name2+'_df14.pickle'\n",
    "        \n",
    "        for x in range(32):\n",
    "            name3  = key+str(x)\n",
    "            length = len(adx_d2[name3])\n",
    "            if (prds[key][0][x]+5) > length:\n",
    "                adx_d2[name3] = adx_d2[name3]\n",
    "            else:\n",
    "                adx_d2[name3] = adx_d2[name3].iloc[length-prds[name][0][x]-5:]\n",
    "\n",
    "        opp  = open(file_name,'rb')\n",
    "        df14 = pickle.load(opp)[cols[name]]\n",
    "        opp.close()\n",
    "\n",
    "        old_short[name] = old_short[name].loc[short_lst[name][0]:]\n",
    "        adx_d3[name] = adx_d3[name].loc[short_d3[name][0]:]\n",
    "        old_hlc[name2] = old_hlc[name2].append(hlc[name])\n",
    "        df14 = df14.append(inds[name][cols[name]])\n",
    "\n",
    "        opp = open(file_name,'wb')\n",
    "        pickle.dump(df14, opp)\n",
    "        opp.close()\n",
    "\n",
    "    opp  = open('Pickles/pickleadjustedintracomplete2.pickle','wb')\n",
    "    opp2 = open('Pickles/shortpickleintra2.pickle','wb')\n",
    "    opp3 = open('NewBase/ADXD/adx_d22.pickle','wb')\n",
    "    opp4 = open('NewBase/ADXD/adx_d32.pickle','wb')\n",
    "    pickle.dump(old_hlc, opp)\n",
    "    pickle.dump(old_short, opp2)\n",
    "    pickle.dump(adx_d3, opp3)\n",
    "    pickle.dump(adx_d2, opp4)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    opp3.close()\n",
    "    opp4.close()\n",
    "    return\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_program(choices=[]):\n",
    "    \"\"\"\n",
    "    Function will wait until the start of the training day(9:30AM) and start the\n",
    "    process about 2 minute before start by calling get_data_ready() which takes roughly\n",
    "    2 minutes to process everything so it starts recording and calculating in real-time\n",
    "    at about 9:30.\n",
    "    \n",
    "    Until the time is between 9:28 and 9:30, it will check if we're in that range, else\n",
    "    it sleeps for 60 seconds and does this until its called.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(30)\n",
    "        now      = datetime.datetime.now()\n",
    "        now_time = now.time()\n",
    "        \n",
    "        if now_time >= datetime.time(9,24) and now_time <= datetime.time(9,30):\n",
    "            if choices != []:\n",
    "                ticks,attr,prds,indiclst,dayhl,temp,nm_dict,ticks2=get_data_ready(choices)\n",
    "            else:\n",
    "                ticks,attr,prds,indiclst,dayhl,temp,nm_dict,ticks2=get_data_ready()\n",
    "                \n",
    "            inds, temp, attr = real_time_quotes(ticks, attr, prds, indiclst, dayhl, temp)\n",
    "            break\n",
    "    \n",
    "    return inds, temp, attr, ticks, ticks2, nm_dict, prds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390"
     ]
    }
   ],
   "source": [
    "#choices = ['AAPL', 0]\n",
    "#inds, hlc, attr, ticks, ticks2, cols, prds = run_program(choices)\n",
    "inds, hlc, attr, ticks, ticks2, cols, prds = run_program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#update_files_EOD(inds, hlc, ticks, ticks2, cols, attr, prds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
