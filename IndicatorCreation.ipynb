{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "from nyse_dates_prds import *\n",
    "from sys import stdout \n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def returns_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This calculates the return values which is the closing price divided by the previous\n",
    "    period's closing price minus 1. This gets us a percentage increase or decrease over\n",
    "    that time period.\n",
    "    \"\"\"\n",
    "    rets_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        # Return value periods have the normal periods plus a 1,2,3,4 and 5 minute period\n",
    "        prds   = [1,2,3,4,5] + periods[name][0]\n",
    "        \n",
    "        # There's 32 periods to calculate returns for, we put them all in a dataframe\n",
    "        for x in range(32):\n",
    "            df[x] = (((close / close.shift(prds[x])) - 1.).fillna(0)).replace([np.inf], 0)\n",
    "        \n",
    "        rets_dict[name] = df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(rets_dict, 'rets', highlowclose, first=True)\n",
    "    else:\n",
    "        dump_indicator_data(rets_dict, 'rets')\n",
    "    print \"RETS DONE\"\n",
    "    return\n",
    "\n",
    "def per_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    Calculates the price to earnings ratio which is the closing price divided by the\n",
    "    difference between the closing price and the previous period's closing price.\n",
    "    \"\"\"\n",
    "    per_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        cl     = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        # For each different period, first subtract current closing price by previous\n",
    "        # periods closing price, replacing any zero with NaN to prevent divide by zero\n",
    "        # bug in pandas and then convert NaN back to zero after the series division done\n",
    "        for x in range(32): \n",
    "            clshf = (cl - cl.shift(prds[x])).replace(0,np.NaN)\n",
    "            df[x] = (cl / clshf).fillna(0)\n",
    "        \n",
    "        per_dict[name] = df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(per_dict, 'per', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(per_dict, 'per')\n",
    "    print \"PER DONE\"\n",
    "    return\n",
    "\n",
    "def kdo_create(highlowclose, periods, dmp_method):  \n",
    "    \"\"\"\n",
    "    This is the stochastic oscillator calculation, which takes each periods max and min\n",
    "    value. Then we first subtract the closing price minus the minimum, then we subtract\n",
    "    the max from the min. We then divide our first calculation by the second and multiply\n",
    "    by 100 which gets us the K oscillator. Then we take the rolling mean value of the small\n",
    "    period of the k values to get our D oscillator.\n",
    "    \"\"\"\n",
    "    d_dict, count = {}, 0\n",
    "    for name in highlowclose.keys(): \n",
    "        d_df       = pd.DataFrame()\n",
    "        high, low  = highlowclose[name]['Highs'], highlowclose[name]['Lows']\n",
    "        close      = highlowclose[name]['Closes']\n",
    "        prds       = periods[name][1]\n",
    "        \n",
    "        # For each period, there's a small and large period, large for k oscillator calc\n",
    "        # and the small for the d oscillator using the k oscillators.\n",
    "        for x in range(32):\n",
    "            num  = prds[x]\n",
    "            num2 = prds[x+1]\n",
    "            \n",
    "            # Get rolling period minimums and maximums for lows and highs respectively\n",
    "            prev_max  = high.rolling(window = num2, center = False).max()\n",
    "            prev_min  = low.rolling(window  = num2, center = False).min()\n",
    "        \n",
    "            cl  =  close    - prev_min\n",
    "            hl  = (prev_max - prev_min).replace(0., np.NaN)\n",
    "            \n",
    "            # K and D oscillators, the D which is important to us\n",
    "            k_df    = ((cl / hl) * 100.).fillna(0.)\n",
    "            d_df[x] = (k_df.rolling(window=num, center=False).mean()).fillna(0.)\n",
    "            \n",
    "        d_dict[name] = d_df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(d_dict, 'kdo', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(d_dict, 'kdo')\n",
    "    print \"KD DONE\"\n",
    "    return\n",
    "\n",
    "def cci_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is our commodity channel index calculation\n",
    "    \"\"\"\n",
    "    cci_dict, count, constant = {}, 0, 0.015\n",
    "    for name in highlowclose.keys():\n",
    "        df   = pd.DataFrame()\n",
    "        # Typical values are that point's day high + low + close / 3\n",
    "        typ  = highlowclose[name]['Typical']\n",
    "        prds = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num = prds[x]\n",
    "            # Get the standard deviation and the mean of each periods values    \n",
    "            typ_std  = typ.rolling(window = num, center = False).std()\n",
    "            typ_mean = typ.rolling(window = num, center = False).mean()\n",
    "            \n",
    "            # Subtract each pts typical value minus the mean typical\n",
    "            ttmean   = typ - typ_mean\n",
    "            \n",
    "            # Then multiply the standard deviation value times the constant value\n",
    "            #  which is a value that is well known for this indicator but can be\n",
    "            #  changed if you believe you can get better calculations with differnt one\n",
    "            ctmad    = (constant * typ_std).replace(0.,np.NaN)\n",
    "            \n",
    "            # Finally, divide the first calculation by the second to get that periods cci\n",
    "            df[x]    = (ttmean / ctmad).fillna(0.)\n",
    "        \n",
    "        cci_dict[name] = df      \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(cci_dict, 'cci', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(cci_dict, 'cci')\n",
    "    print \"CCI DONE\"\n",
    "    return\n",
    "\n",
    "def vol_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the volatility indicator calculation. We get this by taking the period return \n",
    "    values for that company, and then taking the standard deviation of each periods returns,\n",
    "    and multiplying that by the square root of the period length\n",
    "    \"\"\"\n",
    "    vol_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        vol_df = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][2]\n",
    "        \n",
    "        for x in range(32): \n",
    "            # Uses two different periods, small and large, small for return calc portion,\n",
    "            # and large for the actual vol calculation\n",
    "            num       = prds[x]\n",
    "            num2      = prds[x+3]\n",
    "            rets      = (close / close.shift(num) - 1.).fillna(0.)\n",
    "            vol_df[x] = rets.rolling(window=num2, center=False).std() * np.sqrt(num2)\n",
    "        \n",
    "        vol_dict[name] = vol_df        \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "       \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(vol_dict, 'vol', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(vol_dict, 'vol')\n",
    "    print \"VOL DONE\"\n",
    "    return\n",
    "\n",
    "def bol_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the bollinger band calculation. We get this by finding the periods mean and\n",
    "    standard deviation values, adding those together and multiplying by two to get the\n",
    "    upper band values. We then calculate the difference between closing prices and mean\n",
    "    prices, then subtract the upper band values by the mean prices, and then divide these\n",
    "    two answers to get our final bollinger band values.\n",
    "    \"\"\"\n",
    "    bol_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df    = pd.DataFrame()\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            \n",
    "            # Get rolling period means and standard deviations\n",
    "            rm    = (close.rolling(window=num, center=False).mean()).fillna(0)\n",
    "            rstd  = (close.rolling(window=num, center=False).std()).fillna(0)\n",
    "            \n",
    "            # Create an upper band (lower band is rm - rstd*2) but since our bollinger\n",
    "            # calculation is slightly modified we dont need lower band.\n",
    "            upper = rm + rstd * 2. \n",
    "            clrm  = close - rm\n",
    "            uprm  = (upper - rm).replace(0.,np.NaN)\n",
    "            df[x] = (clrm / uprm).fillna(0.)\n",
    "            \n",
    "        bol_dict[name] = df        \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(bol_dict, 'bol', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(bol_dict, 'bol')\n",
    "    print \"BOL DONE\"\n",
    "    return\n",
    "\n",
    "def mom_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the momentum calculator, which is simply the the difference between current\n",
    "    closing price, and closing price x periods ago, multiplied by 100\n",
    "    \"\"\"\n",
    "    mom_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            df[x] = (((close - close.shift(num)) / close.shift(num)) * 100.).fillna(0.)\n",
    "        \n",
    "        mom_dict[name] = df       \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "       \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(mom_dict, 'mom', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(mom_dict, 'mom')\n",
    "    print \"MOM DONE\"\n",
    "    return\n",
    "\n",
    "def sma_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the simple moving average calculation. This is simply the mean closing price\n",
    "    during that period.\n",
    "    \"\"\"\n",
    "    sma_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df    = pd.DataFrame()\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            \n",
    "            # Rolling mean of closing prices, filling NaN's with zero\n",
    "            df[x] = (close.rolling(window=num, center=False).mean()).fillna(0.)\n",
    "            \n",
    "        sma_dict[name] = df       \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "       \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(sma_dict, 'sma', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(sma_dict, 'sma')\n",
    "    print \"SMA DONE\"\n",
    "    return\n",
    "\n",
    "def aro_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the aroon indicator calculation. For each periods values, you how many periods it has\n",
    "    been since you've had the max during that period as well as the min, and subtract from \n",
    "    eachother\n",
    "    \"\"\"\n",
    "    aro_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            # Using an applied method for each period to cut down on computation time\n",
    "            df[x] = close.rolling(window=num, center=False).apply(aro_apply, args=(num,))\n",
    "            \n",
    "        aro_dict[name] = df       \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(aro_dict, 'aro', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(aro_dict, 'aro')\n",
    "    print \"ARO DONE\"\n",
    "    return\n",
    "def aro_apply(df, prd):\n",
    "    # For each rolling period, find the number of days since last period min/max, then\n",
    "    # divide by the period length and finally multiply by 100 to get the upper/lower aroon bands\n",
    "    # Then subtract the upper by lower values\n",
    "    up      = ((prd - df.argmax()) / float(prd)) * 100.\n",
    "    down    = ((prd - df.argmin()) / float(prd)) * 100.\n",
    "    return up - down\n",
    "\n",
    "def mac_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the moving average convergence divergence calculation. To get this we take the \n",
    "    expected moving average of the smaller period, and the larger period. Subtract from eachother,\n",
    "    then take the expected moving average of the smallest period on those calculated values to get\n",
    "    an buy/sell indicator of the macd calculation and use both of these.\n",
    "    \"\"\"\n",
    "    mac_dict, mac_dict2, count = {}, {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        macd_df, macd_df2  = pd.DataFrame(), pd.DataFrame()\n",
    "        close              = highlowclose[name]['Closes']\n",
    "        prds               = periods[name][2]\n",
    "        \n",
    "        for x in range(32):\n",
    "            # This indicator uses 3 period lengths in its calculation, a small, medium, and large\n",
    "            num, num2, num3 = prds[x], prds[x+1], prds[x+3]\n",
    "            \n",
    "            # ewm stands for expected weighted moving means that its a rolling calculation which\n",
    "            # weights more recent vals more heavily than less recent.             \n",
    "            # Get our mid and large rolling ewm averages of closing prices, subtract them, then            \n",
    "            # get our modified version of the macd which is the ewm average of the macd, div macd             \n",
    "            # by this and subtract one            \n",
    "            mid    = close.ewm(ignore_na=False, span=num2, min_periods=0, adjust=True).mean()\n",
    "            large  = close.ewm(ignore_na=False, span=num3, min_periods=0, adjust=True).mean()\n",
    "            macd   = large - mid\n",
    "            small  = macd.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "            result = (macd / small) - 1.\n",
    "            \n",
    "            # we calculate two indicators for this one.\n",
    "            macd_df[x]  = macd\n",
    "            macd_df2[x] = result\n",
    "        \n",
    "        mac_dict[name]  = macd_df  \n",
    "        mac_dict2[name] = macd_df2\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "      \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files([mac_dict, mac_dict2], 'mactwo', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data([mac_dict, mac_dict2], 'mactwo')\n",
    "    print \"MAC DONE\"\n",
    "    return\n",
    "    \n",
    "def adx_create(highlowclose, periods, short, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the average directional index calculation.\n",
    "    \"\"\"\n",
    "    tickers2      = {'USO':'NYSEARCA:USO','GLD':'NYSEARCA:GLD',\n",
    "                     'SPY':'NYSEARCA:SPY','^DJI':'INDEXDJX:.DJI',\n",
    "                     '^GSPC':'INDEXSP:.INX','^IXIC':'INDEXNASDAQ:.IXIC', \n",
    "                     'LMT':'NYSE:LMT'}\n",
    "    \n",
    "    tr_df_dict, adx_dict, adx_dict2, count = {}, {}, {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        if name in tickers2.keys():\n",
    "            name2 = tickers2[name]\n",
    "        else:\n",
    "            name2 = name \n",
    "        adx_df, dx_df = pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        high   = highlowclose[name]['Highs']\n",
    "        low    = highlowclose[name]['Lows']\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            trpm_df = pd.DataFrame()\n",
    "            num     = prds[x]\n",
    "            \n",
    "            # Get the true range values by subtracting highs and lows, highs and period closes,\n",
    "            #  and the lows and period closes, and take the max at each period\n",
    "            tr_df      = pd.DataFrame()\n",
    "            tr_df[0]   = (high - low)\n",
    "            tr_df[1]   = (high - close.shift(num)).fillna(0.)\n",
    "            tr_df[2]   = (low  - close.shift(num)).fillna(0.)\n",
    "            true_range = tr_df.max(axis=1)\n",
    "            \n",
    "            # Get the positive and negative directional indicators\n",
    "            plus_dm   = (high - high.shift(num)).fillna(0.)\n",
    "            minus_dm  = (low.shift(num) - low).fillna(0.)\n",
    "            \n",
    "            # For each pos/neg direc inds, if pos >= than minus, take positive, else take neg\n",
    "            plus_dm   = pd.Series(np.where(plus_dm >= minus_dm, plus_dm, 0.),index=plus_dm.index)\n",
    "            minus_dm  = pd.Series(np.where(minus_dm >= plus_dm, minus_dm, 0.),index=plus_dm.index)\n",
    "            \n",
    "            # If any values are below 0, set them to 0\n",
    "            plus_dm[plus_dm < 0]   = 0.\n",
    "            minus_dm[minus_dm < 0] = 0.\n",
    "        \n",
    "            # Put the true_range and pos/neg directional indicators into a dataframe so these \n",
    "            # can provide precalculated values for the real-time calculations\n",
    "            trpm_df[0] = true_range\n",
    "            trpm_df[1] = plus_dm\n",
    "            trpm_df[2] = minus_dm\n",
    "            \n",
    "            # Dictionary key value is the company plus the period value\n",
    "            tr_name = name2+str(x)\n",
    "            \n",
    "            # Store these values for use in the real-time calculation\n",
    "            tr_df_dict[tr_name] = trpm_df[len(true_range)-num-1:]\n",
    "            \n",
    "            # Calculate first the average true range, using the expected weighted mean of the\n",
    "            # true ranges, then calculate the positive and negative directional movements\n",
    "            atr    = true_range.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "            pos_dm = plus_dm.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "            neg_dm = minus_dm.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "\n",
    "            # Calculate the positive and negative directional indexes, replacing divide by zero\n",
    "            # infinity bug by zero if any.\n",
    "            pos_di  = ((pos_dm / atr) * 100.).replace([np.inf,-np.inf],0.)\n",
    "            neg_di  = ((neg_dm / atr) * 100.).replace([np.inf,-np.inf],0.)\n",
    "            \n",
    "            # Calculate the directional index, replacing any zero infinity bugs by zero\n",
    "            dx_df[x] = (abs(pos_di - neg_di) / (pos_di + neg_di)).replace([np.inf,-np.inf],0.)\n",
    "            \n",
    "            # Take the EMaverage of the directional index to get the average directional index\n",
    "            adx_df[x] = dx_df[x].ewm(ignore_na=False, span=num, min_periods=0, \n",
    "                                     adjust=True).mean() * 100.\n",
    "        \n",
    "        adx_dict[name] = adx_df\n",
    "        adx_dict2[name2] = dx_df[short:]\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    base = 'NewBase/ADXD/'\n",
    "    if not os.path.exists(base):\n",
    "        os.makedirs(base)\n",
    "    opp  = open('NewBase/ADXD/adx_d2.pickle','wb')\n",
    "    opp2 = open('NewBase/ADXD/adx_d3.pickle','wb')\n",
    "    pickle.dump(adx_dict2, opp)\n",
    "    pickle.dump(tr_df_dict, opp2)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(adx_dict, 'adx', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(adx_dict, 'adx')\n",
    "    print \"ADX DONE\"\n",
    "    return\n",
    "\n",
    "def rsi_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the relative strength index calculation.\n",
    "    \"\"\"\n",
    "    rsi_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        rsi_df = pd.DataFrame()\n",
    "        \n",
    "        close   = highlowclose[name]['Closes']\n",
    "        prds    = periods[name][2]\n",
    "\n",
    "        for x in range(32):\n",
    "            # Uses both small and large periods\n",
    "            num  = prds[x]\n",
    "            num2 = prds[x+3]\n",
    "            \n",
    "            # Get closing price differences using small period\n",
    "            deltas  = (close - close.shift(num)).fillna(0.)\n",
    "            \n",
    "            # set up/down df as values above/below zero setting others to 0\n",
    "            up, down = deltas.copy(), deltas.copy()\n",
    "            up[up < 0]     = 0\n",
    "            down[down > 0] = 0\n",
    "\n",
    "            # Get rolling means of up/down dfs with down dfs taking absolute vals\n",
    "            rolup_df   =  up.rolling(window=num2, center=False).mean()\n",
    "            roldown_df = (down.rolling(window=num2, center=False).mean()).abs()\n",
    "\n",
    "            rol_updown = (rolup_df / roldown_df).replace([np.inf,-np.inf],0.)\n",
    "            rsi_df[x] = (100. - (100. / (1. + rol_updown))).replace([np.inf,-np.inf],0.)\n",
    "\n",
    "        rsi_dict[name]  = rsi_df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    # If dmp_method is 0, we update the newbase files directly, else we do our \n",
    "    # intermediary step that is better for time, but uses more space temporarily\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(rsi_dict, 'rsi', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(rsi_dict, 'rsi')\n",
    "    print \"RSI DONE\"\n",
    "    return\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def update_indicators(highlowclose, prd_dict, short, dmp_method, update=False):\n",
    "    \"\"\"\n",
    "    Create our indicators using the highlowclose dictionary, containing all the \n",
    "    dataframes with highs/lows/closes/typicals for each company, the period dict\n",
    "    with all of our period length values, and the dmp_method which is the preferred\n",
    "    way for us to create our newbase indicator files, where passing 0 means we\n",
    "    directly add the indicators to the newbase files after each indicator is created\n",
    "    rather than passing 1 which means we create intermediarry files containing our\n",
    "    indicators in the Companies/ directory where each company will have its own \n",
    "    directory containing all of its indicators. Once this is done, we combine them \n",
    "    for each company. \n",
    "    \n",
    "    The reason for the intermediary step is to reduce memory \n",
    "    consumption which for my computer would exceed limits and slow down the process\n",
    "    significantly where it took 5 times longer for directly updating the files. If you\n",
    "    have a lot of memory though it may actually be faster to update directly. The short\n",
    "    parameter is used for the adx indicator which creates two seperate dictionaries apart\n",
    "    from the indicator, that are used in the real-time indicator calculations for \n",
    "    precalculations but we only need about the last years worth of values for those, so\n",
    "    we pass in a date in short.\n",
    "    \"\"\"\n",
    "    # Calculate all of our indicators using our highlowclose dictionary, the period dict,\n",
    "    # and the preferred way to create our newbase files\n",
    "    returns_create(highlowclose, prd_dict, dmp_method)\n",
    "    per_create(highlowclose, prd_dict, dmp_method)\n",
    "    cci_create(highlowclose, prd_dict, dmp_method)\n",
    "    vol_create(highlowclose, prd_dict, dmp_method)\n",
    "    bol_create(highlowclose, prd_dict, dmp_method)\n",
    "    mom_create(highlowclose, prd_dict, dmp_method)\n",
    "    sma_create(highlowclose, prd_dict, dmp_method)\n",
    "    kdo_create(highlowclose, prd_dict, dmp_method)\n",
    "    mac_create(highlowclose, prd_dict, dmp_method)\n",
    "    rsi_create(highlowclose, prd_dict, dmp_method)\n",
    "    adx_create(highlowclose, prd_dict, short, dmp_method)\n",
    "    aro_create(highlowclose, prd_dict, dmp_method)\n",
    "    \n",
    "    # If dmp_method is 1, we use the intermediary step\n",
    "    if dmp_method == 1:\n",
    "        # If we are just updating rather than creating the indicators for the first time,\n",
    "        # then we pass the highlowclose dictionary which is the shortened version since\n",
    "        # we only need the shortened version if just updating.\n",
    "        if update == False:\n",
    "            combined_company_indicators()\n",
    "        else:\n",
    "            combined_company_indicators(highlowclose)\n",
    "    return\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_newbase_files(dic, nm, intra, first=False):\n",
    "    \"\"\"\n",
    "    Base files are where our combined data for each company. Currently theres 15 \n",
    "    dataframes each for each company. This is so that we only need to process about\n",
    "    4 months of data at a time, so we don't have to have so much data in memory at\n",
    "    once. We only process a max of 25000 rows at a time so it works out to roughly\n",
    "    4 months at a time processed.\n",
    "    \"\"\"\n",
    "    # Each name is a seperate company\n",
    "    for name in intra.keys():\n",
    "        new_df = pd.DataFrame()\n",
    "        \n",
    "        # If we not updating the mac indicator, we only create a single list of\n",
    "        # of column names since the mac indicator has two indicators associated\n",
    "        # with it.\n",
    "        if nm != 'mactwo':\n",
    "            nm_lst = []\n",
    "            for x in range(32):\n",
    "                nm_lst.append(name+'_'+nm+str(x))\n",
    "            # Rename the columns from numbers to names\n",
    "            dic[name].columns = nm_lst\n",
    "            new_df = dic[name]\n",
    "        else:\n",
    "            nm_lst  = []\n",
    "            nm_lst2 = []\n",
    "            for x in range(32):\n",
    "                nm_lst.append(name+'_mac'+str(x))\n",
    "                nm_lst2.append(name+'_'+nm+str(x))\n",
    "            # Rename the columns from numbers to names\n",
    "            dic[0][name].columns = nm_lst\n",
    "            dic[1][name].columns = nm_lst2\n",
    "            # Combine the two mac indicators together\n",
    "            new_df = pd.concat([dic[0][name], dic[1][name]], \n",
    "                        axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Seperate the indicator dataframe into 15 parts using specified dates for\n",
    "        # roughly equal parts.\n",
    "        ddf1  = new_df['2013-01-08':'2013-04-08']\n",
    "        ddf2  = new_df['2013-04-09':'2013-07-08']\n",
    "        ddf3  = new_df['2013-07-09':'2013-10-08']\n",
    "        ddf4  = new_df['2013-10-09':'2014-01-08']\n",
    "        ddf5  = new_df['2014-01-09':'2014-04-10']\n",
    "        ddf6  = new_df['2014-04-11':'2014-07-10']\n",
    "        ddf7  = new_df['2014-07-11':'2014-10-09']\n",
    "        ddf8  = new_df['2014-10-16':'2015-01-09']\n",
    "        ddf9  = new_df['2015-01-12':'2015-04-13']\n",
    "        ddf10 = new_df['2015-04-14':'2015-07-13']\n",
    "        ddf11 = new_df['2015-07-14':'2015-10-12']\n",
    "        ddf12 = new_df['2015-10-13':'2016-01-12']\n",
    "        ddf13 = new_df['2016-01-13':'2016-04-21']\n",
    "        ddf14 = new_df['2016-04-22':'2016-07-08']\n",
    "        ddf15 = new_df['2016-07-11':]\n",
    "        \n",
    "        # Put all of these parts into a list\n",
    "        ddf_lst = [ddf1,ddf2,ddf3,ddf4,ddf5,ddf6,ddf7,ddf8,ddf9,\n",
    "                   ddf10,ddf11,ddf12,ddf13,ddf14,ddf15]\n",
    "        \n",
    "        base = 'NewBase/'+name+'/'\n",
    "        \n",
    "        # If first is True, this is the first indicator and we create\n",
    "        # the indicator files.\n",
    "        if first == True:\n",
    "            # If the base directory doesn't already exist, create it.\n",
    "            if not os.path.exists(base):\n",
    "                os.makedirs(base)\n",
    "                \n",
    "            # For each indicator dataframe part, create our pickled\n",
    "            # indicator file in that companies base directory, naming\n",
    "            # the indicator files using this format: companyname_df0\n",
    "            # where the number is the indicator dataframe part(0-14)\n",
    "            for y in range(len(ddf_lst)):\n",
    "                opp = open(base+name+'_df'+str(y)+'.pickle', 'wb')\n",
    "                pickle.dump(ddf_lst[y], opp)\n",
    "                opp.close()\n",
    "        else:\n",
    "            # For each indicator part, open its corresponding indicator\n",
    "            # dataframe part\n",
    "            for y2 in range(len(ddf_lst)):\n",
    "                opp = open(base+name+'_df'+str(y2)+'.pickle', 'rb')\n",
    "                old = pickle.load(opp)\n",
    "                opp.close()\n",
    "                \n",
    "                # Then combine this indicator part with the indicator dataframe part\n",
    "                comb = pd.concat([old, ddf_lst[y2]], \n",
    "                        axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "                \n",
    "                # Dump the combined dataframe back to the file\n",
    "                opp = open(base+name+'_df'+str(y2)+'.pickle', 'wb')\n",
    "                pickle.dump(comb, opp)\n",
    "                opp.close()\n",
    "    return\n",
    "\n",
    "def update_adx23short(resampled_adj, adx_d2, adx_d3):\n",
    "    \"\"\"\n",
    "    Update the adx2 and adx3 dictionaries that are precalculated for use in the the real-time \n",
    "    calculations to speed up calculation during real-time. These are updated during the real-time\n",
    "    calculations but if you miss a day for some reason these functions update your dictionaries\n",
    "    for you. Also update our short highlowclose dictionary that is a shortened version of our\n",
    "    normal highlowclose dictionary for speed purposes since we don't need the full dictionary\n",
    "    during real-time calcs.\n",
    "    \"\"\"\n",
    "    # The ticks dictionary that contains the company stock symbols that differ between Yahoo\n",
    "    # and Google server symbol retrievals. The key is the yahoo symbol, while the value is\n",
    "    # the google symbol\n",
    "    ticks   = {'LMT':'NYSE:LMT', 'USO':'NYSEARCA:USO', 'GLD':'NYSEARCA:GLD',\n",
    "               'SPY':'NYSEARCA:SPY', '^DJI':'INDEXDJX:.DJI', \n",
    "               '^GSPC':'INDEXSP:.INX', '^IXIC':'INDEXNASDAQ:.IXIC'}\n",
    "    \n",
    "    # Open both the shortened highlowclose dictionary, as well as the adx dictionary that's \n",
    "    # used for precalculations in the real-time calculations.\n",
    "    opp   = open('NewBase/ADXD3/adx_d2.pickle','rb')\n",
    "    opp2  = open('Pickles/shortpickleintra.pickle','rb')\n",
    "    d2    = pickle.load(opp)\n",
    "    short = pickle.load(opp2)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "\n",
    "    new_short, new_d2, new_d3 = {}, {}, {}\n",
    "    for name in resampled_adj.keys():\n",
    "        if name in ticks.keys():\n",
    "            name2 = ticks[name]\n",
    "        else:\n",
    "            name2 = name\n",
    "            \n",
    "        # For each of the new highs/lows/closes/typicals since the last update, add each to\n",
    "        # the previous highlowclose dictionary dataframe. Then add the new adx dictionary\n",
    "        # values to the previous adx dictionary values.\n",
    "        new_short[name2] = short[name2].append(resampled_adj[name])\n",
    "        new_d2[name2]    = d2[name2].append(adx_d2[name])\n",
    "        \n",
    "        # Rename the 2nd adx dictionary precalculation dictonary to use the google names\n",
    "        for w in range(32):\n",
    "            new_d3[name2+str(w)] = adx_d3[name+str(w)]\n",
    "\n",
    "    # Dump everything to there files.\n",
    "    opp  = open('Pickles/shortpickleintra.pickle','wb')\n",
    "    opp2 = open('NewBase/ADXD3/adx_d2.pickle','wb')\n",
    "    opp3 = open('NewBase/ADXD3/adx_d3.pickle','wb')\n",
    "    pickle.dump(new_short, opp)\n",
    "    pickle.dump(new_d2, opp2)\n",
    "    pickle.dump(new_d3, opp3)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    opp3.close()\n",
    "    return\n",
    "\n",
    "def update_for_spldivs(dmp_method, update=False):\n",
    "    \"\"\"\n",
    "    Update a companies indicator values following a dividend or stock split using the\n",
    "    new adjusted high/low/close/typical values that were updated. Pass in a list of\n",
    "    company tickers that need to be updated.\n",
    "    \"\"\"\n",
    "    # If we are just updating one or more companies for a split/dividend, then we\n",
    "    # only will use the partial highlowclose dictionary containing only the companies\n",
    "    # needing the update, rather than updating every company.\n",
    "    if update == False:\n",
    "        opp = open('Pickles/pickleadjustedintracomplete.pickle','rb')\n",
    "    else:\n",
    "        opp = open('Pickles/onlyupdateintra.pickle','rb')\n",
    "    intra = pickle.load(opp)\n",
    "    opp.close()\n",
    "\n",
    "    # Use our stock dating function to get us the stock dates between about 1.5 years ago,\n",
    "    # to about 1 year in the future\n",
    "    rs            = NYSE_tradingdays()\n",
    "    short         = str(rs[0])[:10]\n",
    "    hlc_cols      = ['Highs','Lows','Closes','Typical']\n",
    "\n",
    "    # The prd_lst is in days, ie. 1day, 2day, etc..\n",
    "    prd_lst  = [1,  2,  3,  5,  8,  10, 12, 14, 16, 20, 25,  30,  40,  50,  80,  125]\n",
    "    # The 3 plnums lists are in stock minutes, ie. 10min, 14min, etc..\n",
    "    plnums   = [10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300]\n",
    "    plnums2  = [8,  10, 14, 16, 18, 20, 25, 30, 40, 50, 75,  100, 125, 150, 200, 250, 300, 350]\n",
    "    plnums3  = [6, 8, 10, 14, 16, 18, 20, 25, 30, 40, 50,  75,  100, 125, 150, 200, 250, 300, 350]\n",
    "    prds_dates = {}\n",
    "    for key in intra.keys():\n",
    "        # Make sure there are no duplicate index values and order the columns in hlc_cols order\n",
    "        intra[key] = intra[key][hlc_cols].reset_index().drop_duplicates(\n",
    "                                subset='index',keep='last').set_index('index')\n",
    "        tl = []\n",
    "        # Go through present, to about 0.5 years ago in the dates dictionary, making sure for\n",
    "        # each company, that that date exists since some dates are missing for some companies.\n",
    "        # If the date is missing we skip it so as to ensure that when we measure number of \n",
    "        # stock minutes are in, for example, 50 days, its accurate.\n",
    "        for x in xrange(257, 110, -1):\n",
    "            try:\n",
    "                dt   = str(rs[x])[:10]\n",
    "                test = intra[key][dt]\n",
    "                tl.append(dt)\n",
    "            except:\n",
    "                pass\n",
    "        prd_lst  = [tl[1], tl[2], tl[3], tl[5], tl[8], tl[10], tl[12], tl[14], tl[16], tl[20], \n",
    "                    tl[25], tl[30], tl[40], tl[50], tl[80], tl[125]]\n",
    "        prds_dates[key] = prd_lst\n",
    "    # Pass all of this to our period creation function which calculates all of our period lengths\n",
    "    # for each company.\n",
    "    prd_dict = create_prd_lst2(intra, prds_dates, plnums, plnums2, plnums3)\n",
    "    # Then update all of our indicators\n",
    "    update_indicators(intra, prd_dict, short, dmp_method, update)\n",
    "    return\n",
    "\n",
    "def dump_indicator_data(dic, name):\n",
    "    \"\"\"\n",
    "    This function takes a indicator dictionary, and is identified\n",
    "    with the name variable that contains the indicator abreviation\n",
    "    like 'rsi' for relative strength index. We then create a directory\n",
    "    for each company in the main Companies/ directory. Each companies\n",
    "    directory will contain each indicator as a file for each stored as\n",
    "    pickle files.\n",
    "    \n",
    "    We then create the indicator names for the column names of the \n",
    "    dataframes, using a combination of the company name, the indicator\n",
    "    and the period length value. We then dump the indicator data as a \n",
    "    pickle file into that companies directory. We start every indicator\n",
    "    file after 2013-01-08 as that is when we have data from every company\n",
    "    starting.\n",
    "    \n",
    "    For one indicator, Moving Average Convergence Divergence, we have \n",
    "    2 indicators that are passed from that indicator, and we identify\n",
    "    it as 'mactwo' and for that indicator, we have to name two dataframes\n",
    "    and dump 2 files.\n",
    "    \n",
    "    All of these files will be used to combine indicators to create our\n",
    "    company indicator dataframes, that will be used for predictions.\n",
    "    \"\"\"\n",
    "    # If we not updating the mac indicator, we only create a single list of\n",
    "    # of column names since the mac indicator has two indicators associated\n",
    "    # with it.\n",
    "    if name != 'mactwo':\n",
    "        for key, val in dic.iteritems():\n",
    "            # Make sure our companies directory exists, creating it if not\n",
    "            base = 'Companies/'+key+'/'\n",
    "            if not os.path.exists(base):\n",
    "                os.makedirs(base)\n",
    "\n",
    "            names = []\n",
    "            # Rename our columns from numbers to names\n",
    "            for x in range(32):\n",
    "                names.append(key+'_'+name+str(x))\n",
    "            val.columns = names\n",
    "            \n",
    "            # Dump our indicator to a file\n",
    "            opp = open(base+key+'_'+name+'.pickle', 'wb')\n",
    "            pickle.dump(val['2013-01-08':], opp)\n",
    "            opp.close()\n",
    "    else:\n",
    "        for key in dic[0].keys():\n",
    "            # Make sure our companies directory exists, creating it if not\n",
    "            base = 'Companies/'+key+'/'\n",
    "            if not os.path.exists(base):\n",
    "                os.makedirs(base)\n",
    "                \n",
    "            val = dic[0][key]\n",
    "            val2 = dic[1][key]\n",
    "            \n",
    "            names, names2 = [], []\n",
    "            # Rename our columns from numbers to names\n",
    "            for x in range(32):\n",
    "                names.append(key+'_mac'+str(x))\n",
    "                names2.append(key+'_'+name+str(x))\n",
    "            val.columns  = names\n",
    "            val2.columns = names2\n",
    "            \n",
    "            # Dump our indicators to two files\n",
    "            opp  = open(base+key+'_mac.pickle', 'wb')\n",
    "            opp2 = open(base+key+'_'+name+'.pickle', 'wb')\n",
    "            pickle.dump(val['2013-01-08':], opp)\n",
    "            pickle.dump(val2['2013-01-08':], opp2)\n",
    "            opp.close()\n",
    "            opp2.close()\n",
    "    return\n",
    "\n",
    "def combined_company_indicators(highlowclose=None):\n",
    "    \"\"\"\n",
    "    This function takes the company indicator directories, and combines\n",
    "    them to create our combined company dataframes that each have 416 \n",
    "    columns together. After combining them, we divide this dataframe into\n",
    "    15 similar pieces so that when training data, we only have to use about\n",
    "    25000 records at a time, and don't have to worry as much about memory\n",
    "    issues. \n",
    "    \n",
    "    These new pieces are all dumped in company directories in the NewBase\n",
    "    directory that is created. These pieces will be used both for real\n",
    "    time indicator calculations as well as training and testing for our\n",
    "    neural network.\n",
    "    \"\"\"\n",
    "    # The stocks used for my project\n",
    "    ticks   = ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "               'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "               'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "               'SIRI','WIN','YHOO','BHP','BP',\n",
    "               'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "               'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "               'MGM','AMD','GLW','HPQ','S','T',\n",
    "               '^GSPC','^IXIC','^DJI','GLD','USO','SPY']\n",
    "    # The indicator abreviations\n",
    "    lst = ['rsi', 'vol', 'sma', 'cci', 'per', 'mom', 'bol', \n",
    "           'aro', 'mac', 'mactwo', 'adx', 'kdo', 'rets']\n",
    "    \n",
    "    # If the shortened version of the highlowclose dictionary was not passed, then\n",
    "    # we use all of the companies in ticks, rather than just a certain number of them.\n",
    "    try:\n",
    "        if highlowclose == None:\n",
    "            ticks = ticks\n",
    "        else:\n",
    "            ticks = highlowclose.keys()\n",
    "    except:\n",
    "        ticks = highlowclose.keys()\n",
    "    \n",
    "    for key in ticks:\n",
    "        ddf_lst = []\n",
    "        new_df  = pd.DataFrame()\n",
    "        \n",
    "        folder = 'Companies/'+key+'/'\n",
    "        # For every file in that companies indicator directory, we open it, and combine, doing\n",
    "        # this for every file(indicator) in the directory\n",
    "        for indic in lst:\n",
    "            opp = open(folder+key+'_'+indic+'.pickle', 'rb')\n",
    "            df  = pickle.load(opp)\n",
    "            opp.close()\n",
    "                \n",
    "            new_df = pd.concat([new_df, df], axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Seperate the indicator dataframe into 15 seperate, roughly equal parts\n",
    "        ddf1  = new_df['2013-01-08':'2013-04-08']\n",
    "        ddf2  = new_df['2013-04-09':'2013-07-08']\n",
    "        ddf3  = new_df['2013-07-09':'2013-10-08']\n",
    "        ddf4  = new_df['2013-10-09':'2014-01-08']\n",
    "        ddf5  = new_df['2014-01-09':'2014-04-10']\n",
    "        ddf6  = new_df['2014-04-11':'2014-07-10']\n",
    "        ddf7  = new_df['2014-07-11':'2014-10-09']\n",
    "        ddf8  = new_df['2014-10-16':'2015-01-09']\n",
    "        ddf9  = new_df['2015-01-12':'2015-04-13']\n",
    "        ddf10 = new_df['2015-04-14':'2015-07-13']\n",
    "        ddf11 = new_df['2015-07-14':'2015-10-12']\n",
    "        ddf12 = new_df['2015-10-13':'2016-01-12']\n",
    "        ddf13 = new_df['2016-01-13':'2016-04-21']\n",
    "        ddf14 = new_df['2016-04-22':'2016-07-08']\n",
    "        ddf15 = new_df['2016-07-11':]\n",
    "        # Put them in a list\n",
    "        ddf_lst = [ddf1,ddf2,ddf3,ddf4,ddf5,ddf6,ddf7,ddf8,ddf9,\n",
    "                   ddf10,ddf11,ddf12,ddf13,ddf14,ddf15]\n",
    "        \n",
    "        # Create the indicator dataframe directory if not already created\n",
    "        base = 'NewBase/'+key+'/'\n",
    "        if not os.path.exists(base):\n",
    "            os.makedirs(base)\n",
    "            \n",
    "        # Dump each part as its own file\n",
    "        for x in range(len(ddf_lst)):\n",
    "            opp = open(base+key+'_df'+str(x)+'.pickle', 'wb')\n",
    "            pickle.dump(ddf_lst[x], opp)\n",
    "            opp.close()\n",
    "    return\n",
    "            \n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dump method is whether you choose to temporarily store each companies data in a folder\n",
    "called Companies/ where each subfolder is a company, and within the company folders you\n",
    "temporarily store each indicator, before finally combining all the companies indicators\n",
    "and storing 14 different combined dataframes with roughly 4 months of combined data in each.\n",
    "\n",
    "If you choose the other option, you don't use the temporary storage, and rather go straight to\n",
    "dumping and adding to the dump files in the combined data. \n",
    "\n",
    "The former method is better if you have a lot of available storage space. Each company takes \n",
    "up about 3GB of data so if you're working w/ 50 companies, you need 150GB on top of the 150GB \n",
    "for combining them. You can then delete the companies folder after, unless you want to keep \n",
    "them to be able to look at each individual companies individual indicators. The latter method \n",
    "is better if you don't have a lot of storage space. This method takes WAYYY longer though!!\n",
    "(ex. on 50 companies, it took 3hrs for the first method, and 24 hrs for the second!!)\n",
    "\n",
    "dmp_method = 0 is the latter method(ie. less storage, much more computing time)\n",
    "dmp_method = 1 is the former method(ie. more storage, much less computing time)\n",
    "\"\"\"\n",
    "dmp_method = 1\n",
    "update_for_spldivs(dmp_method)\n",
    "#If just updating, rather than initialing creating the indicators, uncomment below\n",
    "# and then comment the call above.\n",
    "#update_for_spldivs(dmp_method, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
