{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "from nyse_dates_prds import *\n",
    "from sys import stdout \n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def returns_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This calculates the return values which is the closing price divided by the previous\n",
    "    period's closing price minus 1. This gets us a percentage increase or decrease over\n",
    "    that time period.\n",
    "    \"\"\"\n",
    "    rets_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        # Return value periods have the normal periods plus a 1,2,3,4 and 5 minute period\n",
    "        prds   = [1,2,3,4,5] + periods[name][0]\n",
    "        \n",
    "        # There's 32 periods to calculate returns for, we put them all in a dataframe\n",
    "        for x in range(32):\n",
    "            df[x] = (((close / close.shift(prds[x])) - 1.).fillna(0)).replace([np.inf], 0)\n",
    "        \n",
    "        rets_dict[name] = df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(rets_dict, 'rets', highlowclose, first=True)\n",
    "    else:\n",
    "        dump_indicator_data(rets_dict, 'rets')\n",
    "    print \"RETS DONE\"\n",
    "    return\n",
    "\n",
    "def per_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    Calculates the price to earnings ratio which is the closing price divided by the\n",
    "    difference between the closing price and the previous period's closing price.\n",
    "    \"\"\"\n",
    "    per_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        cl     = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32): \n",
    "            clshf = (cl - cl.shift(prds[x])).replace(0,np.NaN)\n",
    "            df[x] = (cl / clshf).fillna(0)\n",
    "        \n",
    "        per_dict[name] = df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(per_dict, 'per', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(per_dict, 'per')\n",
    "    print \"PER DONE\"\n",
    "    return\n",
    "\n",
    "def kdo_create(highlowclose, periods, dmp_method):  \n",
    "    \"\"\"\n",
    "    This is the stochastic oscillator calculation, which takes each periods max and min\n",
    "    value. Then we first subtract the closing price minus the minimum, then we subtract\n",
    "    the max from the min. We then divide our first calculation by the second and multiply\n",
    "    by 100 which gets us the K oscillator. Then we take the rolling mean value of the small\n",
    "    period of the k values to get our D oscillator.\n",
    "    \"\"\"\n",
    "    d_dict, count = {}, 0\n",
    "    for name in highlowclose.keys(): \n",
    "        d_df       = pd.DataFrame()\n",
    "        high, low  = highlowclose[name]['Highs'], highlowclose[name]['Lows']\n",
    "        close      = highlowclose[name]['Closes']\n",
    "        prds       = periods[name][1]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num  = prds[x]\n",
    "            num2 = prds[x+1]\n",
    "            prev_max  = high.rolling(window = num2, center = False).max()\n",
    "            prev_min  = low.rolling(window  = num2, center = False).min()\n",
    "        \n",
    "            cl  =  close    - prev_min\n",
    "            hl  = (prev_max - prev_min).replace(0., np.NaN)\n",
    "            \n",
    "            k_df    = ((cl / hl) * 100.).fillna(0.)\n",
    "            d_df[x] = (k_df.rolling(window=num, center=False).mean()).fillna(0.)\n",
    "            \n",
    "        d_dict[name] = d_df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(d_dict, 'kdo', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(d_dict, 'kdo')\n",
    "    print \"KD DONE\"\n",
    "    return\n",
    "\n",
    "def cci_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is our commodity channel index calculation\n",
    "    \"\"\"\n",
    "    cci_dict, count, constant = {}, 0, 0.015\n",
    "    for name in highlowclose.keys():\n",
    "        df   = pd.DataFrame()\n",
    "        # Typical values are that point's day high + low + close / 3\n",
    "        typ  = highlowclose[name]['Typical']\n",
    "        prds = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num = prds[x]\n",
    "            # Get the standard deviation and the mean of each periods values    \n",
    "            typ_std  = typ.rolling(window = num, center = False).std()\n",
    "            typ_mean = typ.rolling(window = num, center = False).mean()\n",
    "            # Subtract each pts typical value minus the mean typical\n",
    "            ttmean   = typ - typ_mean\n",
    "            # Then multiply the standard deviation value times the constant value\n",
    "            #  which is a value that is well known for this indicator but can be\n",
    "            #  changed if you believe you can get better calculations with differnt one\n",
    "            ctmad    = (constant * typ_std).replace(0.,np.NaN)\n",
    "            # Finally, divide the first calculation by the second to get that periods cci\n",
    "            df[x]    = (ttmean / ctmad).fillna(0.)\n",
    "        \n",
    "        cci_dict[name] = df      \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(cci_dict, 'cci', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(cci_dict, 'cci')\n",
    "    print \"CCI DONE\"\n",
    "    return\n",
    "\n",
    "def vol_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the volatility indicator calculation. We get this by taking the period return \n",
    "    values for that company, and then taking the standard deviation of each periods returns,\n",
    "    and multiplying that by the square root of the period length\n",
    "    \"\"\"\n",
    "    vol_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        vol_df = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][2]\n",
    "        \n",
    "        for x in range(32):    \n",
    "            num       = prds[x]\n",
    "            num2      = prds[x+3]\n",
    "            rets      = (close / close.shift(num) - 1.).fillna(0.)\n",
    "            vol_df[x] = rets.rolling(window=num2, center=False).std() * np.sqrt(num2)\n",
    "        \n",
    "        vol_dict[name] = vol_df        \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(vol_dict, 'vol', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(vol_dict, 'vol')\n",
    "    print \"VOL DONE\"\n",
    "    return\n",
    "\n",
    "def bol_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the bollinger band calculation. We get this by finding the periods mean and\n",
    "    standard deviation values, adding those together and multiplying by two to get the\n",
    "    upper band values. We then calculate the difference between closing prices and mean\n",
    "    prices, then subtract the upper band values by the mean prices, and then divide these\n",
    "    two answers to get our final bollinger band values.\n",
    "    \"\"\"\n",
    "    bol_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df    = pd.DataFrame()\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            rm    = (close.rolling(window=num, center=False).mean()).fillna(0)\n",
    "            rstd  = (close.rolling(window=num, center=False).std()).fillna(0)\n",
    "            upper = rm + rstd * 2. \n",
    "            clrm  = close - rm\n",
    "            uprm  = (upper - rm).replace(0.,np.NaN)\n",
    "            df[x] = (clrm / uprm).fillna(0.)\n",
    "            \n",
    "        bol_dict[name] = df        \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(bol_dict, 'bol', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(bol_dict, 'bol')\n",
    "    print \"BOL DONE\"\n",
    "    return\n",
    "\n",
    "def mom_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the momentum calculator, which is simply the the difference between current\n",
    "    closing price, and closing price x periods ago, multiplied by 100\n",
    "    \"\"\"\n",
    "    mom_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            df[x] = (((close - close.shift(num)) / close.shift(num)) * 100.).fillna(0.)\n",
    "        \n",
    "        mom_dict[name] = df       \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(mom_dict, 'mom', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(mom_dict, 'mom')\n",
    "    print \"MOM DONE\"\n",
    "    return\n",
    "\n",
    "def sma_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the simple moving average calculation. This is simply the mean closing price\n",
    "    during that period.\n",
    "    \"\"\"\n",
    "    sma_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df    = pd.DataFrame()\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            df[x] = (close.rolling(window=num, center=False).mean()).fillna(0.)\n",
    "            \n",
    "        sma_dict[name] = df       \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(sma_dict, 'sma', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(sma_dict, 'sma')\n",
    "    print \"SMA DONE\"\n",
    "    return\n",
    "\n",
    "def aro_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the aroon indicator calculation. For each periods values, you how many periods it has\n",
    "    been since you've had the max during that period as well as the min, and subtract from eachother\n",
    "    \"\"\"\n",
    "    aro_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num   = prds[x]\n",
    "            # Using an applied method for each period to cut down on computation time\n",
    "            df[x] = close.rolling(window=num, center=False).apply(aro_apply, args=(num,))\n",
    "            \n",
    "        aro_dict[name] = df       \n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(aro_dict, 'aro', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(aro_dict, 'aro')\n",
    "    print \"ARO DONE\"\n",
    "    return\n",
    "def aro_apply(df, prd):\n",
    "    up      = ((prd - df.argmax()) / float(prd)) * 100.\n",
    "    down    = ((prd - df.argmin()) / float(prd)) * 100.\n",
    "    return up - down\n",
    "\n",
    "def mac_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the moving average convergence divergence calculation. To get this we take the \n",
    "    expected moving average of the smaller period, and the larger period. Subtract from eachother,\n",
    "    then take the expected moving average of the smallest period on those calculated values to get\n",
    "    an buy/sell indicator of the macd calculation and use both of these.\n",
    "    \"\"\"\n",
    "    mac_dict, mac_dict2, count = {}, {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        macd_df, macd_df2  = pd.DataFrame(), pd.DataFrame()\n",
    "        close              = highlowclose[name]['Closes']\n",
    "        prds               = periods[name][2]\n",
    "        \n",
    "        for x in range(32):\n",
    "            num, num2, num3 = prds[x], prds[x+1], prds[x+3]\n",
    "            \n",
    "            mid    = close.ewm(ignore_na=False, span=num2, min_periods=0, adjust=True).mean()\n",
    "            large  = close.ewm(ignore_na=False, span=num3, min_periods=0, adjust=True).mean()\n",
    "            macd   = large - mid\n",
    "            small  = macd.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "            result = (macd / small) - 1.\n",
    "\n",
    "            macd_df[x]  = macd\n",
    "            macd_df2[x] = result\n",
    "        \n",
    "        mac_dict[name]  = macd_df  \n",
    "        mac_dict2[name] = macd_df2\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files([mac_dict, mac_dict2], 'mactwo', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data([mac_dict, mac_dict2], 'mactwo')\n",
    "    print \"MAC DONE\"\n",
    "    return\n",
    "    \n",
    "def adx_create(highlowclose, periods, short, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the average directional index calculation.\n",
    "    \"\"\"\n",
    "    tickers2      = {'USO':'NYSEARCA:USO','GLD':'NYSEARCA:GLD',\n",
    "                     'SPY':'NYSEARCA:SPY','^DJI':'INDEXDJX:.DJI',\n",
    "                     '^GSPC':'INDEXSP:.INX','^IXIC':'INDEXNASDAQ:.IXIC', \n",
    "                     'LMT':'NYSE:LMT'}\n",
    "    \n",
    "    tr_df_dict, adx_dict, adx_dict2, count = {}, {}, {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        if name in tickers2.keys():\n",
    "            name2 = tickers2[name]\n",
    "        else:\n",
    "            name2 = name \n",
    "        adx_df, dx_df = pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        high   = highlowclose[name]['Highs']\n",
    "        low    = highlowclose[name]['Lows']\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            trpm_df = pd.DataFrame()\n",
    "            num     = prds[x]\n",
    "            \n",
    "            # Get the true range values by subtracting highs and lows, highs and period closes,\n",
    "            #  and the lows and period closes, and take the max at each period\n",
    "            tr_df      = pd.DataFrame()\n",
    "            tr_df[0]   = (high - low)\n",
    "            tr_df[1]   = (high - close.shift(num)).fillna(0.)\n",
    "            tr_df[2]   = (low  - close.shift(num)).fillna(0.)\n",
    "            true_range = tr_df.max(axis=1)\n",
    "            \n",
    "            # Get the positive and negative directional movement values\n",
    "            plus_dm   = (high - high.shift(num)).fillna(0.)\n",
    "            minus_dm  = (low.shift(num) - low).fillna(0.)\n",
    "            plus_dm   = pd.Series(np.where(plus_dm >= minus_dm, plus_dm, 0.),index=plus_dm.index)\n",
    "            minus_dm  = pd.Series(np.where(minus_dm >= plus_dm, minus_dm, 0.),index=plus_dm.index)\n",
    "            plus_dm[plus_dm < 0]   = 0.\n",
    "            minus_dm[minus_dm < 0] = 0.\n",
    "        \n",
    "            trpm_df[0] = true_range\n",
    "            trpm_df[1] = plus_dm\n",
    "            trpm_df[2] = minus_dm\n",
    "            \n",
    "            tr_name = name2+str(x)\n",
    "            # Store these values for use in the real-time calculation\n",
    "            tr_df_dict[tr_name] = trpm_df[len(true_range)-num-1:]\n",
    "            \n",
    "            # Get the EMaverage true value, average pos and neg directional movements\n",
    "            atr    = true_range.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "            pos_dm = plus_dm.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "            neg_dm = minus_dm.ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean()\n",
    "\n",
    "            # Get the positive and negative directional indexes, and use these to get\n",
    "            #  the directional index values.\n",
    "            pos_di  = ((pos_dm / atr) * 100.).replace([np.inf,-np.inf],0.)\n",
    "            neg_di  = ((neg_dm / atr) * 100.).replace([np.inf,-np.inf],0.)\n",
    "            dx_df[x] = (abs(pos_di - neg_di) / (pos_di + neg_di)).replace([np.inf,-np.inf],0.)\n",
    "            \n",
    "            # Then take the EMaverage of the directional index to get the average directional index\n",
    "            adx_df[x] = dx_df[x].ewm(ignore_na=False, span=num, min_periods=0, adjust=True).mean() * 100.\n",
    "        \n",
    "        adx_dict[name] = adx_df\n",
    "        adx_dict2[name2] = dx_df[short:]\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    base = 'NewBase/ADXD/'\n",
    "    if not os.path.exists(base):\n",
    "        os.makedirs(base)\n",
    "    opp  = open('NewBase/ADXD/adx_d2.pickle','wb')\n",
    "    opp2 = open('NewBase/ADXD/adx_d3.pickle','wb')\n",
    "    pickle.dump(adx_dict2, opp)\n",
    "    pickle.dump(tr_df_dict, opp2)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(adx_dict, 'adx', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(adx_dict, 'adx')\n",
    "    print \"ADX DONE\"\n",
    "    return\n",
    "\n",
    "def rsi_create(highlowclose, periods, dmp_method):\n",
    "    \"\"\"\n",
    "    This is the relative strength index calculation.\n",
    "    \"\"\"\n",
    "    rsi_dict, count = {}, 0\n",
    "    for name in highlowclose.keys():\n",
    "        rsi_df = pd.DataFrame()\n",
    "        \n",
    "        close   = highlowclose[name]['Closes']\n",
    "        prds    = periods[name][2]\n",
    "\n",
    "        for x in range(32):\n",
    "            num  = prds[x]\n",
    "            num2 = prds[x+3]\n",
    "            deltas  = (close - close.shift(num)).fillna(0.)\n",
    "            \n",
    "            # Take the positive and negative values of the deltas and set everything\n",
    "            #  else to zero\n",
    "            up, down = deltas.copy(), deltas.copy()\n",
    "            up[up < 0]     = 0\n",
    "            down[down > 0] = 0\n",
    "\n",
    "            # Then get the mean of the positive and negative value deltas\n",
    "            rolup_df   =  up.rolling(window=num2, center=False).mean()\n",
    "            roldown_df = (down.rolling(window=num2, center=False).mean()).abs()\n",
    "\n",
    "            # Then divide these positive and negative values to get the relative strength\n",
    "            #  Then do this calculation to get the relative strength index\n",
    "            rol_updown = (rolup_df / roldown_df).replace([np.inf,-np.inf],0.)\n",
    "            rsi_df[x] = (100. - (100. / (1. + rol_updown))).replace([np.inf,-np.inf],0.)\n",
    "\n",
    "        rsi_dict[name]  = rsi_df\n",
    "        stdout.write(\"\\r%d\" % count)\n",
    "        stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    if dmp_method == 0:\n",
    "        create_newbase_files(rsi_dict, 'rsi', highlowclose)\n",
    "    else:\n",
    "        dump_indicator_data(rsi_dict, 'rsi')\n",
    "    print \"RSI DONE\"\n",
    "    return\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def update_indicators(highlowclose, prd_dict, short, dmp_method, update=False):\n",
    "    \"\"\"\n",
    "    Update all of our indicator values, starting from our start2 value which is the last\n",
    "    date in our indicator dictionaries. We get the added values, put them all in a list,\n",
    "    where we'll send them to our update_newbase_files() function to append them to our\n",
    "    old dictionaries. These functions are only called if you missed the real-time function\n",
    "    for that day for any reason.\n",
    "    \"\"\"\n",
    "    returns_create(highlowclose, prd_dict, dmp_method)\n",
    "    per_create(highlowclose, prd_dict, dmp_method)\n",
    "    cci_create(highlowclose, prd_dict, dmp_method)\n",
    "    vol_create(highlowclose, prd_dict, dmp_method)\n",
    "    bol_create(highlowclose, prd_dict, dmp_method)\n",
    "    mom_create(highlowclose, prd_dict, dmp_method)\n",
    "    sma_create(highlowclose, prd_dict, dmp_method)\n",
    "    kdo_create(highlowclose, prd_dict, dmp_method)\n",
    "    mac_create(highlowclose, prd_dict, dmp_method)\n",
    "    rsi_create(highlowclose, prd_dict, dmp_method)\n",
    "    adx_create(highlowclose, prd_dict, short, dmp_method)\n",
    "    aro_create(highlowclose, prd_dict, dmp_method)\n",
    "    \n",
    "    if dmp_method == 1:\n",
    "        if update == False:\n",
    "            combined_company_indicators()\n",
    "        else:\n",
    "            combined_company_indicators(highlowclose)\n",
    "    return\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def create_newbase_files(dic, nm, intra, first=False):\n",
    "    \"\"\"\n",
    "    Base files are where our combined data for each company. Currently theres 15 \n",
    "    dataframes each for each company. This is so that we only need to process about\n",
    "    4 months of data at a time, so we don't have to have so much data in memory at\n",
    "    once. We only process a max of 25000 rows at a time so it works out to roughly\n",
    "    4 months at a time processed.\n",
    "    \"\"\"\n",
    "    for name in intra.keys():\n",
    "        new_df = pd.DataFrame()\n",
    "        \n",
    "        if nm != 'mactwo':\n",
    "            nm_lst = []\n",
    "            for x in range(32):\n",
    "                nm_lst.append(name+'_'+nm+str(x))\n",
    "            dic[name].columns = nm_lst\n",
    "            new_df = dic[name]\n",
    "        else:\n",
    "            nm_lst  = []\n",
    "            nm_lst2 = []\n",
    "            for x in range(32):\n",
    "                nm_lst.append(name+'_mac'+str(x))\n",
    "                nm_lst2.append(name+'_'+nm+str(x))\n",
    "            dic[0][name].columns = nm_lst\n",
    "            dic[1][name].columns = nm_lst2\n",
    "            new_df = pd.concat([dic[0][name], dic[1][name]], axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        ddf1  = new_df['2013-01-08':'2013-04-08']\n",
    "        ddf2  = new_df['2013-04-09':'2013-07-08']\n",
    "        ddf3  = new_df['2013-07-09':'2013-10-08']\n",
    "        ddf4  = new_df['2013-10-09':'2014-01-08']\n",
    "        ddf5  = new_df['2014-01-09':'2014-04-10']\n",
    "        ddf6  = new_df['2014-04-11':'2014-07-10']\n",
    "        ddf7  = new_df['2014-07-11':'2014-10-09']\n",
    "        ddf8  = new_df['2014-10-16':'2015-01-09']\n",
    "        ddf9  = new_df['2015-01-12':'2015-04-13']\n",
    "        ddf10 = new_df['2015-04-14':'2015-07-13']\n",
    "        ddf11 = new_df['2015-07-14':'2015-10-12']\n",
    "        ddf12 = new_df['2015-10-13':'2016-01-12']\n",
    "        ddf13 = new_df['2016-01-13':'2016-04-21']\n",
    "        ddf14 = new_df['2016-04-22':'2016-07-22']\n",
    "        ddf15 = new_df['2016-07-25':]\n",
    "        ddf_lst = [ddf1,ddf2,ddf3,ddf4,ddf5,ddf6,ddf7,ddf8,ddf9,\n",
    "                   ddf10,ddf11,ddf12,ddf13,ddf14,ddf15]\n",
    "        \n",
    "        base = 'NewBase/'+name+'/'\n",
    "        if first == True:\n",
    "            if not os.path.exists(base):\n",
    "                os.makedirs(base)\n",
    "            for y in range(len(ddf_lst)):\n",
    "                opp = open(base+name+'_df'+str(y)+'.pickle', 'wb')\n",
    "                pickle.dump(ddf_lst[y], opp)\n",
    "                opp.close()\n",
    "        else:\n",
    "            for y2 in range(len(ddf_lst)):\n",
    "                opp = open(base+name+'_df'+str(y2)+'.pickle', 'rb')\n",
    "                old = pickle.load(opp)\n",
    "                opp.close()\n",
    "                \n",
    "                comb = pd.concat([old, ddf_lst[y2]], axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "                \n",
    "                opp = open(base+name+'_df'+str(y2)+'.pickle', 'wb')\n",
    "                pickle.dump(comb, opp)\n",
    "                opp.close()\n",
    "    return\n",
    "\n",
    "def update_adx23short(resampled_adj, adx_d2, adx_d3):\n",
    "    \"\"\"\n",
    "    Update the adx2 and adx3 dictionaries that are precalculated for use in the the real-time \n",
    "    calculations to speed up calculation during real-time. These are updated during the real-time\n",
    "    calculations but if you miss a day for some reason these functions update your dictionaries\n",
    "    for you. Also update our short highlowclose dictionary that is a shortened version of our\n",
    "    normal highlowclose dictionary for speed purposes since we don't need the full dictionary\n",
    "    during real-time calcs.\n",
    "    \"\"\"\n",
    "    ticks   = ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "               'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "               'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "               'SIRI','WIN','YHOO','BHP','BP',\n",
    "               'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "               'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "               'MGM','AMD','GLW','HPQ','S','T',\n",
    "               '^GSPC','^IXIC','^DJI','GLD','USO','SPY']\n",
    "    ticks2  = ['BPOP','FITB','HBAN','CMCSA','EBAY','AAPL','AMAT','BRCD','CSCO',\n",
    "               'GOOG','INTC','LVLT','MSFT','MU','NVDA','ORCL','QCOM','SIRI','WIN',\n",
    "               'YHOO','BHP','BP','RIO','XOM','GE','F','MO','XRX','GS','JPM','LYG',\n",
    "               'MS','RF','USB','WFC','MRK','PFE','NYSE:LMT','MGM','AMD','GLW',\n",
    "               'HPQ','S','T','NYSEARCA:USO','NYSEARCA:GLD','NYSEARCA:SPY',\n",
    "               'INDEXDJX:.DJI','INDEXSP:.INX','INDEXNASDAQ:.IXIC']\n",
    "    \n",
    "    opp   = open('NewBase/ADXD3/adx_d2.pickle','rb')\n",
    "    opp2  = open('Pickles/shortpickleintra.pickle','rb')\n",
    "    d2    = pickle.load(opp)\n",
    "    short = pickle.load(opp2)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "\n",
    "    new_short, new_d2, new_d3 = {}, {}, {}\n",
    "    for name,name2 in zip(ticks, tickers2):\n",
    "        new_short[name2] = short[name2][:'2016-08-26'].append(resampled_adj[name])\n",
    "        new_d2[name2]    = d2[name2][:'2016-08-26'].append(adx_d2[name])\n",
    "        for w in range(32):\n",
    "            new_d3[name2+str(w)] = adx_d3[name+str(w)]\n",
    "\n",
    "    opp  = open('Pickles/shortpickleintra.pickle','wb')\n",
    "    opp2 = open('NewBase/ADXD3/adx_d2.pickle','wb')\n",
    "    opp3 = open('NewBase/ADXD3/adx_d3.pickle','wb')\n",
    "    pickle.dump(new_short, opp)\n",
    "    pickle.dump(new_d2, opp2)\n",
    "    pickle.dump(new_d3, opp3)\n",
    "    opp.close()\n",
    "    opp2.close()\n",
    "    opp3.close()\n",
    "    return\n",
    "\n",
    "def update_for_spldivs(dmp_method, update=False):\n",
    "    \"\"\"\n",
    "    Update a companies indicator values following a dividend or stock split using the\n",
    "    new adjusted high/low/close/typical values that were updated. Pass in a list of\n",
    "    company tickers that need to be updated.\n",
    "    \"\"\"\n",
    "    if update == False:\n",
    "        opp = open('Pickles/pickleadjustedintracomplete.pickle','rb')\n",
    "    else:\n",
    "        opp = open('Pickles/onlyupdateintra.pickle','wb')\n",
    "    intra = pickle.load(opp)\n",
    "    opp.close()\n",
    "\n",
    "    rs            = NYSE_tradingdays()\n",
    "    short         = str(rs[0])[:10]\n",
    "    hlc_cols      = ['Highs','Lows','Closes','Typical']\n",
    "\n",
    "    prd_lst  = [1,  2,  3,  5,  8,  10, 12, 14, 16, 20, 25,  30,  40,  50,  80,  125]\n",
    "    plnums   = [10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300]\n",
    "    plnums2  = [8,  10, 14, 16, 18, 20, 25, 30, 40, 50, 75,  100, 125, 150, 200, 250, 300, 350]\n",
    "    plnums3  = [6,  8,  10, 14, 16, 18, 20, 25, 30, 40, 50,  75,  100, 125, 150, 200, 250, 300, 350]\n",
    "    prds_dates = {}\n",
    "    for key in intra.keys():\n",
    "        intra[key] = intra[key][hlc_cols].reset_index().drop_duplicates(\n",
    "                                subset='index',keep='last').set_index('index')\n",
    "        tl = []\n",
    "        for x in xrange(257, 110, -1):\n",
    "            try:\n",
    "                dt   = str(rs[x])[:10]\n",
    "                test = intra[key][dt]\n",
    "                tl.append(dt)\n",
    "            except:\n",
    "                pass\n",
    "        prd_lst  = [tl[1], tl[2], tl[3], tl[5], tl[8], tl[10], tl[12], tl[14], tl[16], tl[20], \n",
    "                    tl[25], tl[30], tl[40], tl[50], tl[80], tl[125]]\n",
    "        prds_dates[key] = prd_lst\n",
    "    prd_dict = create_prd_lst2(intra, prds_dates, plnums, plnums2, plnums3)\n",
    "    update_indicators(intra, prd_dict, short, dmp_method, update)\n",
    "    return\n",
    "\n",
    "def dump_indicator_data(dic, name):\n",
    "    if name != 'mactwo':\n",
    "        for key, val in dic.iteritems():\n",
    "            base = 'Companies/'+key+'/'\n",
    "            if not os.path.exists(base):\n",
    "                os.makedirs(base)\n",
    "\n",
    "            names = []\n",
    "            for x in range(32):\n",
    "                names.append(key+'_'+name+str(x))\n",
    "            val.columns = names\n",
    "\n",
    "            opp = open(base+key+'_'+name+'.pickle', 'wb')\n",
    "            pickle.dump(val['2013-01-08':], opp)\n",
    "            opp.close()\n",
    "    else:\n",
    "        for key in dic[0].keys():\n",
    "            base = 'Companies/'+key+'/'\n",
    "            if not os.path.exists(base):\n",
    "                os.makedirs(base)\n",
    "                \n",
    "            val = dic[0][key]\n",
    "            val2 = dic[1][key]\n",
    "            \n",
    "            names, names2 = [], []\n",
    "            for x in range(32):\n",
    "                names.append(key+'_mac'+str(x))\n",
    "                names2.append(key+'_'+name+str(x))\n",
    "            val.columns  = names\n",
    "            val2.columns = names2\n",
    "            \n",
    "            opp  = open(base+key+'_mac.pickle', 'wb')\n",
    "            opp2 = open(base+key+'_'+name+'.pickle', 'wb')\n",
    "            pickle.dump(val['2013-01-08':], opp)\n",
    "            pickle.dump(val2['2013-01-08':], opp2)\n",
    "            opp.close()\n",
    "            opp2.close()\n",
    "    return\n",
    "\n",
    "def combined_company_indicators(highlowclose=None):\n",
    "    ticks   = ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "               'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "               'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "               'SIRI','WIN','YHOO','BHP','BP',\n",
    "               'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "               'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "               'MGM','AMD','GLW','HPQ','S','T',\n",
    "               '^GSPC','^IXIC','^DJI','GLD','USO','SPY']\n",
    "    lst = ['rsi', 'vol', 'sma', 'cci', 'per', 'mom', 'bol', \n",
    "           'aro', 'mac', 'mactwo', 'adx', 'kdo', 'rets']\n",
    "    try:\n",
    "        if highlowclose == None:\n",
    "            ticks = ticks\n",
    "        else:\n",
    "            ticks = highlowclose.keys()\n",
    "    except:\n",
    "        ticks = highlowclose.keys()\n",
    "    \n",
    "    for key in ticks:\n",
    "        ddf_lst = []\n",
    "        new_df  = pd.DataFrame()\n",
    "        \n",
    "        folder = 'Companies/'+key+'/'\n",
    "        for indic in lst:\n",
    "            opp = open(folder+key+'_'+indic+'.pickle', 'rb')\n",
    "            df  = pickle.load(opp)\n",
    "            opp.close()\n",
    "                \n",
    "            new_df = pd.concat([new_df, df], axis=1).fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        ddf1  = new_df['2013-01-08':'2013-04-08']\n",
    "        ddf2  = new_df['2013-04-09':'2013-07-08']\n",
    "        ddf3  = new_df['2013-07-09':'2013-10-08']\n",
    "        ddf4  = new_df['2013-10-09':'2014-01-08']\n",
    "        ddf5  = new_df['2014-01-09':'2014-04-10']\n",
    "        ddf6  = new_df['2014-04-11':'2014-07-10']\n",
    "        ddf7  = new_df['2014-07-11':'2014-10-09']\n",
    "        ddf8  = new_df['2014-10-16':'2015-01-09']\n",
    "        ddf9  = new_df['2015-01-12':'2015-04-13']\n",
    "        ddf10 = new_df['2015-04-14':'2015-07-13']\n",
    "        ddf11 = new_df['2015-07-14':'2015-10-12']\n",
    "        ddf12 = new_df['2015-10-13':'2016-01-12']\n",
    "        ddf13 = new_df['2016-01-13':'2016-04-21']\n",
    "        ddf14 = new_df['2016-04-22':'2016-07-22']\n",
    "        ddf15 = new_df['2016-07-25':]\n",
    "        ddf_lst = [ddf1,ddf2,ddf3,ddf4,ddf5,ddf6,ddf7,ddf8,ddf9,\n",
    "                   ddf10,ddf11,ddf12,ddf13,ddf14,ddf15]\n",
    "        \n",
    "        base = 'NewBase/'+key+'/'\n",
    "        if not os.path.exists(base):\n",
    "            os.makedirs(base)\n",
    "            \n",
    "        for x in range(len(ddf_lst)):\n",
    "            opp = open(base+key+'_df'+str(x)+'.pickle', 'wb')\n",
    "            pickle.dump(ddf_lst[x], opp)\n",
    "            opp.close()\n",
    "    return\n",
    "            \n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dump method is whether you choose to temporarily store each companies data in a folder\n",
    "called Companies/ where each subfolder is a company, and within the company folders you\n",
    "temporarily store each indicator, before finally combining all the companies indicators\n",
    "and storing 14 different combined dataframes with roughly 4 months of combined data in each.\n",
    "\n",
    "If you choose the other option, you don't use the temporary storage, and rather go straight to\n",
    "dumping and adding to the dump files in the combined data. \n",
    "\n",
    "The former method is better if you have a lot of available storage space. Each company takes up about \n",
    "3GB of data so if you're working w/ 50 companies, you need 150GB on top of the 150GB for combining \n",
    "them. You can then delete the companies folder after, unless you want to keep them to be able to look \n",
    "at each individual companies individual indicators. The latter method is better if you don't have a\n",
    "lot of storage space. This method takes WAYYY longer though!!(ex. on 50 companies, it took 3hrs for\n",
    "the first method, and 24 hrs for the second!!)\n",
    "\n",
    "dmp_method = 0 is the latter method(ie. less storage, much more computing time)\n",
    "dmp_method = 1 is the former method(ie. more storage, much less computing time)\n",
    "\"\"\"\n",
    "dmp_method = 1\n",
    "update_for_spldivs(dmp_method)\n",
    "#If just updating, rather than initialing creating the indicators, uncomment below\n",
    "# and then comment the call above.\n",
    "#update_for_spldivs(dmp_method, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
