{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "#from googlefinance import getQuotes, getNews\n",
    "#from yahoo_finance import Share\n",
    "from dateutil import rrule  \n",
    "#from sys import stdout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "#import urllib\n",
    "import cPickle as pickle\n",
    "import time\n",
    "#import csv\n",
    "#import os\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "#### # Credit to https://gist.github.com/jckantor/d100a028027c5a6b8340 ######\n",
    "     #  for these next two trading date functions\n",
    "def NYSE_holidays(a = datetime.date.today() - datetime.timedelta(days=372), \n",
    "                  b = datetime.date.today() + datetime.timedelta(days=365)): \n",
    "    \n",
    "    # Generate ruleset for holiday observances on the NYSE \n",
    "    rs = rrule.rruleset()\n",
    "    \n",
    "    # Include all potential holiday observances \n",
    "    ###############################################\n",
    "    \n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=31, \n",
    "                         byweekday=rrule.FR))               \n",
    "    # New Years Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=1))                                    \n",
    "    # New Years Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=2, \n",
    "                         byweekday=rrule.MO))                   \n",
    "    # MLK Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,\n",
    "                         byweekday=rrule.MO(3)))                            \n",
    "    # Washington's Bday\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=2,\n",
    "                         byweekday=rrule.MO(3)))                          \n",
    "    # Good Friday \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,byeaster=-2)) \n",
    "    # Memorial Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=5, \n",
    "                         byweekday=rrule.MO(-1)))                        \n",
    "    # Independence Day \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=3, \n",
    "                         byweekday=rrule.FR))              \n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=4))                                   \n",
    "    # Independence Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=5, \n",
    "                         byweekday=rrule.MO))               \n",
    "    # Labor Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=9, \n",
    "                         byweekday=rrule.MO(1)))                          \n",
    "    # Thanksgiving Day\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=11, \n",
    "                         byweekday=rrule.TH(4)))                          \n",
    "    # Christmas \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=24, \n",
    "                         byweekday=rrule.FR))                \n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=25))                                     \n",
    "    # Christmas\n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=26, \n",
    "                         byweekday=rrule.MO))                \n",
    "    ######################################################\n",
    "\n",
    "    # Exclude potential holidays that fall on weekends \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,until=b,\n",
    "                          byweekday=(rrule.SA,rrule.SU))) \n",
    "    return rs \n",
    "\n",
    "def NYSE_tradingdays(a = datetime.date.today() - datetime.timedelta(days=372), \n",
    "                     b = datetime.date.today() + datetime.timedelta(days=365)): \n",
    "    # Generate ruleset for NYSE trading days \n",
    "    rs = rrule.rruleset() \n",
    "    rs.rrule(rrule.rrule(rrule.DAILY,dtstart=a,until=b)) \n",
    "    \n",
    "    # Exclude weekends and holidays \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,byweekday=(rrule.SA,rrule.SU)))\n",
    "    rs.exrule(NYSE_holidays(a, b)) \n",
    "    \n",
    "    return rs \n",
    "\n",
    "def NYSE_holidays2(a, b): \n",
    "    rs = rrule.rruleset()\n",
    "    \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=31, \n",
    "                         byweekday=rrule.FR))               \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=1))                                    \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,bymonthday=2, \n",
    "                         byweekday=rrule.MO))                    \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=1,\n",
    "                         byweekday=rrule.MO(3)))                            \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=2,\n",
    "                         byweekday=rrule.MO(3)))                          \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,byeaster=-2)) \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=5, \n",
    "                         byweekday=rrule.MO(-1)))                         \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=3, \n",
    "                         byweekday=rrule.FR))              \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=4))                                   \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=7,bymonthday=5, \n",
    "                         byweekday=rrule.MO))               \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=9, \n",
    "                         byweekday=rrule.MO(1)))                          \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=11, \n",
    "                         byweekday=rrule.TH(4)))                          \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=24, \n",
    "                         byweekday=rrule.FR))                \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=25))                                     \n",
    "    rs.rrule(rrule.rrule(rrule.YEARLY,dtstart=a,until=b,bymonth=12,bymonthday=26, \n",
    "                         byweekday=rrule.MO))                \n",
    "\n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,until=b,\n",
    "                          byweekday=(rrule.SA,rrule.SU))) \n",
    "    return rs \n",
    "\n",
    "def NYSE_tradingdays2(a, b): \n",
    "    rs = rrule.rruleset() \n",
    "    rs.rrule(rrule.rrule(rrule.DAILY,dtstart=a,until=b)) \n",
    "    \n",
    "    rs.exrule(rrule.rrule(rrule.WEEKLY,dtstart=a,byweekday=(rrule.SA,rrule.SU)))\n",
    "    rs.exrule(NYSE_holidays2(a, b)) \n",
    "    \n",
    "    return rs\n",
    "########################################################################\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def company_news(newstickers):\n",
    "    \"\"\"\n",
    "    Use Google's server and use https://github.com/hongtaocai/googlefinance module to retrieve\n",
    "    the real-time stock news data for each company stock symbol fed to it.\n",
    "    \"\"\"\n",
    "    tickernewslist = {}\n",
    "    \n",
    "    for each in newstickers:\n",
    "        tickernewslist[each] = getNews(each)  \n",
    "    return tickernewslist\n",
    "\n",
    "def splits(stock_split_dates):\n",
    "    \"\"\"\n",
    "    Read in the stock split site data to discover if any of our watched companies have \n",
    "    stock splits in the near future so the data can be adjusted for it. Return which \n",
    "    companies if any are discovered on it.\n",
    "    \"\"\"\n",
    "    splitup        = urllib.urlopen(stock_split_dates).read()\n",
    "    companies      = splitup.find('Announced')\n",
    "    stop           = splitup.find('th_No_BG', companies)\n",
    "    company_splits = []\n",
    "    \n",
    "    # Find which companies are listed along with the stock split ratio,\n",
    "    #  the ex-date, and the payment date.\n",
    "    while True:\n",
    "        done      = splitup.find(')</a>',stop)+1\n",
    "        next_stop = splitup.rfind('(', stop, done)\n",
    "        ratio_s   = splitup.find('<td>',done)+4\n",
    "        ratio_e   = splitup.find('<',ratio_s)\n",
    "        payment_e = splitup.find('2016',done)+4\n",
    "        payment_s = splitup.rfind('<td>',stop,payment_e)+4\n",
    "        exdate_s  = splitup.find('<td>',payment_e)+4\n",
    "        exdate_e  = splitup.find('</td>',exdate_s)\n",
    "        \n",
    "        company_splits.append([splitup[next_stop+1:done-1],\n",
    "                               splitup[exdate_s:exdate_e],\n",
    "                               splitup[ratio_s:ratio_e],\n",
    "                               splitup[payment_s:payment_e]])\n",
    "        \n",
    "        tbody = splitup.find('</table>',done)\n",
    "        test  = splitup.find('(',done)\n",
    "        \n",
    "        if tbody < test:\n",
    "            break\n",
    "        stop = done\n",
    "    return company_splits\n",
    "\n",
    "def dividends(dividend_dates): \n",
    "    \"\"\"\n",
    "    Like the stock_splits() function, searches a site to find if any of our companies have\n",
    "    an upcoming dividend so as to adjust for it. Return a list of companies, if any.\n",
    "    \"\"\"\n",
    "    company_dividends = []\n",
    "    for x in xrange(0,2):\n",
    "        if x != 0:\n",
    "            dividends      = urllib.urlopen(dividend_dates).read()  \n",
    "            end            = dividends.find('Next')-2\n",
    "            start          = dividends.rfind('href=',0,end)+6\n",
    "            next_day       = dividends[start:end]\n",
    "            dividend_dates = 'http://www.nasdaq.com/dividend-stocks/'+next_day \n",
    "            \n",
    "        dividends = urllib.urlopen(dividend_dates).read()  \n",
    "        start     = dividends.find('Payment Date')\n",
    "        \n",
    "        while True:\n",
    "            start      = dividends.find('&#40;',start)+5\n",
    "            end        = dividends.find('&#41;',start)\n",
    "            ex_date_e  = dividends.find('2016',end)+4\n",
    "            ex_date_s  = dividends.rfind('>',end,ex_date_e)+1\n",
    "            div_e      = dividends.find('</',ex_date_e+1)\n",
    "            div_s      = dividends.rfind('>',ex_date_e,div_e)+1\n",
    "            rec_date_e = dividends.find('2016',div_e)+4\n",
    "            rec_date_s = dividends.rfind('>',div_e,rec_date_e)+1\n",
    "            proceed    = dividends.find('2016',rec_date_e)+4\n",
    "            pay_date_e = dividends.find('2016',proceed)+4\n",
    "            pay_date_s = dividends.rfind('>',proceed,pay_date_e)+1\n",
    "            \n",
    "            company_dividends.append([dividends[start:end],\n",
    "                                    dividends[ex_date_s:ex_date_e],\n",
    "                                    dividends[div_s:div_e],\n",
    "                                    dividends[rec_date_s:rec_date_e],\n",
    "                                    dividends[pay_date_s:pay_date_e]])\n",
    "            \n",
    "            test = dividends.find('&#40;',start)\n",
    "            if test == -1:\n",
    "                break\n",
    "            start = end      \n",
    "    return company_dividends\n",
    "\n",
    "def yahoo_backup():\n",
    "    \"\"\"\n",
    "    Due to Google occasionally blocking our google server api, a yahoo backup is \n",
    "    set up to retrieve stock info from google finance's page. It is far slower so\n",
    "    it's to be avoided if at all possible. Increasing latency between each call can\n",
    "    help Google from your blocking.\n",
    "    \"\"\"\n",
    "    test_array = []\n",
    "    yahoo      = 'http://finance.yahoo.com/q?s='\n",
    "    tickers    =     ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "                      'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "                      'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "                      'SIRI','WIN','YHOO','BHP','BP',\n",
    "                      'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "                      'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "                      'MGM','AMD','EMC','GLW','HPQ','S','T',\n",
    "                      'USO','GLD','SPY',\n",
    "                      '%5EDJI','^GSPC','%5EIXIC']\n",
    "    \n",
    "    # Retrieve the stock price for each company\n",
    "    for each in tickers:\n",
    "        stock       = yahoo + each\n",
    "        page        = urllib.urlopen(stock).read()\n",
    "        price       = page.find('time_rtq_ticker')\n",
    "        price_stop  = page.find('</span>',price)\n",
    "        price_start = page.rfind('>',price_stop-10,price_stop)+1\n",
    "        \n",
    "        test_array.append(page[price_start:price_stop])\n",
    "    return test_array\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def real_time_quotes(tickers, attributes, periods, tickerlist, combined_list, \n",
    "                     indexdict, day_hl, closes_short):\n",
    "    \"\"\"\n",
    "    As the function name implies, it retrieves stock data in real time. It will be started at\n",
    "    the beginning of the trading day(9:30AM Eastern) and ends at the end of the trading day\n",
    "    (4:00PM Eastern). It retrieves the data from Google's server by calling getQuotes which uses\n",
    "    the same program used to retrieve stock news. Credit goes to \n",
    "    https://github.com/hongtaocai/googlefinance for creating the program to retrieve the data.\n",
    "    \n",
    "    Once the data is retrieved, it's returned in a list of dictionaries, one for each company. \n",
    "    We'll take this info, convert the price to a float from a string. Then that data is taken \n",
    "    and used to create our indicators in real time. Half are calculated in the function \n",
    "    calculate_attributes() which is called here, the other half is called from that function \n",
    "    and calls calculate_extra_attributes().\n",
    "    \n",
    "    If the Google server blocks our retrieval process, we call our yahoo_backup() function \n",
    "    which retrieves the data straight from the yahoo finance's website. This is considerably \n",
    "    slower so it's always better to increase the sleep time between each retrieval to slow down \n",
    "    how often we're retrieving data as this has proved to be effective in combatting being blocked \n",
    "    in the first place.\n",
    "    \n",
    "    Returns 3 dictionaries: \n",
    "        -tickerlist = Just the a list of dictionaries for each day for the whole trading day for \n",
    "                      each company. Each company can be called with its stock ticker symbol and \n",
    "                      that retrieves the list of dictionaries.\n",
    "        -combined   = Lists the latest indicator list. There are 24 indicators ranging from \n",
    "                      bollinger bands, to PE/Ratios, to Volatilities, etc.. as well as daily \n",
    "                      returns from the top 5 major indexes including the S&P, NYSE, etc. Also \n",
    "                      the daily returns for the top 7 major sectors, such as technology, healthcare, \n",
    "                      etc.\n",
    "        -last_close = The most current closing price for each company, this will be used to update \n",
    "                      the day-end closing price that's placed in the closing price dictionary. This \n",
    "                      closing price dictionary is a pickled file to increase speed and has the \n",
    "                      closing prices of the stocks since going public.\n",
    "    \"\"\"    \n",
    "    last_close, count = {}, 0\n",
    "    \n",
    "    # Use the python time module to time how long to retrieve stock data for. A normal\n",
    "    #  trading day means 6.5 hours of trading data.\n",
    "    seconds = 60\n",
    "    minutes = 60\n",
    "    hours   = 6.5\n",
    "    t_end   = time.time() + (seconds * minutes * hours)\n",
    "    \n",
    "    while time.time() < t_end:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            # Try to retrieve the quotes from the Google server\n",
    "            next_quotes = getQuotes(tickers) \n",
    "            \n",
    "            index_returns = []\n",
    "            for x, name in zip(range(52), tickers):\n",
    "                # Append our new data to our data list\n",
    "                tickerlist[name].append(next_quotes[x])\n",
    "                \n",
    "                # Set close value with time\n",
    "                close_value      = float(next_quotes[x]['LastTradePrice'].replace(',',''))\n",
    "                time_value       = pd.to_datetime(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                last_close[name] = [close_value, time_value]\n",
    "                \n",
    "                # Append current stock price to our year's worth of closing prices\n",
    "                closes_short[name][time_value] = close_value\n",
    "                    \n",
    "                # If the latest stock price is a day low or day high, set new high/low \n",
    "                day_hl[name][0] = min(close_value, day_hl[name][0])\n",
    "                day_hl[name][1] = max(close_value, day_hl[name][1])\n",
    "                \n",
    "                # Use the return values of gold, oil, indexes, and sectors for indicator values\n",
    "                if x > 43:\n",
    "                    closings       = indexdict[name]\n",
    "                    minute         = close_value / closings[0].iloc[count] - 1.\n",
    "                    hour           = close_value / closings[1].iloc[count] - 1.\n",
    "                    day            = close_value / closings[2].iloc[count] - 1.\n",
    "                    two_day        = close_value / closings[3].iloc[count] - 1.\n",
    "                    three_day      = close_value / closings[4].iloc[count] - 1.\n",
    "                    five_day       = close_value / closings[5].iloc[count] - 1.\n",
    "                    index_returns += [minute, hour, day, two_day, three_day, five_day]\n",
    "                \n",
    "            # Call the attributes function which calculates our list of indicators in \n",
    "            #  real time which will be used to feed to the real-time recurrent neural netwk\n",
    "            combined_list, mylist, attributes = calculate_attributes(tickers, last_close, \n",
    "                                                closes_short, attributes, combined_list, day_hl, \n",
    "                                                index_returns, periods)\n",
    "            stdout.write(\"\\r%d\" % count)\n",
    "            stdout.flush()   \n",
    "        \n",
    "        except:\n",
    "            # If Google's server denies our retrieval, call yahoo_backup()\n",
    "            next_quotes = yahoo_backup()\n",
    "            for x in xrange(0,len(next_quotes)):\n",
    "                tickerlist[x].append(next_quotes[x])\n",
    "                  \n",
    "            print \"PRIMARY FAIL\", count\n",
    "            pass\n",
    "        \n",
    "        # Stop the timing, and subtract this from 60 seconds,ie. every loop = 60secs\n",
    "        end = time.time()\n",
    "        time.sleep(60 - (end-start))\n",
    "        count += 1\n",
    "        \n",
    "    return tickerlist, combined\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def get_data_ready():\n",
    "    \"\"\"\n",
    "    First function called at the beginning of the trading day.\n",
    "    \n",
    "    Setup the stock ticker symbols for both the Google and Yahoo servers, the Google \n",
    "    for real-time stock data, and the yahoo for previous high/low values. Then call the \n",
    "    setup function to create the dictionaries for several of the indicators getting the \n",
    "    previous data for these indicators by using the historical stock data brought in from \n",
    "    the pickled file. Then put all these newly created dictionaries in a list called attributes.\n",
    "    \n",
    "    Then find the companies with upcoming stock splits and dividends and compare them to \n",
    "    our list to see if we have any companies with upcoming dates so we can adjust for them. \n",
    "    As of right now (2016-06-11), the function to adjust has not been created but will be soon.\n",
    "    \n",
    "    Finally call the company_news() function to retrieve info for each company and \n",
    "    index/sector for new articles on them. As of right now the function to automatically \n",
    "    read the data, and convert the data to extra information to help in prediction, has not \n",
    "    been created but will be created soon. It will utilize a neural network to comprehend \n",
    "    the data, possible using Google's new Parsy McParseFace module just released that is \n",
    "    built for NLP(Natural Language Processing).\n",
    "    \"\"\"\n",
    "    #Historical intraday trading data for every company/index/sector at 1 min intervals\n",
    "    # since 2012\n",
    "    bonn        = open('pickledadjustedintracomplete3.pickle', 'rb')\n",
    "    intra_data  = pickle.load(bonn)\n",
    "    bonn.close()\n",
    "    \n",
    "    # Dictionaries that will be used throughout program\n",
    "    macd_slmult, tickerlist, combined_list, retdict, day_hl = {}, {}, {}, {}, {}\n",
    "    hlc_short, adx_short, rsi_short, mac_short, kdo_short = {}, {}, {}, {}, {}\n",
    "    lhl, lml, closes_short = {}, {}, {}\n",
    "    \n",
    "    #Dates at certain trading days(Ex. d1 means yesterdays date, d3 means 3 trading days agos\n",
    "    # date). hr is the start the previous datetime one hour ago at the start of the trading\n",
    "    # day(ie. yesterdays date + 3:00PM)\n",
    "    rs = NYSE_tradingdays()\n",
    "    d257, d252, d100 = str(rs[0])[:10],   str(rs[5])[:10],   str(rs[157])[:10]\n",
    "    d50,  d40,  d30  = str(rs[207])[:10], str(rs[217])[:10], str(rs[227])[:10]\n",
    "    d26,  d25,  d20  = str(rs[231])[:10], str(rs[232])[:10], str(rs[237])[:10]\n",
    "    d16,  d14,  d12  = str(rs[241])[:10], str(rs[243])[:10], str(rs[245])[:10]\n",
    "    d10,  d8,   d5   = str(rs[247])[:10], str(rs[249])[:10], str(rs[252])[:10]\n",
    "    d3,   d2,   d1   = str(rs[254])[:10], str(rs[255])[:10], str(rs[256])[:10]\n",
    "    hr = d1 + ' 15'\n",
    "    #All period lengths for indicators and returns.\n",
    "    #  kd/voli/mom/rsi/adx/cci/bol = 3-20 days\n",
    "    #  aroon                       = 8-30 days\n",
    "    #  sma                         = 14-100 days \n",
    "    #  pe                          = 16-252 days\n",
    "    prd_lst  = [d3, d5, d8, d10, d12, d14, d16, d20, d25, d30, d40, d50, d100, d252]\n",
    "    #  macd_small_period           = 3-20 days\n",
    "    #  macd_large_period           = 12-50 days \n",
    "    prd_lst2 = [d3, d5, d8, d10, d12, d14, d16, d20, d26, d30, d40, d50]\n",
    "    #  return_periods              = 1hr - 5days\n",
    "    prd_lst3 = [hr, d1, d2, d3, d5]\n",
    "    \n",
    "    # Ticker symbols for both yahoo and google servers\n",
    "    tickers        = ['BPOP','FITB','HBAN','CMCSA','EBAY','AAPL','AMAT','BRCD','CSCO',\n",
    "                      'GOOG','INTC','LVLT','MSFT','MU','NVDA','ORCL','QCOM','SIRI','WIN',\n",
    "                      'YHOO','BHP','BP','RIO','XOM','GE','F','MO','XRX','GS','JPM','LYG',\n",
    "                      'MS','RF','USB','WFC','MRK','PFE','NYSE:LMT','MGM','AMD','EMC','GLW',\n",
    "                      'HPQ','S','T','NYSEARCA:USO','NYSEARCA:GLD','NYSEARCA:SPY',\n",
    "                      'INDEXDJX:.DJI','INDEXSP:.INX','INDEXNASDAQ:.IXIC']\n",
    "    yahoo_tickers  = ['BPOP','FITB','HBAN','CMCSA','EBAY','AAPL','AMAT','BRCD','CSCO',\n",
    "                      'GOOG','INTC','LVLT','MSFT','MU','NVDA','ORCL','QCOM','SIRI','WIN',\n",
    "                      'YHOO','BHP','BP','RIO','XOM','GE','F','MO','XRX','GS','JPM','LYG',\n",
    "                      'MS','RF','USB','WFC','MRK','PFE','LMT','MGM','AMD','EMC','GLW','HPQ',\n",
    "                      'S','T','USO', 'GLD', 'SPY','^DJI', '^GSPC', '^IXIC']\n",
    "    \n",
    "    # Setup attribute dictionaries\n",
    "    highlowclose = add_typical_create(intra_data, yahoo_tickers)\n",
    "    adx_dict     = adx_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    rsi_dict     = rsi_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    sma_dict     = sma_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    mom_dict     = mom_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    bol_dict     = bol_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    vol_dict     = vol_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    cci_dict     = cci_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    aro_dict     = aro_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    kd_dict      = kd_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    pe_dict      = pe_create(highlowclose, yahoo_tickers, prd_lst)\n",
    "    macd_dict    = macd_create(highlowclose, yahoo_tickers, prd_lst2)\n",
    "    returns_dict = returns_create(highlowclose, yahoo_tickers, prd_lst3)\n",
    "    \n",
    "    for name, name2 in zip(highlowclose.keys(), tickers):\n",
    "        #Set up the tickerlist which will contain the days closing prices, combined_list will\n",
    "        # contain the indicator values\n",
    "        tickerlist[name2]    = []\n",
    "        combined_list[name2] = []\n",
    "        \n",
    "        #Only use the shortened dictionaries to save memory as you only need them\n",
    "        # for real-time calculations\n",
    "        hlc_short[name]      = highlowclose[name][day257:]\n",
    "        adx_short[name]      = adx_dict[name][day257:]\n",
    "        rsi_short[name]      = rsi_dict[name][day257:]\n",
    "        mac_short[name]      = macd_dict[name][day257:]\n",
    "        kdo_short[name]      = kd_dict[name][day257:]\n",
    "        \n",
    "        # Create the shortened close dictionary for calculations and lengths of those dfs\n",
    "        closes_short[name]   = hlc_short[name]['Closes']\n",
    "        clss_day_start[name] = len(closes_short[name][:day1])\n",
    "        \n",
    "        # day_hl is day highs/lows that will keep track of those throughout day\n",
    "        # lhl and lml are lengths of the highlowclose df lengths and shortened lengths\n",
    "        # mi stands for minute, the last calculation yesterday\n",
    "        day_hl[name]         = [None, None]\n",
    "        lml[name]            = len(hlc_short[name])\n",
    "        lhl[name]            = len(highlowclose[name])\n",
    "        mi                   = len(hlc_short[name]) - 1\n",
    "        \n",
    "        # Used to calculate returns for indicators\n",
    "        retdict[name]        = [[] for i in range(6)]\n",
    "        retdict[name][0]     = hlc_short[name]['Closes'].iloc[mi:]\n",
    "        retdict[name][1]     = hlc_short[name]['Closes'][hr:]\n",
    "        retdict[name][2]     = hlc_short[name]['Closes'][day1:]\n",
    "        retdict[name][3]     = hlc_short[name]['Closes'][day2:]\n",
    "        retdict[name][4]     = hlc_short[name]['Closes'][day3:]\n",
    "        retdict[name][5]     = hlc_short[name]['Closes'][day5:]\n",
    "        \n",
    "        multlist = []\n",
    "        for x in range(8):\n",
    "            # Used to get the starting row number for each period length so\n",
    "            # we don't have to calculate them each time in the calculations func\n",
    "            prd_start    = len(highlowclose[name][:prd_lst[x]])\n",
    "            aro_start    = len(highlowclose[name][:prd_lst[x+2]])\n",
    "            sma_start    = len(highlowclose[name][:prd_lst[x+5]])\n",
    "            per_start    = len(highlowclose[name][:prd_lst[x+6]])\n",
    "            day_start    = len(highlowclose[name][:prd_lst2[1]])\n",
    "            \n",
    "            # Lengths of each period needed in the calculation func, done here for\n",
    "            # efficiency\n",
    "            len_prd      = len(highlowclose[name][prd_lst[x]:])\n",
    "            len_aro      = len(highlowclose[name][prd_lst[x+2]:])\n",
    "            len_mac      = len(highlowclose[name][prd_lst2[x]:])\n",
    "            len_mac2     = len(highlowclose[name][prd_lst2[x+3]:])\n",
    "            \n",
    "            # Moving average convergence divergence small and large multiplier, done here\n",
    "            # for efficiency\n",
    "            macd_smult   = 780. / (len_mac  + 390.)\n",
    "            macd_lmult   = 780. / (len_mac2 + 390.)\n",
    "            \n",
    "            # combine everything\n",
    "            cmb = [macd_smult, macd_lmult, len_prd, len_aro, prd_start, \n",
    "                   aro_start, sma_start, per_start, day_start]\n",
    "            multlist.append(cmb)  \n",
    "        macd_slmult[name] = multlist  \n",
    "    \n",
    "    try:\n",
    "        # Find companies with upcoming dividends and stock splits\n",
    "        div_dates = 'http://www.nasdaq.com/dividend-stocks/dividend-calendar.aspx'\n",
    "        spl_dates = 'http://www.nasdaq.com/markets/upcoming-splits.aspx'\n",
    "        dividend  = pd.DataFrame(dividends(div_dates),\n",
    "                                 columns=['Symbol','ExDiv','Div','RecDate','PayDate'])\n",
    "        split     = pd.DataFrame(splits(split_dates),\n",
    "                                 columns=['Symbol','ExDate','Ratio','Payable'])\n",
    "        divs      = dividend['Symbol'].values\n",
    "        spls      = split['Symbol'].values\n",
    "\n",
    "        # Print out which companies have stock splits and dividends upcoming, if any\n",
    "        for both, i in [divs, spls], range(2):\n",
    "            for each in both:\n",
    "                if each in new_tickers:\n",
    "                    if i == 0:\n",
    "                        print d, \"DIV\"\n",
    "                    else:\n",
    "                        print d, \"SPLITS\"\n",
    "        \n",
    "        #Return each companys trading news for the day from Googles servers and dump\n",
    "        # them into a pickle file\n",
    "        todays_news   = company_news(tickers)\n",
    "        tn = open((str(rs[257])[:10]+'daynews.pickle'), 'wb')\n",
    "        pickle.dump(todays_news, tn)\n",
    "        tn.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Combine lists and dicts\n",
    "    periods    = [prd_lst, prd_lst2, prd_lst3, macd_slmult, clss_day_start, lhl, lml]\n",
    "    attributes = [hlc_short, adx_short, rsi_short, mac_short, kdo_short]\n",
    "    \n",
    "    return tickers, attributes, periods, tickerlist, combined_list, retdict, day_hl, closes_short\n",
    "\n",
    "def run_program():\n",
    "    \"\"\"\n",
    "    Function will wait until the start of the training day(9:30AM) and start the\n",
    "    process about 2 minute before start by calling get_data_ready() which takes roughly\n",
    "    2 minutes to process everything so it starts recording and calculating in real-time\n",
    "    at about 9:30.\n",
    "    \n",
    "    Until the time is between 9:28 and 9:30, it will check if we're in that range, else\n",
    "    it sleeps for 60 seconds and does this until its called.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        now      = datetime.datetime.now()\n",
    "        now_time = now.time()\n",
    "        \n",
    "        if now_time >= datetime.time(9,28) and now_time <= datetime.time(9,30):\n",
    "            tickers, attributes, p, tlist, clist, rdct, dhl, cls = get_data_ready()\n",
    "            tlist, combined = real_time_quotes(ts, attrs, p, tlist, clist, rdct, dhl, cls)\n",
    "            break\n",
    "    \n",
    "    return tlist, combined\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tickerlist, combined = run_program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def add_typical_create(data, yahoo_tickers):\n",
    "    hlc = {}\n",
    "    \n",
    "    for name in yahoo_tickers:\n",
    "        df            = data[name]\n",
    "        df['Typical'] = (df['Highs'] + df['Lows'] + df['Closes']) / 3.\n",
    "        hlc[name]     = df\n",
    "\n",
    "    return hlc\n",
    "\n",
    "def create_prd_lst(highlowclose, prd_lst, prd_lst2, prd_lst_nums, prd_lst_nums2, prd_lst_nums3):\n",
    "    prd_dict  = {}\n",
    "    \n",
    "    for name in highlowclose.keys():\n",
    "        close = highlowclose[name]['Closes']\n",
    "\n",
    "        index = close.index\n",
    "        start = datetime.datetime.strptime((str(index[0])[:10]), '%Y-%m-%d').date()\n",
    "        end   = datetime.datetime.strptime((str(index[-1])[:10]), '%Y-%m-%d').date()\n",
    "        rs2   = NYSE_tradingdays2(start, end)\n",
    "        \n",
    "        len_p  = []\n",
    "        len_p2 = []\n",
    "        all_l  = []\n",
    "        for each in prd_lst:\n",
    "            len_p.append(len(close[:(str(rs2[each-1])[:10])]))\n",
    "        for each2 in prd_lst2:\n",
    "            len_p2.append(len(close[:(str(rs2[each2-1])[:10])]))\n",
    "\n",
    "        lst1 = prd_lst_nums  + len_p\n",
    "        lst2 = prd_lst_nums2 + len_p\n",
    "        lst3 = prd_lst_nums3 + len_p\n",
    "        lst4 = prd_lst_nums  + len_p2\n",
    "        \n",
    "        all_l.append(lst1)\n",
    "        all_l.append(lst2)\n",
    "        all_l.append(lst3)\n",
    "        all_l.append(lst4)\n",
    "        \n",
    "        prd_dict[name] = all_l\n",
    "    return prd_dict\n",
    "\n",
    "def returns_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        cols   = []\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        \n",
    "        prds   = periods[name][0]\n",
    "        prds   = [1, 5] + prds\n",
    "        \n",
    "        for each in prds:\n",
    "            cols.append(str(each)+'mins')\n",
    "        \n",
    "        for x in range(34):\n",
    "            df[x] = ((close / close.shift(prds[x]) - 1.).fillna(0)).replace([np.inf], 0)\n",
    "        df.columns = cols\n",
    "            \n",
    "        dmp = open('Indicators/Returns/hist_rets_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def pe_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        cl     = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32): \n",
    "            clshf = (cl - cl.shift(prds[x])).replace(0,np.NaN)\n",
    "            df[x] = (cl / clshf).fillna(0)\n",
    "            \n",
    "        dmp = open('PriceEarningsRatio/hist_per_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def kd_create(highlowclose, periods):  \n",
    "    for name in highlowclose.keys(): \n",
    "        k_df  = pd.DataFrame()\n",
    "        d_df  = pd.DataFrame()\n",
    "        high  = highlowclose[name]['Highs']\n",
    "        low   = highlowclose[name]['Lows']\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][1]\n",
    "        \n",
    "        for x in range(32):\n",
    "            prev_max  = high.rolling(window = prds[x+1], center = False).max()\n",
    "            prev_min  = low.rolling(window  = prds[x+1], center = False).min()\n",
    "        \n",
    "            cl  =  close    - prev_min\n",
    "            hl  = (prev_max - prev_min).replace(0, np.NaN)\n",
    "            \n",
    "            k_df[x] = ((cl / hl) * 100.).fillna(0)\n",
    "            d_df[x] = (k_df[x].rolling(window=prds[x], center=False).mean()).fillna(0)\n",
    "            \n",
    "        dmp  = open('StochasticOscillators/hist_k_'+name+'.pickle','wb')\n",
    "        dmp2 = open('StochasticOscillators/hist_d_'+name+'.pickle','wb')\n",
    "        pickle.dump(k_df, dmp)\n",
    "        pickle.dump(d_df, dmp2)\n",
    "        dmp.close()\n",
    "        dmp2.close()\n",
    "    return\n",
    "\n",
    "def cci_create(highlowclose, periods):\n",
    "    constant = 0.015\n",
    "    \n",
    "    for name in highlowclose.keys():\n",
    "        df   = pd.DataFrame()\n",
    "        typ  = highlowclose[name]['Typical']\n",
    "        prds = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            typ_std  = typ.rolling(window = prds[x], center = False).std()\n",
    "            typ_mean = typ.rolling(window = prds[x], center = False).mean()\n",
    "            ttmean   = typ - typ_mean\n",
    "            ctmad    = (constant * typ_std).replace(0,np.NaN)\n",
    "            df[x]    = (ttmean / ctmad).fillna(0)\n",
    "        \n",
    "        dmp = open('CommodityChannelIndex/hist_cci_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def vol_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        vol_df = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):    \n",
    "            rets      = (close / close.shift(1) - 1.).fillna(0)\n",
    "            vol_df[x] = rets.rolling(window=prds[x], center=False).std() * np.sqrt(prds[x])\n",
    "        \n",
    "        dmp = open('Volatility/hist_vol_'+name+'.pickle','wb')\n",
    "        pickle.dump(vol_df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def bol_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        df    = pd.DataFrame()\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            rm    = (close.rolling(window=prds[x], center=False).mean()).fillna(0)\n",
    "            rstd  = (close.rolling(window=prds[x], center=False).std()).fillna(0)\n",
    "            upper = rm + rstd * 2. \n",
    "            clrm  = close - rm\n",
    "            uprm  = (upper - rm).replace(0,np.NaN)\n",
    "            df[x] = (clrm / uprm).fillna(0)\n",
    "            \n",
    "        dmp = open('Bollinger/hist_bol_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def mom_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            df[x] = ((close / close.shift(prds[x])) - 1.).fillna(0)\n",
    "        \n",
    "        dmp = open('Momentum/hist_mom_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def sma_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        df    = pd.DataFrame()\n",
    "        close = highlowclose[name]['Closes']\n",
    "        prds  = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            df[x] = (close.rolling(window=prds[x], center=False).mean()).fillna(0)\n",
    "            \n",
    "        dmp = open('SimpleMovingAverage/hist_sma_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "\n",
    "def aro_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        df     = pd.DataFrame()\n",
    "        close  = highlowclose[name]['Closes']\n",
    "        prds   = periods[name][0]\n",
    "        \n",
    "        for x in range(32):\n",
    "            df[x] = close.rolling(window=prds[x], center=False).apply(aro_apply, args=(prds[x],))\n",
    "            \n",
    "        dmp = open('AroonIndicator/hist_aro_'+name+'.pickle','wb')\n",
    "        pickle.dump(df, dmp)\n",
    "        dmp.close()\n",
    "    return\n",
    "def aro_apply(df, prd):\n",
    "    up      = ((prd - df.argmax()) / float(prd)) * 100.\n",
    "    down    = ((prd - df.argmin()) / float(prd)) * 100.\n",
    "    return up - down\n",
    "\n",
    "def macd_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        small_df  = pd.DataFrame()\n",
    "        large_df  = pd.DataFrame()\n",
    "        macd_df   = pd.DataFrame()\n",
    "        close     = highlowclose[name]['Closes']\n",
    "        prds      = periods[name][2]\n",
    "        \n",
    "        for x in range(32):\n",
    "            df_close    = close[prds[x]:]\n",
    "            macd_small  = pd.Series(0.0, close.index)\n",
    "            macd_large  = pd.Series(0.0, close.index)\n",
    "            macd_series = pd.Series(0.0, close.index)\n",
    "            \n",
    "            macd_small[prds[x]   - 1] = sum(close[:prds[x]])   / prds[x]\n",
    "            macd_large[prds[x+3] - 1] = sum(close[:prds[x+3]]) / prds[x+3]\n",
    "            \n",
    "            i = prds[x]\n",
    "            df_close.apply(ema_calc, args=(prds[x], prds[x+3],))\n",
    "            \n",
    "            small_df[x] = macd_small\n",
    "            large_df[x] = macd_large\n",
    "            macd_df[x]  = macd_series\n",
    "        \n",
    "        dmp  = open('MovingAverageCD/hist_sm_'+name+'.pickle','wb')\n",
    "        dmp2 = open('MovingAverageCD/hist_lg_'+name+'.pickle','wb')\n",
    "        dmp3 = open('MovingAverageCD/hist_mac_'+name+'.pickle','wb')\n",
    "        pickle.dump(small_df, dmp)\n",
    "        pickle.dump(large_df, dmp2)\n",
    "        pickle.dump(macd_df, dmp3)\n",
    "        dmp.close()\n",
    "        dmp2.close()\n",
    "        dmp3.close()\n",
    "    return\n",
    "def ema_calc(cl, prd, prd2):\n",
    "    global i\n",
    "    macd_smult = 2. / (prd  + 1.)\n",
    "    macd_lmult = 2. / (prd2 + 1.)\n",
    "    try:\n",
    "        if i < prd2:\n",
    "            macd_small[i]  = (cl - macd_small[i-1]) * macd_smult + macd_small[i-1]\n",
    "        else:\n",
    "            macd_large[i]  = (cl - macd_large[i-1]) * macd_lmult + macd_large[i-1]\n",
    "            macd_series[i] = macd_small[i] - macd_large[i]\n",
    "        i += 1\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "def adx_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        adx_df   = pd.DataFrame()\n",
    "        tr14_df  = pd.DataFrame()\n",
    "        pdm14_df = pd.DataFrame()\n",
    "        ndm14_df = pd.DataFrame()\n",
    "        high     = highlowclose[name]['Highs']\n",
    "        low      = highlowclose[name]['Lows']\n",
    "        close    = highlowclose[name]['Closes']\n",
    "        prds     = periods[name][0]\n",
    "        \n",
    "        plus_dm  = (high - high.shift(1)).fillna(0)\n",
    "        minus_dm = (low.shift(1) - low).fillna(0)\n",
    "        plus_dm[plus_dm < 0]   = 0\n",
    "        minus_dm[minus_dm < 0] = 0\n",
    "        \n",
    "        tr_df     = pd.DataFrame()\n",
    "        tr_df[0]  = (high - low)\n",
    "        tr_df[1]  = (high - close.shift(1)).fillna(0)\n",
    "        tr_df[2]  = (low  - close.shift(1)).fillna(0)\n",
    "        tr_max_df = tr_df.max(axis=1)\n",
    "        \n",
    "        ad_df     = pd.DataFrame()\n",
    "        ad_df[0]  = plus_dm\n",
    "        ad_df[1]  = minus_dm\n",
    "        ad_df[2]  = tr_max_df\n",
    "        \n",
    "        for x in range(32):\n",
    "            tr_series    = pd.Series(0.0, high.index)\n",
    "            pdm_series   = pd.Series(0.0, high.index)\n",
    "            ndm_series   = pd.Series(0.0, high.index)\n",
    "            adx_series   = pd.Series(0.0, high.index)\n",
    "                \n",
    "            tr_series[prds[x]]  = sum(tr_max_df[1 : prds[x] + 1])\n",
    "            pdm_series[prds[x]] = sum(plus_dm[1 : prds[x] + 1])\n",
    "            ndm_series[prds[x]] = sum(minus_dm[1 : prds[x] + 1])\n",
    "            \n",
    "            i  = prds[x] + 1\n",
    "            df = ad_df.iloc[prds[x] + 1:]\n",
    "            t1 = df.apply(lambda row: adx_calc(row[0], row[1], row[2], prds[x]+1), axis=1)\n",
    "\n",
    "            pos_di  = ((pdm_series / tr_series) * 100.).replace([np.inf],0)\n",
    "            neg_di  = ((ndm_series / tr_series) * 100.).replace([np.inf],0)\n",
    "            dx      = ((pos_di - neg_di) / (pos_di + neg_di)).replace([np.inf],0)\n",
    "\n",
    "            adx_series[(2 * prds[x])] = sum(dx.iloc[prds[x] + 1 : (2 * prds[x]) + 1])\n",
    "            \n",
    "            i  = 2 * prds[x] + 1\n",
    "            df = dx.iloc[(2 * prds[x] + 1):]\n",
    "            t2 = df.apply(lambda row: adx_calc2(row, (2 * prds[x] + 1)))\n",
    "            \n",
    "            tr14_df[x]   = tr_series\n",
    "            pdm14_df[x]  = pdm_series \n",
    "            ndm14_df[x]  = ndm_series\n",
    "            adx_df[x]    = adx_series\n",
    "        \n",
    "        dmp  = open('AverageDirectionalIndex/hist_tr14_'+name+'.pickle','wb')\n",
    "        dmp2 = open('AverageDirectionalIndex/hist_pdm14_'+name+'.pickle','wb')\n",
    "        dmp3 = open('AverageDirectionalIndex/hist_ndm14_'+name+'.pickle','wb')\n",
    "        dmp4 = open('AverageDirectionalIndex/hist_adx_'+name+'.pickle','wb')\n",
    "        pickle.dump(tr14_df, dmp)\n",
    "        pickle.dump(pdm14_df, dmp2)\n",
    "        pickle.dump(ndm14_df, dmp3)\n",
    "        pickle.dump(adx_df, dmp4)\n",
    "        dmp.close()\n",
    "        dmp2.close()\n",
    "        dmp3.close()\n",
    "        dmp4.close()\n",
    "    return\n",
    "def adx_calc(cl, cl2, cl3, prd):\n",
    "    global i\n",
    "    try:\n",
    "        tr_series[i]  = tr_series[i-1]  - (tr_series[i-1]  / prd) + cl3\n",
    "        pdm_series[i] = pdm_series[i-1] - (pdm_series[i-1] / prd) + cl\n",
    "        ndm_series[i] = ndm_series[i-1] - (ndm_series[i-1] / prd) + cl2\n",
    "        i += 1\n",
    "    except:\n",
    "        return\n",
    "def adx_calc2(cl, prd):\n",
    "    global i\n",
    "    try:\n",
    "        adx_series[i] = ((adx_series[i-1] * (prd-1.)) + dx.iloc[i]) / prd\n",
    "        i += 1\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "def rsi_create(highlowclose, periods):\n",
    "    for name in highlowclose.keys():\n",
    "        rsi_df  = pd.DataFrame()\n",
    "        avgg_df = pd.DataFrame()\n",
    "        avgl_df = pd.DataFrame()\n",
    "        \n",
    "        close   = highlowclose[name]['Closes']\n",
    "        deltas  = (close - close.shift(1)).fillna(0)\n",
    "        prds    = periods[name][0]\n",
    "\n",
    "        for x in range(32):\n",
    "            avg_gain_series = pd.Series(0.0, deltas.index)\n",
    "            avg_loss_series = pd.Series(0.0, deltas.index)\n",
    "            \n",
    "            gain_sum = deltas[1 : prds[x] + 1]\n",
    "            gain_per = gain_sum[gain_sum > 0].sum() / float(prds[x])\n",
    "            \n",
    "            loss_sum = -deltas[1 : (prds[x] + 1)]\n",
    "            loss_per = loss_sum[loss_sum < 0].sum() / float(prds[x])\n",
    "            \n",
    "            avg_gain_series.iloc[prds[x]] = gain_per \n",
    "            avg_loss_series.iloc[prds[x]] = loss_per\n",
    "            \n",
    "            # Now calculate RSI using the Wilder smoothing method, starting with n+1 delta.\n",
    "            up   = lambda y:  y if y > 0 else 0\n",
    "            down = lambda y: -y if y < 0 else 0\n",
    "            \n",
    "            i  = prds[x] + 1\n",
    "            df = deltas[prds[x] + 1:]\n",
    "            t1 = df.apply(lambda row: rsi_calc(row, (prds[x]+1)))\n",
    "                 \n",
    "            rs  = (avg_gain_series / avg_loss_series).replace([np.inf], 100.)\n",
    "            rsi = 100. - (100. / (1. + rs))\n",
    "            \n",
    "            avgg_df[x] = avg_gain_series\n",
    "            avgl_df[x] = avg_loss_series\n",
    "            rsi_df[x] = rsi\n",
    "        \n",
    "        dmp  = open('RelativeStrengthIndex/hist_avgg_'+name+'.pickle','wb')\n",
    "        dmp2 = open('RelativeStrengthIndex/hist_avgl_'+name+'.pickle','wb')\n",
    "        dmp3 = open('RelativeStrengthIndex/hist_rsi_'+name+'.pickle','wb')\n",
    "        pickle.dump(avgg_df, dmp)\n",
    "        pickle.dump(avgl_df, dmp2)\n",
    "        pickle.dump(rsi_df, dmp3)\n",
    "        dmp.close()\n",
    "        dmp2.close()\n",
    "        dmp3.close()\n",
    "    return\n",
    "def rsi_calc(cl, prd):\n",
    "    global i\n",
    "    try:\n",
    "        avg_gain_series[i] = ((avg_gain_series[i-1] * (prd-1.)) + up(cl))   / float(prd)\n",
    "        avg_loss_series[i] = ((avg_loss_series[i-1] * (prd-1.)) + down(cl)) / float(prd)\n",
    "        i += 1\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "bonn       = open('pickledadjustedintracomplete3.pickle', 'rb')\n",
    "highlowclose = pickle.load(bonn)\n",
    "bonn.close()\n",
    "\n",
    "prd_lst      = [1, 2, 3, 5, 8, 10, 12, 14, 16, 20, 25, 30, 40, 50, 100, 252]\n",
    "prd_lst2     = [1, 2, 3, 5]\n",
    "plnums       = [10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300]\n",
    "plnums2      = [8, 10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300, 350]\n",
    "plnums3      = [6, 8, 10, 14, 16, 18, 20, 25, 30, 40, 50, 75, 100, 125, 150, 200, 250, 300, 350]\n",
    "\n",
    "prd_dict     = create_prd_lst(highlowclose, prd_lst, prd_lst2, plnums, plnums2, plnums3)\n",
    "#highlowclose = add_typical_create(intra_cpy, yahoo_tickers)\n",
    "#sma_dict     = sma_create(highlowclose, prd_dict)\n",
    "#mom_dict     = mom_create(highlowclose, prd_dict)\n",
    "#bol_dict     = bol_create(highlowclose, prd_dict)\n",
    "#vol_dict     = vol_create(highlowclose, prd_dict)\n",
    "#cci_dict     = cci_create(highlowclose, prd_dict)\n",
    "#pe_dict      = pe_create(highlowclose, prd_dict)\n",
    "#kd_dict      = kd_create(highlowclose, prd_dict)\n",
    "#aro_dict     = aro_create(highlowclose, prd_dict)\n",
    "#rsi_dict     = rsi_create(highlowclose, prd_dict)\n",
    "#macd_dict    = macd_create(highlowclose, prd_dict)\n",
    "#adx_dict     = adx_create(highlowclose, prd_dict)\n",
    "#returns_dict = returns_create(highlowclose, prd_dict)\n",
    "\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adx_create(highlowclose, prd_dict)\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "#arof   = open('Indicators/AroonIndicator/hist_aro_'+ticker+'.pickle','rb')\n",
    "#bolf   = open('Indicators/Bollinger/hist_bol_'+ticker+'.pickle','rb')\n",
    "#ccif   = open('Indicators/CommodityChannelIndex/hist_cci_'+ticker+'.pickle','rb')\n",
    "#momf   = open('Indicators/Momentum/hist_mom_'+ticker+'.pickle','rb')\n",
    "#volf   = open('Indicators/Volatility/hist_vol_'+ticker+'.pickle','rb')\n",
    "#smaf   = open('Indicators/SimpleMovingAverage/hist_sma_'+ticker+'.pickle','rb')\n",
    "#perf   = open('Indicators/PriceEarningsRatio/hist_per_'+ticker+'.pickle','rb')\n",
    "#kdof   = open('Indicators/StochasticOscillators/hist_d_'+ticker+'.pickle','rb')\n",
    "#macf   = open('Indicators/MovingAverageCD/hist_mac_'+ticker+'.pickle','rb')\n",
    "#rsif   = open('Indicators/RelativeStrengthIndex/hist_rsi_'+ticker+'.pickle','rb')\n",
    "#adxf   = open('Indicators/AverageDirectionalIndex/hist_adx_'+ticker+'.pickle','rb')\n",
    "retsf  = open('Indicators/Returns/hist_rets_'+ticker+'.pickle','rb')\n",
    "\n",
    "#aro    = pickle.load(arof)\n",
    "#bol    = pickle.load(bolf)\n",
    "#cci    = pickle.load(ccif)\n",
    "#mom    = pickle.load(momf)\n",
    "#vol    = pickle.load(volf)\n",
    "#sma    = pickle.load(smaf)\n",
    "#per    = pickle.load(perf)\n",
    "#kdo    = pickle.load(kdof)\n",
    "#mac    = pickle.load(macf)\n",
    "#rsi    = pickle.load(rsif)\n",
    "#adx    = pickle.load(adxf)\n",
    "rets   = pickle.load(retsf)\n",
    "\n",
    "#arof.close()\n",
    "#bolf.close()\n",
    "#ccif.close()\n",
    "#momf.close()\n",
    "#volf.close()\n",
    "#smaf.close()\n",
    "#perf.close()\n",
    "#kdof.close()\n",
    "#macf.close()\n",
    "#rsif.close()\n",
    "#adxf.close()\n",
    "retsf.close()\n",
    "\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def calculate_indicators(tickers, yr_closes_dict, attributes, periods, count, last_close, day_hl):\n",
    "    hlc_dict              = attributes[0]\n",
    "    kdo_dict              = attributes[1]\n",
    "    rsi_dict              = attributes[2]\n",
    "    adx_dict              = attributes[3]\n",
    "    mac_dict              = attributes[4]\n",
    "    com_dict              = periods[3]\n",
    "    day_dict              = periods[4]\n",
    "    lengths_hlc_dfs       = periods[5]\n",
    "    lengths_yr_closes_dfs = periods[6]\n",
    "    \n",
    "    for name in tickers:\n",
    "        #Companies days highs/lows/typical_prices at each time interval since 2011 as well\n",
    "        # as the previous years worth of closing prices\n",
    "        highs          = hlc_dict[name]['Highs']\n",
    "        lows           = hlc_dict[name]['Lows']\n",
    "        typs           = hlc_dict[name]['Typicals']\n",
    "        clss           = yr_closes_dict[name]\n",
    "        \n",
    "        #lhl has the lengths of each highlowclose company df in order to\n",
    "        # efficiently retrieve the last days highs,lows, and adx,kd,rsi values without \n",
    "        # calculating it 58 times per call, instead its calculated once in make_year_list(). \n",
    "        # Same goes for lml which is the previous closing year forward lengths\n",
    "        length_hlc     = lengths_hlc_dfs[name]\n",
    "        length_clss    = lengths_yr_closes_dfs[name]\n",
    "        prev_day       = day_dict[name] + count\n",
    "        \n",
    "        # Get prev days highs and lows using precalculated highlowclose df lengths\n",
    "        prev_day_hi    = highs.iloc[length_hlc]\n",
    "        prev_day_lo    = lows.iloc[length_hlc]\n",
    "        \n",
    "        # Closing value at this time yesterday\n",
    "        neg24hr_cls    = clss.iloc[prev_day]\n",
    "\n",
    "        # Todays current low, high, close, time, typical value, and amount change since yesterday\n",
    "        day_lo         = day_hl[name][0]\n",
    "        day_hi         = day_hl[name][1]\n",
    "        day_cls        = last_close[name][0]\n",
    "        day_time       = last_close[name][1]\n",
    "        day_typ        = (day_lo + day_hi + day_cls) / 3.\n",
    "        day_chg        = day_cls - neg24hr_cls\n",
    "        \n",
    "        # Get the current true range, positive and negative directional movement values\n",
    "        true_rng_short = max((day_hi-day_lo), abs(day_hi-neg24hr_cls), abs(day_lo-neg24hr_cls))   \n",
    "        # If either are less than 0, set to 0\n",
    "        pos_dm_short   = max((day_high - prev_day_hi), 0.0)\n",
    "        neg_dm_short   = max((day_low  - prev_day_lo), 0.0)  \n",
    "        \n",
    "        # If day change is positive, set the day_positive, else set the absolute value to day_neg\n",
    "        if day_chg >= 0:\n",
    "            day_pos, day_neg = day_chg, 0.0\n",
    "        else:\n",
    "            day_pos, day_neg = 0.0, abs(day_chg) \n",
    "\n",
    "        # Each company has 8 different period length values to get find the most useful period values\n",
    "        indic_list = []\n",
    "        for x in range(8):\n",
    "            # Df's containing that companies prev values using a certain period length\n",
    "            kdo_df         = kdo_dict[name][x]\n",
    "            rsi_df         = rsi_dict[name][x]\n",
    "            adx_df         = adx_dict[name][x]\n",
    "            mac_df         = mac_dict[name][x]\n",
    "            #length of the first 8 period lengths for each company as well as the period starting\n",
    "            # value which is period start(ex. 5 days ago) plus the count we're at today so its exactly\n",
    "            # 5 trading days ago at the same time in the day as now. Day start is the prev day value \n",
    "            # at this time needed for kd, and macd calculations. Closing period is what several funcs\n",
    "            # use which simply gets us the df from our period_start to the present.\n",
    "            len_prd        = comb_dict[name][x][2]\n",
    "            day_start      = comb_dict[name][x][8] + count\n",
    "            prd_start      = comb_dict[name][x][4] + count\n",
    "            close_prd      = clss.iloc[prd_start:]\n",
    "            \n",
    "            \n",
    "            #INDICATOR CALCULATIONS:_________________________\n",
    "            # Commodity Channel Index:\n",
    "            typ_prd        = typs.iloc[prd_start:]\n",
    "            typ_mean       = typ_prd.mean()\n",
    "            typ_mad        = typ_prd.mad()\n",
    "            cci            = (day_typ - typ_mean) / (0.015 * typ_mad)\n",
    "            # Aroon index:\n",
    "            aro_start      = comb_dict[name][x][5] + count\n",
    "            aro_prd        = clss.iloc[aro_start:]['Closes']\n",
    "            len_aro        = float(comb_dict[name][x][3])\n",
    "            \n",
    "            aro_pos        = ((len_aro - (length_clss - aro_prd[aro_prd.idxmax()])) / len_aro) * 100.\n",
    "            aro_neg        = ((len_aro - (length_clss - aro_prd[aro_prd.idxmin()])) / len_aro) * 100.\n",
    "            aro            = aro_pos - aro_neg\n",
    "            # Moving Average Convergence Divergence:\n",
    "            prev_day_mac   = mac_df.iloc[day_start]\n",
    "            prev_day_short = prev_day_mac[0]\n",
    "            prev_day_long  = prev_day_mac[1]\n",
    "            \n",
    "            short_multplr  = comb_dict[name][x][0]\n",
    "            long_multplr   = comb_dict[name][x][1]\n",
    "            \n",
    "            mac_short      = (day_cls - prev_day_short) * short_multplr + prev_day_short\n",
    "            mac_long       = (day_cls - prev_day_long)  * long_multplr  + prev_day_long\n",
    "            macd           = mac_short - mac_long\n",
    "            # Relative Strength Index:\n",
    "            prev_rsi_vals  = rsi_df.iloc[length_hlc]\n",
    "            prev_avgg      = prev_rsi_vals[0]\n",
    "            prev_avgl      = prev_rsi_vals[1]\n",
    "            \n",
    "            avg_g          = ((prev_avgg * (len_prd - 1.)) + day_pos) / len_prd\n",
    "            avg_l          = ((prev_avgl * (len_prd - 1.)) + day_neg) / len_prd\n",
    "            rsi            = min((100. - (100. / (1. + (avg_g / avg_l)))),100.)\n",
    "            # Average Directional Index:\n",
    "            prev_adx_vals  = adx_df.iloc[length_hlc]\n",
    "            prev_true_rng  = prev_adx_vals[0] \n",
    "            prev_pos_dm    = prev_adx_vals[1]\n",
    "            prev_neg_dm    = prev_adx_vals[2]\n",
    "            prev_adx       = prev_adx_vals[3]\n",
    "            \n",
    "            true_rng       = prev_true_rng - (prev_true_rng / len_prd) + true_rng_short\n",
    "            pos_dm         = prev_pos_dm   - (prev_pos_dm   / len_prd) + pos_dm_short\n",
    "            neg_dm         = prev_neg_dm   - (prev_neg_dm   / len_prd) + neg_dm_short  \n",
    "            pos_di         = (pos_dm / true_rng) * 100.\n",
    "            neg_di         = (neg_dm / true_rng) * 100.\n",
    "            dx             = (pos_di - neg_di) / (pos_di + neg_di) \n",
    "            adx            = ((prev_adx * 13.) + dx) / len_prd\n",
    "            # Bollinger Bands:\n",
    "            bol_mean       = close_prd.mean()\n",
    "            bol_std        = close_prd.std()\n",
    "            bol_uppr       = bol_mean + bol_std * 2.\n",
    "            bol            = (day_cls - bol_mean) / (bol_uppr - bol_mean)\n",
    "            # Stochastic Oscillators:\n",
    "            hi_prd         = highs.iloc[prd_start:]\n",
    "            lo_prd         = lows.iloc[prd_start:]\n",
    "            prev_min       = min(lo_prd)\n",
    "            prev_max       = max(hi_prd)\n",
    "            \n",
    "            prev_day_k     = kdo_df.iloc[day_start]\n",
    "            prev_k         = kdo_df.iloc[length_hlc]\n",
    "            \n",
    "            k              = (day_cls - prev_min) / (prev_max - prev_min) * 100.\n",
    "            d              = (prev_k + prev_day_k + k) / 3.\n",
    "            #Simple Moving Average:\n",
    "            sma_start      = comb_dict[name][x][6] + count\n",
    "            sma_prd        = clss.iloc[sma_start:]\n",
    "            sma            = sma_prd.mean()\n",
    "            # Price/Earnings Ratio:\n",
    "            per_start      = comb_dict[name][x][7] + count\n",
    "            per_prd        = clss.iloc[per_start:]\n",
    "            prev_prd_per   = per_prd.iloc[0]\n",
    "            pe             = day_cls / (day_cls - prev_prd_per)\n",
    "            # Momentum:\n",
    "            prev_prd_mom   = close_prd.iloc[0]\n",
    "            mom            = day_cls / prev_prd_mom - 1.\n",
    "            # Volatility:\n",
    "            vol            = ((close_prd / close_prd.shift(1) - 1.).std() * np.sqrt(float(len_prd)))\n",
    "            ###\n",
    "            # Combine indicators\n",
    "            indic_list.append([rsi, vol, sma, cci, pe, mom, bol, aro, macd, adx, d])\n",
    "            #______________________________________________\n",
    "            \n",
    "            # Update the dictionary dfs to include the new values at this timestep for each period length\n",
    "            kdo_dict[name][x].ix[day_time]  = [k, d]\n",
    "            mac_dict[name][x].ix[day_time]  = [mac_short, mac_long, macd]\n",
    "            rsi_dict[name][x].ix[day_time]  = [avg_g, avg_l, rsi]\n",
    "            adx_dict[name][x].ix[day_time]  = [true_rng, pos_dm, neg_dm, adx]\n",
    "        \n",
    "        # highlowclose only updated once for each company per timestep, then all dict put together\n",
    "        # then indicator list is set to the company key name for each company\n",
    "        hlc_dict[name].ix[day_time] = [day_high, day_low, day_cls, day_typ]\n",
    "        attributes = [hlc_dict, kd_dict, rsi_dict, adx_dict, macd_dict]\n",
    "        indicators[name] = indicator_list\n",
    "       \n",
    "    return indicators, attributes\n",
    "\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED\n"
     ]
    }
   ],
   "source": [
    "def add_new_data(intra_data=None):\n",
    "    try:\n",
    "        if intra_data == None:\n",
    "            bonn        = open('pickledadjustedintracomplete3.pickle', 'rb')\n",
    "            intra_data  = pickle.load(bonn)\n",
    "            bonn.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    indx      = intra_data['BPOP'].index\n",
    "    prev_date = str(indx[-1])[:10]\n",
    "    next_year = str(int(prev_date[:4])+1)\n",
    "    end_date  = next_year + prev_date[4:]\n",
    "    start     = datetime.datetime.strptime(prev_date, '%Y-%m-%d').date()\n",
    "    end       = datetime.datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "    rs2       = NYSE_tradingdays2(start, end)\n",
    "    from_date = str(rs2[1])[:10]\n",
    "    \n",
    "    data      = retrieve_bonnet_data(from_date)\n",
    "    adj       = adjust_bonnet_high_lows(data)\n",
    "    \n",
    "    intra_cpy = intra_data.copy()\n",
    "    adj_cpy   = adj.copy()\n",
    "    \n",
    "    for key, value in adj_cpy.iteritems():\n",
    "        new_indx = []\n",
    "        index    = value.index\n",
    "        \n",
    "        for each in index:\n",
    "            new_indx.append(pd.Timestamp(each))\n",
    "        value.index = new_indx\n",
    "        \n",
    "        value['Typical'] = (value['Highs'] + value['Lows'] + value['Closes']) / 3.\n",
    "        \n",
    "        intra_cpy[key] = intra_cpy[key].append(value)\n",
    "\n",
    "    return intra_cpy, adj\n",
    "\n",
    "def retrieve_bonnet_data(fromd):\n",
    "    yahoo_tickers  = ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "                      'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "                          'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "                          'SIRI','WIN','YHOO','BHP','BP',\n",
    "                          'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "                          'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "                          'MGM','AMD','EMC','GLW','HPQ','S','T',\n",
    "                          'USO', 'GLD', 'SPY',\n",
    "                          'DJI', 'GSPC', 'IXIC']\n",
    "    pages = {}\n",
    "    today = str(datetime.date.today())\n",
    "    for name in yahoo_tickers:\n",
    "        page = 'http://www.thebonnotgang.com/quotes/q.php?timeframe=1m&dayFrom='+fromd+'&dayTo='+today+'&symbol='+name\n",
    "\n",
    "        if (name != 'DJI') or (name != 'GSPC') or (name != 'IXIC'):\n",
    "            name = name\n",
    "        else:\n",
    "            name = '^' + name\n",
    "\n",
    "        pages[name] = urllib.urlopen(page).read()\n",
    "\n",
    "    complete_list = []\n",
    "    for each in yahoo_tickers:\n",
    "        current = pages[each]\n",
    "        start = current.find('2016')\n",
    "        end = len(current)\n",
    "        test = True\n",
    "        df_list = []\n",
    "\n",
    "        while test == True:\n",
    "            date_end = current.find(';',start)\n",
    "            open_end = current.find(';',date_end+1)\n",
    "            high_end = current.find(';',open_end+1)\n",
    "            low_end  = current.find(';',high_end+1)\n",
    "            close_end = current.find(';',low_end+1)\n",
    "\n",
    "            date  = current[start:date_end]\n",
    "            high  = float(current[open_end+1:high_end].replace(',','.'))\n",
    "            low   = float(current[high_end+1:low_end].replace(',','.'))\n",
    "            close = float(current[low_end+1:close_end].replace(',','.'))\n",
    "\n",
    "            t = int(date[11:13])-4\n",
    "            if t < 10:\n",
    "                t = '0'+str(t)\n",
    "            else:\n",
    "                t = str(t)\n",
    "            date = date[0:11]+t+date[13:]\n",
    "\n",
    "            day = [date, high, low, close]\n",
    "            df_list.append(day)\n",
    "\n",
    "            start = current.find('2016-', close_end)\n",
    "            if start == -1:\n",
    "                break\n",
    "\n",
    "        df = pd.DataFrame(df_list, columns=['Time','Highs','Lows','Closes'])\n",
    "        df = df.set_index('Time')\n",
    "\n",
    "        complete_list.append(df)\n",
    "    return complete_list\n",
    "\n",
    "def adjust_bonnet_high_lows(adjusted_intra):\n",
    "    yahoo_tickers  = ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "                      'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "                          'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "                          'SIRI','WIN','YHOO','BHP','BP',\n",
    "                          'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "                          'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "                          'MGM','AMD','EMC','GLW','HPQ','S','T',\n",
    "                          'USO', 'GLD', 'SPY',\n",
    "                          '^DJI', '^GSPC', '^IXIC']\n",
    "    \n",
    "    new_adjusted_intra = {}\n",
    "    \n",
    "    z = 0\n",
    "    for df in adjusted_intra:\n",
    "        highs  = df['Highs'].values.tolist()\n",
    "        lows   = df['Lows'].values.tolist()\n",
    "        closes = df['Closes'].values.tolist()\n",
    "        index  = df.index\n",
    "        prev   = 0\n",
    "\n",
    "        for x in xrange(1,len(highs)):\n",
    "            if index[x][8:10] == index[x-1][8:10]:\n",
    "                highs[x] = max(highs[prev:x+1])\n",
    "                lows[x]  = min(lows[prev:x+1])\n",
    "            else:\n",
    "                prev = x\n",
    "\n",
    "        df         = pd.DataFrame([index,highs,lows,closes]).T\n",
    "        df.columns = ['Time','Highs','Lows','Closes']\n",
    "        df         = df.set_index(['Time'])\n",
    "\n",
    "        new_adjusted_intra[yahoo_tickers[z]] = df\n",
    "        z += 1\n",
    "    return new_adjusted_intra\n",
    "\n",
    "def append_new_to_intra_data(adj):\n",
    "    aj   = open('pickledadjustedintracomplete2.pickle', 'rb')\n",
    "    data = pickle.load(aj)\n",
    "    aj.close()\n",
    "\n",
    "    for key, value in adj.iteritems():\n",
    "        data[key] = data[key].append(value)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_prev_10d1min_intraday(self):\n",
    "    \"\"\"\n",
    "    Added to retrieve the previous 10 days worth of intraday trading data with 1 minute\n",
    "    intervals.\n",
    "    \"\"\"\n",
    "    tickers    = ['BPOP','FITB','HBAN','CMCSA','EBAY',\n",
    "                  'AAPL','AMAT','BRCD','CSCO','GOOG','INTC',\n",
    "                  'LVLT','MSFT','MU','NVDA','ORCL','QCOM',\n",
    "                  'SIRI','WIN','YHOO','BHP','BP',\n",
    "                  'RIO','XOM','GE','F','MO','XRX','GS','JPM',\n",
    "                  'LYG','MS','RF','USB','WFC','MRK','PFE','LMT',\n",
    "                  'MGM','AMD','EMC','GLW','HPQ','S','T',\n",
    "                  'USO', 'GLD', 'SPY',\n",
    "                  '.DJI','.INX','IXIC',\n",
    "                  'XLY','XLP','XLE','XLF','XLV','XLI','XLK']\n",
    "    \n",
    "    pages = {}\n",
    "    for each in tickers:\n",
    "        page = 'http://www.google.com/finance/getprices?i=60&p=10d&f=d,c&df=cpct&q='+each\n",
    "        pages[each] = urllib.urlopen(page).read()\n",
    "\n",
    "    complete_list = []\n",
    "    day_list      = []\n",
    "    \n",
    "    for each in tickers:\n",
    "        current = pages[each]\n",
    "        start = current.find('\\na')+2\n",
    "        end = len(current)\n",
    "        test = True\n",
    "        df_list = []\n",
    "        \n",
    "        while test == True:\n",
    "            date = current.find(',',start)\n",
    "            price = current.find('\\n',date)\n",
    "            df_list.append(float(current[date+1:price]))\n",
    "\n",
    "            if price != end-1:\n",
    "                if current[price+1] == 'a':\n",
    "                    day_list.append(df_list)\n",
    "                    df_list = []\n",
    "                start = price+2    \n",
    "            else:\n",
    "                test = False\n",
    "                complete_list.append(day_list)\n",
    "                day_list = []        \n",
    "    return complete_list\n",
    "\n",
    "def plot_data(returns):\n",
    "    \"\"\"Plot stock returns\"\"\"\n",
    "    ax = returns.plot(label='COMPANY')\n",
    "        \n",
    "    # Add axis labels and legend\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_bollinger(highlowcloselist, yahoo_tickers, period, key):\n",
    "    \"\"\"Takes the rolling means and the upper and lower bands and plots the data.\"\"\"\n",
    "    close = highlowcloselist[key][2]\n",
    "    rm    = pd.rolling_mean(close, window=period)\n",
    "    rstd  = pd.rolling_std(close,  window=period)\n",
    "    upper = rm + rstd * 2.\n",
    "    lower = rm - rstd * 2.\n",
    "        \n",
    "    # Plot raw closing values, rolling mean and Bollinger Bands\n",
    "    ax = close[period : (8 * period)].plot(title=\"Bollinger Bands\",label='COMPANY')\n",
    "    rm[period:].plot(label='Rolling mean', ax=ax)\n",
    "    upper[period:].plot(label='upper band', ax=ax)\n",
    "    lower[period:].plot(label='lower band', ax=ax)\n",
    "    \n",
    "    # Add axis labels and legend\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()     \n",
    "\n",
    "def compute_and_plot_returns(highlowcloselist, yahoo_tickers, periods, chosen_period, chosen_key):\n",
    "    \"\"\"Compute the return values for each company by calling compute_returns() and then\n",
    "       plot a chosen ticker/chosen period with the returned values\"\"\"\n",
    "    #Compute daily returns\n",
    "    returns = compute_returns(highlowcloselist, yahoo_tickers, periods)\n",
    "    \n",
    "    # To list possible keys to plot:\n",
    "    #print returns.keys()\n",
    "    \n",
    "    # For each key, there are 6 possible return lengths to look at:\n",
    "    # - Minute returns = returns[key][0]\n",
    "    # - Hour returns   = returns[key][1]\n",
    "    # - Day returns    = returns[key][2]\n",
    "    # - 2-day returns  = returns[key][3]\n",
    "    # - 3-day returns  = returns[key][4]\n",
    "    # - 5-day returns  = returns[key][5]\n",
    "    \n",
    "    plot_data(returns[chosen_key][chosen_period])\n",
    "\n",
    "def day_end_operations(tickers, combined, last_close):\n",
    "    \"\"\"\n",
    "    An end of day function called once the trading day is over to update our pickled\n",
    "    files filled with daily closing prices and indicator values.\n",
    "    \n",
    "    Open our pickled files, read them in, and add our latest closing prices and \n",
    "    indicator values to them to show the day end values.\n",
    "    \n",
    "    Finally once new values are appending, write the files back to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    cl       = open('smallcompanyclosingslist.pickle','rb')\n",
    "    fe       = open('smallcompanyfeatureslist.pickle','rb')\n",
    "    closings = pickle.load(cl)\n",
    "    stats    = pickle.load(fe)\n",
    "    cl.close()\n",
    "    fe.close()\n",
    "    \n",
    "    try:\n",
    "        i = 0\n",
    "        for each in tickers:\n",
    "\n",
    "            date           = datetime.date.today()\n",
    "            day_end_close  = last_close[i]\n",
    "            last_stats     = combined[i]\n",
    "            close_df       = pd.DataFrame([day_end_close], index=[date])\n",
    "            stats_df       = pd.DataFrame(last_stats, index=[date])\n",
    "            closings[each] = closings[each].append(close_df)  \n",
    "            stats[each]    = stats[each].append(stats_df)\n",
    "            i += 1  \n",
    "\n",
    "        cl2 = open('smallcompanyclosingslist.pickle','wb')\n",
    "        fe2 = open('smallcompanyfeatureslist.pickle','wb')\n",
    "        pickle.dump(closings,cl2)\n",
    "        pickle.dump(stats,fe2)\n",
    "        cl2.close()\n",
    "        fe2.close()  \n",
    "    except:\n",
    "            print each\n",
    "            raise \n",
    "    \n",
    "    return closings, stats\n",
    "\n",
    "def normalize(combined):\n",
    "    normed = {}\n",
    "    for key,value in combined.iteritems():\n",
    "        normed[key] = (value - value.mean()) / (value.max() - value.min()) \n",
    "    \n",
    "    return normed\n",
    "\n",
    "def yahoo_historical_retrieval(yahoo_tickers):\n",
    "    historical, b = {}, 0\n",
    "    \n",
    "    for each in yahoo_tickers:\n",
    "        try:\n",
    "            test = Share(each)\n",
    "            history = test.get_historical('2011-07-01','2016-06-18')\n",
    "            historical[each] = history\n",
    "        except:\n",
    "            print each\n",
    "            raise\n",
    "    \n",
    "        stdout.write(\"\\r%d\" % b)\n",
    "        stdout.flush()\n",
    "        b += 1\n",
    "    return historical\n",
    "\n",
    "def adjust_yahoo(historical, yahoo_tickers):\n",
    "    adj_historical = {}\n",
    "    \n",
    "    for each in yahoo_tickers:\n",
    "        date, adj_close, close = [], [], []\n",
    "\n",
    "        for x in xrange(0, len(historical[each])):\n",
    "            date.append(historical[each][x]['Date'])\n",
    "            adj_close.append(float(historical[each][x]['Adj_Close'].replace(',','')))\n",
    "            close.append(float(historical[each][x]['Close'].replace(',','')))\n",
    "\n",
    "        df         = pd.DataFrame([adj_close, close, date]).T \n",
    "        df.columns = ['Adj_Close', 'Close', 'Date']\n",
    "        df         = df.set_index('Date')\n",
    "\n",
    "        adj_historical[each] = df\n",
    "    return adj_historical\n",
    "\n",
    "def unadj_intra():\n",
    "    unadj_intr = {}\n",
    "    \n",
    "    for file in os.listdir(\"C:/Users/JohnSmith2/version-control/Projects/projects/trading/intraday\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            name = str(file)[:str(file).find('_')]\n",
    "            \n",
    "            if (name != 'DJI') or (name != 'GSPC') or (name != 'IXIC'):\n",
    "                name = name\n",
    "            else:\n",
    "                name = '^' + name \n",
    "            \n",
    "            path = \"intraday/\" + str(file)\n",
    "            df   = pd.read_csv(path, sep=';', decimal=',')\n",
    "            df   = df[['timestamp','high','low','close']]\n",
    "            \n",
    "            unadj_intr[name] = df\n",
    "    return unadj_intr\n",
    "\n",
    "def adj_intra(unadj_intra, adj_yahoo):\n",
    "    adj_intr = {}\n",
    "    \n",
    "    for key,value in unadj_intra.iteritems():\n",
    "        value  = value.set_index('timestamp')\n",
    "        index2 = value.index\n",
    "        prev   = 0\n",
    "        \n",
    "        for x in xrange(1,len(value)):\n",
    "            if index2[x][8:10] != index2[x-1][8:10]:\n",
    "                value[prev:x] *= (adj_yahoo[key].loc[index2[x][:10]]['Adj_Close'] / \n",
    "                                  adj_yahoo[key].loc[index2[x][:10]]['Close'])\n",
    "                prev = x\n",
    "        \n",
    "        adj_intr[key] = value\n",
    "    return adj_intr\n",
    "\n",
    "def adjust_high_lows(adjusted_intra):\n",
    "    new_adjusted_intra = {}\n",
    "    \n",
    "    for key,value in adjusted_intra.iteritems():\n",
    "        highs  = value['high'].values.tolist()\n",
    "        lows   = value['low'].values.tolist()\n",
    "        closes = value['close'].values.tolist()\n",
    "        index  = value.index\n",
    "        prev   = 0\n",
    "\n",
    "        for x in xrange(1,len(highs)):\n",
    "            if index[x][8:10] == index[x-1][8:10]:\n",
    "                highs[x] = max(highs[prev:x+1])\n",
    "                lows[x]  = min(lows[prev:x+1])\n",
    "            else:\n",
    "                prev = x\n",
    "\n",
    "        df         = pd.DataFrame([index,highs,lows,closes]).T\n",
    "        df.columns = ['Time','Highs','Lows','Closes']\n",
    "        df         = df.set_index(['Time'])\n",
    "\n",
    "        new_adjusted_intra[key] = df\n",
    "    return new_adjusted_intra\n",
    "\n",
    "print \"LOADED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
